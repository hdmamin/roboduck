3/29/23 wed
-----------
Sequential Chain
{input_vars} -> render prompt 1 -> {output_str1} 
{output_str1} -> input to prompt 2 -> render prompt 2 -> {output_str1, output_str2}

I want
{input_vars} -> render prompt 1 -> {output_str1}
{concat(render_prompt1, output_str1)} -> input to prompt 2 -> render prompt 2 -> {output_str2}
    (But also along the way, store output of each step in a way we can access later.)

Take the inputs and pass to our first chain.
Then concat the first chain's prompt to its output.

3/30/23 thurs
-------------
Chain 0
[system msg, first question] -> first answer
[system msg, first question, first answer, second question] -> second answer
[system msg, first question, first answer, second question, second answer, third question] -> third answer

approaches O(n^2) wrt encoding questions (i.e. first question is encoded n times where n is number of questions). Whereas if we make this a single prompt, encoding is O(n) (i.e. each section is encoded once).

4/30/23 sun
-----------
dynamic docs thoughts

goals
-make it easy for a new user or dev to quickly answer questions about how to do something with the library
-this means supporting/streamlining relevant actions:
    -search
    -grok
    -understand (I use "grok" to mean understanding implementation details, "understand" to mean understand design decisions)
-easy to maintain (ideally avoid having to write actual docs. But keeping dynamic docs updated might be even harder realistically.)
    
implementation
-stick whole codebase into a long prompt for gpt4 (long context window version), then inject user question
    -could retrieve relevant commit from github and do this live, potentially no manual updates whatsoever
-embed file chunks (or maybe 1 chunk per function/class/variable?). Then use vector-based retriever to get top k relevant results, and pass only those to gpt (no need for super long context window)
    -use openai or try free option here? Lower stakes than the actual debugging functionality itself.
    -prob means need to generate new embeddings on each release. Maybe include them in setup.py data_files or something? Maybe can add github build action to do this automatically.
-just ask gpt to write static docs and I revise as needed
-fine tuned codex-like model. No docs, just better code generation.
    -new fine-tuning job one each release. Maybe can add github build action to do this automatically.

thoughts
-dynamic approaches are interesting but could be a whole project in and of themselves
-leaning towards starting with standard auto docs + maybe a few quickstart examples (the latter could perhaps be gpt-assisted but not sure if that would really even help much - compiling prompt takes some time and I tried to design everything to be very simple to use anyway)

5/7/23 sun
----------
storing code vs full_code options:
-check prompt name/kwargs, store one or the other depending
-always store both
-always store all kwargs

Only last_new_code is every used atm, nothing uses last_code (though someone could). And I believe I always ask gpt to generate a fixed version of the current snippet. So I'm leaning towards just always storing code snippet.

5/11/23 thurs
-------------
from_config
	[optional] user kwargs
	template_kwargs
	> Safest way: just update template_kwargs with global config kwargs. Then update the result with user kwargs.

init
	[optional] kwargs (may be form user OR template)
	> If kwargs are from template, assume that from_config already updated them with global config kwargs. If kwargs are from user, we don't want to update them anyway. So we only use global kwargs if field is NOT already set in passed in kwargs.

5/16/23 tues
------------
possible causes of nested dir docs error:
X -nested dir inside docs/ causes a problem
	> Can't be this bc we're able to successfully build docs for errors.py from inside docs/cli.
-nested dir inside lib/roboduck causes a problem
-having module and dir with same name causes problem

5/18/23 thurs
-------------
dynamic history updating thoughts
-_history is currently a list that includes system message. We probably want a dequeue that EXCLUDES system message so we can easily and efficiently popleft at any point to discard the oldest message.
-could make _history itself this queue and then tack on system message each time we currently access it
OR
perhaps make _history a property and implement logic to do that automatically
-problem is, history() is already a method and properties often make use of leading underscore "private" var, but in this case the property would already have the leading underscore. Adding a second would add name mangling.
	-can avoid this by using another attr self._messages or something that the _history property can reference.
	-after implementing a quick/dirty toy example, I'm realizing every time we access _history we'll need to convert deque to a list so we can add system message, so it's either O(N) there or O(N) when popping index 1. Seems like comparable downsides so maybe just stick with the simpler implementation of using a single list and popping the item at idx 1 repeatedly. Seems unlikely that this is something where optimization would be particularly useful. Context window has a fixed size (with current arches anyway) so technically the number of messages is limited to a fixed k (worst case assume 1 char per message) so technically I guess you could argue the pop is still O(1).


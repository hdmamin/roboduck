2/1/23 wed
----------
X -settle on project
X -find all old relevant work
    > added 3 relevant files to this dir for now. The fourth (and perhaps most developed) effort is in the ~/nbextension-duckling dir.
~ -decide how to structure dir(s)/repo(s) (Will prob have multiple semi-related outputs, perhaps 1-5.)
    > Tentatively thinking monorepo. Easier to track everything that way.
-pick repo name
    > cairina, from "cairina moschata", one of the biggest, most powerful looking species of duck ("muscovy"). Think (rubber) duck on steroids.
X -add readme img
    > :D
~ -read distill AI-augmented human intelligence post

2/2/23 thurs
------------
X -premortem
X -map out very tentative plan for order to take things on (e.g. debugger -> jupyter magic -> interactive error messages -> nbextension -> reloading stuff -> LLM native programming language)
    > Lets start with debugger, then jupyter magic. The former is pretty far along and the latter should largely build on top of the debugger. We can take stock after that - depending on timing, can take a break or fit in error messages before break or move on to one of the bigger rocks (live reloading stuff or nbextension) if it went surprisingly fast.
X -consider whether notebook extension proj should continue or start over as jupyterlab extension
    -find stats on how commonly used each is?
        > In 2019 jetbrains python dev survey, DS use cases skewed twoards notebook (13% vs 5%; if we normalize to avoid non-jupyter options, that becomes 72% and 28%). By 2020 survey, it was 11% vs 6% (65% and 35% normalized). In 2021, they didn't break down results by DS users, so it's just 3% vs 2% (60% vs 40% normalized). In the "additional but not main IDE" question, it was 25% vs 12% notebooks (68% vs 32% normalized). In kaggle 2022 survey, 66.5% used nb while 23.7% used lab (74% vs 26% normalized). So (somewhat surprisingly) it seems like notebook is still considerably more popular, though perhaps slowly shifting towards lab. I like nb and started building in that so it's tempting to continue in that, but I guess I don't need to decide quite yet given that we're starting with the debugger.
~ -create cookiecutter template for each major project
    > Just start with debugger subdir (now "roboduck", i.e. a more powerful rubber duck). Uploaded to pypi to claim name.
-finish distill AI-augmented human intelligence post

2/3/23 fri
----------
X -remind myself where I left off w/ debugger work
    X -run existing code (do)
    -skim over existing code (read)
    X -read through done and outstanding todos
    X -write some todo items for what comes next
X -possible bug: seems like if we haven't saved nb since latest changes, load_ipynb func can load an outdated version (possibly? Though tit did that once but then I looked again and it seemed to be up to date so idk)
    > Confirmed that was happening but fixed it w/ help of a stackoverflow function.
X -consider adding option for "I don't know". Or maybe something like "If you don't know what's causing the bug, say "I don't know". Then write a list of 5 plausible causes that the developer can check for when debugging." (take advantage of its strength at generating lists, thinking of possibilities we might not)
X -update prompt to explain to gpt what the chat_db()/roboduck()/rbd() func call is
X -rename usages from llmdb to rdb or similar?
X -update func names to fit roboduck lib
X -add stop word to prompt to prevent starting another question
-finish distill AI-augmented human intelligence post

2/4/23 sat
----------
X -see if we can color user/model turns differently. (Bit hard to read atm, though partly bc I'm printing out full prompt for debugging purposes.)
X -add backup value when response is nothing
X -debug why I'm getting so many empty responses
    > Turns out I had docstring quotes in my stop phrases which can be problematic since that is a valid thing to generate at times. Also didn't start my prompt with docstring quotes which maybe degrades quality a bit. But if I add them back in, then I feel like I need it in stopwords again. Hmmm.
X -make codex respond in second person
X -figure out how to make multiline str start with quotes in yaml (or abandon their use)
    > Turns out my old method already supports that. Just don't escape them.
-consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
-hide user warning about using codex model name.

2/5/23 sun
----------
X -consider removing stop seq """ from 3 debug prompts (in case it actually needs to generate that w/in solution)
    > Yes, try it out. Just have roboduck rstrip them.
    > Update: with this format, codex seems to like closing the docstring before doing anything else, then jumping straight to code, meaning no explanation gets written. May need to revert to old no-docstring method.
    > Also realized I haven't been bumping the prompt versions. Whoops. I guess maybe this only needs to be done when I actually bump the jabberwocky pypi version and/or the roboduck pypi version.
_ -consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
    > I think for now let's leave this be - I'll be seeing a lot of query results over the next couple months so it will become apparent if this is something we need (i.e. if quality is an issue). I also suspect we may see a chatgpt/gpt4/codex2 release that makes this type of prompt engineering less necessary before the end of this project, so no need to optimize that dimension now.
X -hide user warning about using codex model name
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.

2/6/23 mon
----------
X -try reverting debug prompts to no opening docstring (see sun notes for why)
    > Nope, tried this again but it really seems like it needs the docstring otherwise it just jumps straight to code. Added logit bias instead of stopword. Also add another stopword since I observed some cases where it skipped repeating the DEVELOPER QUESTION but did repeat LOCAL VARIABLES and everything afterwards.
X -see how chatGPT does
    > As expected, noticeably better. Codex seems to like repeating things and writing lengthy answers here even though I added the word "concise" to the instructions. I bet a better code api will become available within the next few months.
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
X -stream completion instead of waiting for whole thing
    > Added streaming with .02s between characters. Maminbot used .015 but somehow even .02 seems really fast in jupyter. Maybe streamlit had extra latency so it wasn't really .015s there.

2/7/23 tues
-----------
~ -consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
    X -write draft of truncated_repr func
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/8/23 wed
-----------
X -add file from jabberwocky (I think?) to avoid counting jupyter notebooks in github code % count
-handle huge data structures (big df, long list, etc.) 
    X -handle case where seq[:1] still produces repr that's too long
    X -test behavior on dfs again (potential opportunity to do something custom like "pd.DataFrame([col1, col2, ...])"? As in literally that string, just call truncated_repr on cols.)
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/9/23 thurs
-----------
~ -handle huge data structures (big df, long list, etc.) 
    X -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it would be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
        > I guess I intentionally made dotdict have the same str and repr as a regular dict so it's tough to come up with a rule that would help here. I could change its str/repr, just leave it as the regular dict version, or update my debug prompt to show the type of each var too (could be a separate section like GLOBALS_TYPES, could be a comment after each value in GLOBALS like `'nums': [1, 3, 4...], # list`. Last idea is interesting.
    X -try out idea about specifying var types in comments
        > Wrote new func. I like this.
    X -fix bug: empty sets represented wrong
        > Because empty lists and tuples use literals like [] but sets use 'set()' so my logic around the last non-brace index failed. Added some hardcoded logic around setes.
    X -fix bug: dict-like objects have extra quotes
        > Must have added this mistakenly, fixed now.
    X -tweak debug prompt to acknowledge that roboduck func may not be present (in case we repurpose these)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/10/23 fri
-----------
X -handle huge data structures (big df, long list, etc.) 
    X -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
        > Just hardcode for now. There's not going to be some magic number here that's perfect.
    X -document (mention that length is more of a rough guideline, not strictly enforced)
_ -consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
    > Actually I think this is pretty good.
X -tweak prompts to hopefully reduce repetition a bit (been noticing this more lately)
    > Added frequency_penalty of 0.2 (pretty minor change for now, range is -2 to 2). Haven't tested yet, just keep an eye on things as I continue working on it. Don't want to sink too much time into selecting perfect prompt params now bc I think better models may be available soon.
~ -maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
    > Tried replacing load_prompt w/ PromptManager and realized the issue is that I wnat to check the resolved prompt, and pm.query() with verbose mode just prints. But found pm.prompt() and pm.kwargs() methods so it is possible. However, we have to pass GPT obj to PM on instantiation so if we want to support diff backends in roboduck init, we still need to create PM inside it. So not really saving any time. But maybe this work will help pave the way if I want to transition to ConversationManager at some point.
    > Haven't tested this yet but it's ready to be tried. I kept the old cls around in case I want to revert (w/out messing w/ jupyter/git interaction weirdness).
-try running magic again and see if problems arise

2/11/23 sat
-----------
X -handle pandas series separately in truncated_repr (noticed it was not handled that well, was truncating the metadata for some reason)
    > Also refactored func a bit.
X -try running new version of roboduck with PM (just run all cells from cls def down to debugging session cell)
    X -debug if necessary
    > Had to fix both prompt() and query() calls, they take slightly different args than GPT.query() which in hindsight is maybe not ideal but I guess part of the point is that it's supposed to simplify the call and if it had an identical interface it wouldn't be any easier. Works now, though codex's answer was very wrong.
-consider whether to try replacing PM with convmanager
-if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
-try running jupyter magic again and see if problems arise

2/12/23 sun
-----------
X -add my jabberwocky pre-commit hook to avoid pushing api key
_ -consider whether to try replacing PM with convmanager
    > Don't want to get too bogged down on exact interfact yet. Don't know if upcoming chatgpt API might have slightly different UX (e.g. conversation_id) - this could be used in place of my conversationmanager cls, which is more helpful for the alexa skill with its auto-persona generation via wikipedia.
_ -if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
~ -try running jupyter magic again and see if problems arise
    > Existing code worked fine. Taking another stab at setting shell attr instead of global var to help with code insertion but not working yet.
    > Confirmed that I CAN set a var ;ole get_ipython().xyz = 123 and it persists (just working in global scope for both).
    > BUT if I set it in RoboduckDB.ask_language_model, it does not seem to persist by the time the magic checks it. Need to investigate more. I think this is similar to the issue I ran into last time. Maybe shell obj is recreated every time we change scope or something?

2/13/23 mon
-----------
X -debug failure to set get_ipython() var in certain contexts (see yesterday's last few bullet points)
    > Realized there's no real reason this has to be set on the Ipython object. I ended up creating a cache class and setting a var there - not sure if this is meaningfully better than global (we're just editing a global class var, I suppose) but maybe it is, idk.
X -add "keep an eye on" and "won't do" sections to dailies
~ -extract code from gpt completion when inserting code cell
    > Tried ast.parse-based method, pretrained huggingface classifier method, considered ways of breaking down prompt into 2 to make extraction easier. Ended up implementing method that makes a second codex call but only when the magic exits, not on every conversational turn, that extracts code. Seems to work alright in playground but need to debug a bit more - code snippet is no longer being used to populate cell below. Also, without live typing it feels a lot slower.
-finish distill post on augmenting human intellect

2/14/23 tues
------------
X -debug code extraction in magic: why is it failing to populate next cell now?
    > First val returned by gpt.query is list, had to extract str.
X -consider other appraoches to code extraction (really feel like a simpler version relying on templating should be possible)
    > Have prompt ask for a two part solution and title the sections carefully to try to encourage this separation. Seems promising so far. Had to add some new stop words. Just updating debug and debug_full prompts atm, leave duckling prompt for later.
~ -update roboduck cls to avoid printing the SOLUTION PART 2 part
    > Did planning but no implementation yet. A bit tricky since we're printing one token at a time and SOLUTION tokenizes into ['S', 'OL', 'UTION']. It becomes very slightly less complex if we go lowercase (['s', 'olution']) but even so we probably need a solution that builds up a queue of tokens to print and prints with a slight delay (i.e. always check the last x tokens, prob 4-8, before printing. If we find ourselves in the SOLUTION PART 2 phrase, we pop those tokens off and don't print them until we hit a newline).
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/15/23 wed
-----------
X -move testing cells to bottom of nb to avoid re-run on kernel restart
    > Frequent restarts are necessary to make magic update, even if we manually delete it before registering it again. Weird that this issue just popped up. Wonder if related to deleting cookies somehow? Seems unlikely. Better solution is to do as much of debugging as possible in Roboduck cls and just update magic occasionally once a big chunk of progress has been made.
~ -update roboduck cls to avoid printing the SOLUTION PART 2 part
    ~ -implement algo from yesterday: SOLUTION tokenizes into ['S', 'OL', 'UTION']. It becomes very slightly less complex if we go lowercase (['s', 'olution']) but even so we probably need a solution that builds up a queue of tokens to print and prints with a slight delay (i.e. always check the last x tokens, prob 4-8, before printing. If we find ourselves in the SOLUTION PART 2 phrase, we pop those tokens off and don't print them until we hit a newline).
        > WIP. Quite tricky. Getting close now - remaining bug is that openai seems to (at least sometimes) return "SOLUTION PART 2" as a single token (or else I'm unintentionally converting it before adding it to the queue somehow?). Think the solution is to first confirm that that's actually happening (i.e. print out cur on each iteration). Then if that is true, update algo to grab items from start of queue until the desired char limit is met and track the index, and pop that many items. Or just add an extra check if first val is SOLUTION PART 2, though that wouldn't handle the case of them returning it as 2 tokens. (I initially thought maybe they add all stop words as single tokens to vocab somehow, but then I remembered only PART 1 and 3 versions are stop words, not 2.)
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/16/23 thurs
-------------
X -update roboduck cls to avoid printing the SOLUTION PART 2 part
    X -Debug algo to avoid printing SOLUTION PART 2
        X -print out cur on each iteration to confirm that openai actually sometimes returns SOLUTION PART 2 as a single token even though codex is supposed to tokenize it as 5 tokens
        _ -if ^ is true, update algo to grab items from start of queue until the desired char limit is met and track the index, and pop that many items. Or just add an extra check if first val is SOLUTION PART 2, though that wouldn't handle the case of them returning it as 2 tokens. (I initially thought maybe they add all stop words as single tokens to vocab somehow, but then I remembered only PART 1 and 3 versions are stop words, not 2.)
            > Decided to go with simpler approach (see below).
        X -implement simple approach
            > Wanted to keep this part simple for now. Might end up just having chatgpt/codex 3 api extract code portion once at end when magic is used in insert mode. For now, set colon right after SOLUTION PART N to try to avoid the issue of having a title of unknown content and variable size. Then just relied on assumption that gpt will continue to treat SOLUTION PART 2 as a single streaming step (it's still multiple tokens, but apparently they don't necessarily stream 1 token per step).
            X -tweak prompts
                > Added ANSWER KEY back to stop words and moved LOC (start of local variables) to logit bias.
            X -update magic to work with new solution
                > Had to keep SOLUTION PART 2 in codecompletioncache so we can split on it. Also stripped leading newline in inserted code cell.
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/17/23 fri
-----------
X -consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
    > Considered refactoring further to use ConversationManager but I think by project's end we'll switch over to the chatgpt api anyway which should add enough statefulness for us. Not expecting super longform conversations anyway.
X -cleanup funcs (lots of comments, todos, etc)
X -port everything to a lib
    X -consider if these will live in same lib, same module, etc.
        > Same lib, diff module. Jupyter extension stuff might go elsewhere but debugger and magic should be same lib because the magic just calls the debugger.
-document funcs/classes
-finish distill post on augmenting human intellect

2/18/23 sat
-----------
X -document funcs/classes
    > Documented three modules.
-finish distill post on augmenting human intellect
-start scoping out possible stack trace explainer
    -what is the exact use case - show better error messages all the time? Only if some env var is set? Only if we run a script with some special command? Or enter a conversational console on error? Static vs conversational is one of the first questions to answer.
    -write todos for next steps

2/19/23 sun
-----------
~ -install editable version of lib and test imports
    X -debugger works in jupyter
    X -magic works in jupyter
    ~ -debugger works in ipython
        > Live typing is kind of slow and choppy, but works.
    ! -magic works in ipython
        > We get AttributeError: can't set attribute when calling %duck magic. Caused by the line that sets shell.debugger_cls to RoboDuckDB, seems like ipython shell makes itself read only maybe? Weird that it works in jupyter though. I suspect we may also need to change debugger's method of reading in source code and possibly magic's method of handling insert mode.
-start scoping out possible stack trace explainer
    X -revisit exisitg debug_on_fail work
        > Actually ended up getting this working quite nicely. Ended up writing a nice monkeypatch and excepthook class that should probably end up in htools (currenly in debug_on_error.py). There's some interesting potential here but I'm not sure it's the right place to inject the LLM.
    X -what is the exact use case - show better error messages all the time? Only if some env var is set? Only if we run a script with some special command? Or enter a conversational console on error? Static vs conversational is one of the first questions to answer.
        > Key is we don't really want it to do this on every single error (esp in interactive shell), but want it to be as low friction as possible to use once we want to. I'm thinking maybe just catch errors using excepthook, print normal stack trace, and include a simple y/n input asking the user if they want it explained.
    X -look into prettyerrors source code
        > Actually looks fairly complex, may be easies after all to mess with sys.excepthook. Note that my current version of that doesn't seem to affect ipython, maybe it's already overridden sys.excepthook?
    X -write todos for next steps
-finish distill post on augmenting human intellect

2/20/23 mon
-----------
X -port new funcs from debug_on_error to htools
    X -monkeypatch
    X -excepthook (careful - as written, just importing it registers it. Maybe put in its own module or move to a method of a class that also lets you revert to default excepthook)
    X -get working in ipython
        > Not trivial, but got it working. Ipython does not use sys.excepthook.
-get editable version of lib working
    -magic works in ipython
        -fix line causing attributeerror in magic duck cls
        -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
        -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/21/23 tues
-----------
-get editable version of lib working
    ~ -magic works in ipython
        X -fix line causing attributeerror in magic duck cls
            > Still can't work in default ipython but we handle the error gracefully and provide a command that shows the user how to start an ipython shell that DOES make this magic available. Looks like minimal progress but this was actually a big win.
        -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
        -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/22/23 wed
-----------
X -get editable version of lib working
    X -magic works in ipython
        X -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
            > Only tries to load this if we set full_context=True. I added some documentation saying this is unavailable in ipython.
            > Update: realized it actually wasn't too crazy to make this work in ipython. Added utils func to load current ipython session and optionally format it and incorporated it into debugger cls, along with a new file_type "ipython session".
        ! -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
            > Interesting, it actually still works (i.e. inserts code into the new cell that appears when we exit the debugger).
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/23/23 thurs
-------------
~ -explore excepthook strategy for intelligent error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    ~ -see what default excepthook does
    ~ -try to reproduce that
    X -add in input asking for y/n
    X -color prompt to stand out from stack trace
    ~ -add in LM query if above is Y
        > Options:
            > 1. just enter debug session
                > Basically same as magic, though faster to enter would be nice. Still, maybe should differentiate this more.
            > 2. try to reuse roboduckdb cls but manually create a query (assume user question will basically be "what caused this error and how do I fix it?")
                > Probably ideal if possible, not sure if it is though.
            > 3. Manually call with jabberwocky.
                > Doable but clunky.
            > 4. Refactor jabberwocky call out of roboduckdb cls and call same helper func here.
                > Maybe the most likely solution ultimately.
            > For now, started with calling pdb.post_mortem with custom roboduck class. Works, though I do notice the answer was not good in this case and it looks like it *might* be caused by a missing global_var rather than just poor GPT.
                > Never mind, the global var is there. GPT is just wrong. Again, don't worry about this too much - chatgpt/codex3 would likely do better here.

2/24/23 fri
-----------
~ -figure out way to avoid making user type query. Recall main options are:
    -make new error prompt and just call gpt manually
    -refactor behavior out of roboduckdb cls
    X -figure out how to call roboduckdb.ask_language_model directly instead of making user do it (perhaps look into what that interact() method called inside post_mortem() actually does)
        > Essentially did this by adding commands to the debugger cmdqueue. This does what I envisioned but now I'm realizing it doesn't give gpt access to the stack trace. Pretty sure that should be possible though.
_ -provide shell shortcut command to launch ipython in mode that enables debug magic
    > Not sure this is actually better. Stick with current method for now.
    _ -implement
    X -update error message in debugger.py
        > Updated to include instructions on how to make setting persist (and to make it auto import the debug magic instead of just configuring ipython so that we're *allowed* to use it if we did import it).
X -allow user to pass kwargs into roboduck()
X -add get_ipython import in magic module (technically unnecessary, I think, but the missing import makes me a bit uneasy0

2/25/23 sat
-----------
~ -easy:
    X -document shell.RoboDuckTerminalInteractiveShell cls (very short, just explaining why this is necessary: ipython frozen attr etc.)
    -finish distill post on augmenting human intellect
    -watch bret victor Future of Programming vid
    -read Engelbart paper on augmenting human intellect
~ -talky errors 
    -goal: avoid user having to type question, while still passing in both traceback/error + state to gpt
        X -dive into traceback.print_exception to see where it's getting the error message + stack trace from
        -update funcs in scratch_errors.py to pass that info down chain of funcs so it can be included in prompt
        -update prompt.yaml or create new one to include (possibly optional) stack trace
        X -print out prompt and confirm it includes everything I want
        X -incorporate y/n prompt from nb01 excepthook (i.e. don't query for EVERY error)
        X -port over logic from htools.autodebug to work in non-ipython contexts too
        -revisit magic and see if we should use this new logic there too (bc it's always used when an error's just occurred)

2/26/23 sun
-----------
-talky errors 
    ~ -goal: avoid user having to type question, while still passing in both traceback/error + state to gpt
        ~ -update funcs in scratch_errors.py to pass stack trace down chain of funcs so it can be included in prompt
            X -write new stack trace debug prompt that includes stack trace var
        -update prompt.yaml or create new one to include (possibly optional) stack trace
        -revisit magic and see if we should use this new logic there too (bc it's always used when an error's just occurred)
X -replace some roboduck names w/ duck (duck is taken on pypi and I'm reasonably pleased w/ roboduck as a lib name, but it would be nice for the natural language debugger to be as easy to invoke as possible and duck is fewer chars)
    > Had to do this in jabberwocky prompts too.
X -generate roboduck mascot/image with midjourney/stable diffusion/dalle 2
    > Could still use some touchups - change green body to yellow, remove mystery object on ground (watering can?)

2/27/23 mon
-----------
X -talky errors 
    X -goal: avoid user having to type question, while still passing in both traceback/error + state to gpt
        X -update funcs in scratch_errors.py to pass stack trace down chain of funcs so it can be included in prompt
            X -debugger cls updates
                X -make cls allow user to specify prompt name (to support stack trace debug prompt) instead of just full=T/F
                X -make sure correct vars are passed in (depending on prompt's available kwargs?)
                    > Along the way, wrote new jabberwocky field_names utils func and added wrapper method to PromptManager. Haven't bumped version and uploaded to pypi yet though, because more changes are probably coming.
            X -update scratch_errors script to specify correct prompt (stack trace version)
        X -update prompt.yaml or create new one to include (possibly optional) stack trace
        > Works, although logic might be a little fragile. Also observed more annoying repetition because I'm limited to 4 stop words and STACK_TRACE adds another (and looks like logit bias isn't as effective as I'd hoped at avoiding LOCAL VARIABLES). But again, don't get bogged down here - chatgpt probably fixes this problem.
-revisit magic and see if we should use this new stack trace prompt there too (bc it's always used when an error's just occurred)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

2/28/23 tues
------------
X -talky errors 
    X -port to lib
    X -document
    X -check works in ipython
    X -check works in python script
    ! -check works w/ 'python -c "my code"'
        > Not yet, can't access source code.
~ -look into debugger bug when running code from terminal with 'python -c {my_code}'
    > Looks like this won't be trivial to solve - can't easily access source code w/ inspect or sys.argv, for example. Might be able to get it from appropriate frame object. Look into this more tomorrow.
X -add error handling to htools autodebug ipython exception registration
-revisit magic and see if we should use new stack trace prompt there too (bc it's always used when an error's just occurred; alternatively, could maybe make prompt depend on flag)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/1/23 wed
----------
~ -revisit magic and see if we should use new stack trace prompt there too (bc it's always used when an error's just occurred; alternatively, could maybe make prompt depend on flag)
    > Wrote new partial stack_trace() to get stack trace as str manually.
    > Added auto flag in magic to allow user to skip the interactive session and just ask the default question.
    > Added require_confirmation flag in excepthook to allow us to skip user confirmation, like in the magic auto mode (bc user already requested it).
    > Tested magic auto flag in jupyter and it works!
    > Still need to add support for using auto mode and insert mode at the same time in magic - currently auto mode does not insert. Could also just not support this and raise warning - could be rather tricky given how differently I implemented the different modes.
-figure out how to access source code when running code from terminal with 'python -c {my_code}'. (Gpt suggests maybe can extract from frame obj.)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/2/23 thurs
------------
X -add support for using auto mode and insert mode at the same time in magic (currently auto mode does not insert. Could also just not support this and raise warning - could be rather tricky given how differently I implemented the different modes.)
    X -look into how difficult it might be to support this
    X -implement (either add support or raise warning)
    > Implemented. Not hard after all - since I managed to reuse debugger cls in errors module, it already sets CompletionCache attr.
    > Also renamed some flags, confirmed they can be called together (e.g. "-ip" instead of "-i -p"), and revised docstring accordingly.
X -filter warning in magic when registering excepthook func due to errors module import
! -figure out how to access source code when running code from terminal with 'python -c {my_code}'. (Gpt suggests maybe can extract from frame obj.)
    > Continued trying but really seems difficult (maybe impossible? Surely there's SOME way but it might not be doable without some non-python hacks, idk). Tried to hack something together to use ~/bash_history but that seems unreliable and it doesn't work anyway.
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/3/23 fri
----------
X -prototype json/yaml version of prompt w/ chatgpt
    > Prototype in roboduck/data/scratch/debug_prompt_yaml_response. Still need to test with api though. I initially wanted json response but chatgpt was having a really hard time keeping python indentation in json - I'm guessin because dumping code to json removes tabs by default and so a lot of code in json training data probably didn't have much indentation. But yaml works pretty well so far and the human readable aspect is actually pretty nice given than the response is basically a bunch of natural language.
    > ðŸ¤” Now I'm wondering if the increased parsability of a highly structured response is worth it - it might require us to wait for the whole response rather than showing live words, and more waiting is annoying. Could put the "solved" field first but the whole point was to try to make it "think" first before reaching a diagnosis.
_ -debug why completions often include leading newline when I thought I stripped them (maybe need to check on first few i rather than just i=0?)
    > Wait on this - would probably become a non issue if I switch to yaml. Same with addressing the repeated newlines at the end of a prompt - gpt3 did this a lot lately but chatgpt shows no signs of doing that.
X -cleanup repo a bit
    > Moved some old scratch code files to new roboduck/data/scratch dir.
-think about some early stage ideas:
    -maybe error module should add gpt response to stack trace like raise RuntimeError(gpt_response) from e?
    -potential logging module: what would this look like? More informative logs would be nice, but maybe this could just entail importing error module and logging the new stack trace that involves gpt's response (see bullet above)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/4/23 sat
----------
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI
~ -think about some early stage ideas:
    ~ -maybe error module should add gpt response to stack trace like raise RuntimeError(gpt_response) from e?
        > Took first stab at implementing this but there are still some possible kinks to work out. I tried to make post_mortem do this but I realized in error mode, we still require the user to hit -y so it's hard to test automatically. Also, sys.last_value etc doesn't seem to be getting updated anymore - maybe bc the default excepthook does that somewhere and we overwrote that?
    ~ -potential logging module: what would this look like? More informative logs would be nice, but maybe this could just entail importing error module and logging the new stack trace that involves gpt's response (see bullet above)
        > Looked into logger module a bit. A few ways to do this (custom logger, custom LogRecord, etc.) but I think a better way might be to start by having our custom error mode update the exception obj to include the explanation so it automatically gets logged.
X -change or document that question is ignored by default in post_mortem
    > Was going to remove but realized prompt is still a WIP and we can specify different prompts, so it's plausible that a future change could make this useful. Just documented.
X -document debugger precmd method
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/5/23 sun
----------
X -Look into error when importing roboduck.errors in ipython session without special flag (AttributeError: module 'sys' has no attribute 'last_type'. Caused by the stack_trace partial def. Maybe just wrap that w/ better error message?)
    > Change from partial to function with better error message. Sys.last_type is often not defined at import time and partial definition requires it to exist already.
X -exploring support for auto updating exception str content after writing completion
    _ -look into way to allow user to set requires_confirmation=False (for possible logging in production as opposed to interactive dev)
        > I think the main use case is for logging, so maybe just make a custom logger that works a bit like the jupyter magic and calls excepthook manually.
    X -look into how sys.last_value etc. are set. Maybe I can manually update that rather than my current exc.args mutation in post_mortem? Prob would be ideal to do this in the debugger cls rather than postmortem bc postmortem only affects error mode
        > I set these 3 values in sys.excepthook which seems to be where this occurs normally (indirectly, in a called function). I update the error message in post_mortem, not debugger cls, because exiting debugger sets pdb related values for sys.last_type etc anyway.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/6/23 mon
----------
~ -logging module
    X -write errors.enable() func
        > Decided I didn't agree with yesterday's decision to avoid this, I think it as mostly out of a desire to avoid refactoring errors module. Rewrote this and confirmed it works in ipython.
        > Also let user pass in Pdb cls.
    X -look into base logger class to see what options are for where to slot in the gpt completion
    X -select first option to try (can always revisit decision if it ends up not working as nicely as expected)
        > Override _log in Logger subclass.
    ~ -implement
        > Sort of works, but sys.last_value etc aren't getting updated. Need to debug further.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/7/23 tues
----------
~ -logging module
    X -investigate why sys.last_value etc aren't being updated when using new logger cls in logger.py
        > Actually, issue was that logger called my excepthook with sys.last_value etc and that was wrong because it now only gets updated INSIDE the excepthook itself. Managed to extract those args from the exception obj.
    X -fix
    > Parametrized sleep between chars in debugger and added option for silent output. Logger seems to work, though could use a couple refinements/default tweaking.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/8/23 wed
----------
X -document new silent param in debugger cls (and possibly in errors module kwargs?)
X -logging module
    X -custom logger seems to lose some default formatting, e.g. starting messages with time or INFO/WARNING/etc. See if we can fix that.
    > Also added similar functionality as MultiLogger to make it easy to configure logging to stdout and/or file.
X -removed old commented out quickmail func version from htools
    > Also uploaded 7.5.0 to pypi.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/9/23 thurs
------------
X -easy:
    X -document logger cls + init
    X -document other classes
~ -start playing around with langchain
    > Realistically, I should probably eventually replace my jabberwocky usage with langchain. Don't do that yet though - go through some more tutorials and toy around a bit more to make sure it will be able to do the things I want.
X -medium:
    _ -create a more minimal install for jabberwocky (no youtube transcription stuff, for example)
        > Hold off while considering move to langchain.
    X -touch up roboduck mascot image
        ~ -change green body to yellow
        _ -remove mystery object on ground (watering can?)
        > Experimented a bit in gimp. I'd need to get a lot better to really do what I want, and I'm running into a known fuzzy select bug on mac. Made a more blue and yellow variant but the original is kind of growing on me. Leaning towards just keeping it.
        > Also made a more grayish version online. Now I'm kind of digging the blue one. And I realized the watering can thing may be supposed to be something a mechanic would use to oil robots? That sort of makes sense. Changed readme to use blue version for now, will see if I like it.
-hard:
    -jabberwocky changes to support chatgpt
        -think openai lib version needs to be updated, make sure things stil work afterwards
        -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
        -update enginemap engines if necessary
        -update GPTBackend.query if necessary
        -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
        -try out new debug prompt (data/scratch/) from api instead of UI

3/10/23 fri
-----------
-play around with langchain more with eventual eye towards possibly replacing jabberwocky
~ -start adding tests
    X -pick framework (prob pytest)
    X -look up examples
    ~ -write tests
        > Was starting to question if I really want to do this but testing first module already paid off - second half of truncated_repr seemingly got lost when copy/pasting from jupyter to lib ðŸ˜³. Seems fixed now.
    -add CI/CD on push/merge?
~ -brief glimpse of old reload.py script
    > Lots of todos here, still very WIP. Think it is worth trying to scavenge some things from this, but could consider designing new solution first and then seeing how it differs. Might find something better or more elegant, like how my autodebug module worked way better by starting over.
-passive:
    -finish distill post on augmenting human intellect
    -watch bret victor Future of Programming vid
    -read Engelbart paper on augmenting human intellect

3/11/23 sat
-----------
X -peruse saved git repos around auto refactoring, stack overflow errors, live program tracing
    > Livepython tool is interesting, seemingly executes code line by line and lets us pause in between. Seems like it might allow for something like: try to execute this line, if it fails debug it and try to fix it, then run the fixed code. But maybe it's unnecessary - sys.excepthook already executes on errors, and sys.settrace seems very problematic in ipython.
X -func to color new parts of a str differently (similar to colordiff)
    > Still need to incorporate this into...somewhere. Might lend itself better to a json/yaml response where we know exactly where the code starts and ends. (Granted, we could probably ask gpt to add a field called "diff" but I guess that introduces more chances of inconsistencies.)
-continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/12/23 sun
-----------
X -consider where (and how) we might incorporate new colordiff func
    > Errors module (enabled by default) and logging module (disabled by default). Not compatible with live typing and probably not ideal when logging to file, but works nicely in error message. Note that both of these modules previously ignored code completion entirely, but I decided it's worth keeping.
    > Note: do I need to update response color in debugger and/or errors module? I realized whole response is currently printed in green, hiding the code diffs. It's visible later when logging/printing the exception. But I guess diffs aren't supported for live typing anyway. Could enter color mode once we hit SOLUTION PART 2, I suppose, but that feels like it will require some custom queue/trie logic that will probably disappear once I update the prompt anyway. Hold off for now.
    > Ended up adding a bunch of new attrs to cache, which does make it feel more useful. Also added a reset() method to set all of them back to empty strs, which magic does.
-continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/13/23 mon
-----------
~ -continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
    > Messed around a bit with basic chains, tuned my cs agent -> PR -> json pipeline a little bit, tried writing a chain that parses a hopefully json str response to a dict, toyed around with some Memory implementations (perhaps less necessary due to chatGPT, though still useful for long conversations).
    > One learning: a Chain actually seems to be somewhat synonymous with the heavier weight Prompt class I envisioned for jabberwocky 3.0. E.g. you import an entirely different class to perform a Summarization task. Don't necessarily love that but I guess it helps define pre/postprocessing logic (but still...couldn't you do something like Task('summarize') and then have Task create a Summarize obj? Guess that's similar functionally but the UX feels a lot nicer to me. No forest of imports.).
    > Looked a little bit at async stuff too but didn't try this yet. For some reason I'm kind of surprised they didn't go with threads - seems like using async has major repercussions bc IIRC async funcs must be called by other async funcs, and in my experience they can be quite finicky when working with CLIs (or maybe that's fire-specific).
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/14/23 tues
------------
~ -continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
    X -vector DB/doc search
    X -youtube
    > Spent >2 hours playing around with this, lots of fun. Doc search may be less necessary now given increased context length, but still a nice cost saving measure AND gives potential to scale up to more ambitious use cases, perhaps.
    -agent
    -tracing
    -chatgpt (messed with this a little bit but not enough, only tried single turn)
-consider: 
    -how much time/effort to replace jabberwock with langchain in roboduck?
    -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
    -key question: whether jabberwocky or langchain, do I want to sacrifice live typing for cleaner structured yaml/json output?
        -and do I want/need to change davinci to a chat model? RLHF probably improves quality quite a bit.
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/15/23 wed
-----------
 X -continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
    X -agent
    ~ -tracing
        > Read through demo nb, looks like in practice this just refers to running an agent (or perhaps a generic chain) with verbose=True. NB doesn't actually import anything from a tracing module.
    ~ -index (is this that different from the doc search? Maybe a way to speed that up?)
        > Seems like index is the higher level api wrapped around document stores/vector dbs. Read through demo nb but no need to run code rn.
    X -chatgpt (messed with this a little bit but not enough, only tried single turn)
        > Since I've used their chatmodel a bit at this point, I took the opportunity to lump this in with exploring their streaming capabilities and so far it's not super encouraging - not really built to yield values.
        > Actually maybe it's fine, just a different interface. Should be able to implement whatever logic you want in the on_new_token cls, presumably - may just need to pass in more vars from global state.
    > Think we've done enough langchain aimless exploration for now. Starting tomorrow we need to consider whether to rewrite roboduck with this or not and whether to use this in next part of project (or next project entirely). Currently leaning towards just keeping jabberwocky for existing roboduck stuff and then using next proj as an opportunity to transition to langchain. The main drawback is that jabberwocky doesn't currently support chat models and those would probably be a lot better than what I'm currently using. But I think we could get 99% of the way their with the 3.5 models and perhaps a hacky use of ConversationManager for the NL debugger. Errors/logging/magic prob don't really need any conversational state, really.
-consider: 
    -how much time/effort to replace jabberwock with langchain in roboduck?
    -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
    -key question: whether jabberwocky or langchain, do I want to sacrifice live typing for cleaner structured yaml/json output?
        -and do I want/need to change davinci to a chat model? RLHF probably improves quality quite a bit.
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/16/23 thurs
-------------
X -add getLogger() func to logging module to match builtin logging interface
X -rename RoboDuckDB to DuckDB in all modules
X -document lib modules 
X -add missing notebook summaries
X -make readmes
X -cp langchain colab to scratch nbs in repo
    X -rm ALL api keys
    X -download and move to repo
-consider: 
    -langchain usage
        -how much time/effort to replace jabberwock with langchain in roboduck?
        -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
        -confirm how easily jabberwocky works w/ 3.5 models (at least prob should update some name maps in EngineMap? And support for engine int values will probably start fading away)
        -briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
    -do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
        -consider tradeoffs
        -make decision
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/17/23 fri
-----------
-consider: 
    X -langchain usage
        X -how much time/effort to replace jabberwock with langchain in roboduck?
            > Changes would at least be pretty well contained to the debugger module (the debugger class, in fact). Not huge lift I suppose. The bigger issue may be that langchain seems to define jabberwocky prompts as classes that need to be imported and perhaps instantiated with different kwargs (?). Probably could recover my interface if I really wanted and just do something like getattr(langchain.prompts, name) or whatever their current interface is. I suppose I'd probably need to implement some custom prompts there but that's not a huge lift.
            > Implementing streaming their way might be a little bit annoying - maybe it would be fine but I could imagine having to implement a different callback for each module that uses a prompt (maybe?). Again, probably not that bad given how self contained the debugger module/cls are.
            > Estimate: might add on the order of 1 month? With a pretty wide prediction interval around that, say 1-12 weeks.
            > Buuuut if I don't do this I may need to update jabberwocky to work w/ chatgpt, gpt4, etc. That might take a similar amount of time if not longer.
        -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
            > Pretty sure but not certain that streaming would still work. Pretty sure but not certain that my interface of passing prompt name to debugger class or calling functions would work (though in practice the user should probably never do that).
        X -confirm how easily jabberwocky works w/ recent openai updates
			X -3.5 models
			X -4 models
			X -chat models
			> Details in nb. Not supported but not too crazy to add that.
        -briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
    -do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
        -consider tradeoffs
        -make decision
-confirm if I remove duck() call from source code
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/18/23 sat
------------
X -confirm whether I currently remove duck() call from source code in debugger cls source code loading
	> Looked through code briefly and didn't see it. Confirmed by testing dev mode in ipython shell, probably more reliable/comprehensive check than reading code. Uncovered new issue that updated openai api breaks jabberwocky, though.
X -if necessary, remove duck() call from source code (full and/or partial, as necessary)
	> Confirmed works in ipython.
X -look into newly discovered openai error on query
	> Seems to be resolved by restarting ipython. Guessing that was a long running session and autoreload didn't work, just like in jupyter. I imported jabberwocky afterwards in the same new session (after importing openai and confirming that query worked) and querying with it worked so I think we're good.
-consider: 
	-briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
	-briefly try out new debugging prompt in langchain. How to do this, how much work?
	-make decision on first approach to work on: expanding jabberwocky functionality or replacing jabberwocky usage w/ langchain in roboduck
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/19/23 sun
-----------
X -briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
	> Does seem promising. Some kinks to work out around when to pass in code context vs. just user question, when to use conv manager vs prompt manager vs actual chat model, also need to support new 3.5 and 4 model names. But I like the experience of being able to question gpt about its solution, which requires us to maintain some state.
~ -consider: 
	-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
	-briefly try out new debugging prompt in langchain. How to do this, how much work?
	X -make decision on first approach to work on: expanding jabberwocky functionality or replacing jabberwocky usage w/ langchain in roboduck
			> Start w/ updating jabberwocky, I guess. Would be nice to have that up to date for alexa too, potentially.
~ -jabberwocky updates
	X -support 3.5 engine names
	X -support 4.0 engine names
	>>> Tentatively added support but realized those new models don't seem to support the completions endpoint, only chat. Could get a little hairy auto-delegating to them.
	-support chatcompletion usage
		-via convmanager?
		-or via new cls?
		-or leave out of jabberwocky but use native openai implementation in roboduck?
		-rename gpt3 refs to gpt?
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/20/23 mon
-----------
 -consider: 
	-how to support chatcompletion usage in jabberwocky (recall 3.5/4/turbo models do not support completions endpoint after all, just chat)
		-via convmanager, delegate to another helper method when recognizing a chat-like model name in kwargs?
		-or via new chatmanager cls?
		-or leave out of jabberwocky but use native openai implementation in roboduck?
		-or leave jabberwocky for older versions of openai and bite the bullet and shift over to langchain now
	-rename gpt3 refs to gpt? (If so, remember to update both lib and alexa. Ignore gui)
	-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
	-briefly try out new debugging prompt in langchain. How to do this, how much work?
X -add CI/CD to run tests on push/merge?
	> Added yml file. Not sure yet why not running.
	> $default-branch variable didn't work. Needed to hardcode to 'main'.
	> Build passes in <2 minutes. Will get slower as (if?) I add tests. But I'll probably also pare down requirements which should help speed things up a little. Also could consider pushing to dev branch and only merging periodically since it kind of annoys me that it runs tests on every single push atm.
X -flatten dir structure

3/21/23 tues
------------
 ~ -consider: 
	X -how to support chatcompletion usage in jabberwocky (recall 3.5/4/turbo models do not support completions endpoint after all, just chat)
		! -via convmanager, delegate to another helper method when recognizing a chat-like model name in kwargs?
            > Bc of how we select query_func in GPT.query, that doesn't quite seem like the right UX. GPT._get_query_func would need to select different func based on model name (probably), OR make query_gpt3 func perform the delegation to chat_model. Would need to repurpose prompt param for `messages` which would be a little weird and not sure if all params carry over. Thinking it might be cleanest to just leave jabberwocky as is supporting pre-chat models and move to langchain from now on? (Though really, most if not all queries here COULD be done w/ non-chat models. But the more powerful models seem to be only available as chat endpoint so it's probably still very much worth using them even if the stateful aspect is relatively unimportant.)
		! -or via new chatmanager cls?
            > I guess we could do this but I think at this point it would be easier to just ignore existing jabberwocky stuff and wrap openai ChatCompletion itself, in which case we might as well not even put it in jabberwocky.
		! -or leave out of jabberwocky but use native openai implementation in roboduck?
            > Possible, but using roboduck as a reason to get more familiar with langchain isn't such a bad idea.
		X -or leave jabberwocky for older versions of openai and bite the bullet and shift over to langchain now
            > Let's at least give a custom prompt/chain a quick try.
        > Leaning no chat support in jabberwocky. Langchain or something similar will probably be big, no use resisting.
	-rename gpt3 refs to gpt? (If so, remember to update both lib and alexa. Ignore gui)
	-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
~ -briefly try out new debugging prompt in langchain. How to do this, how much work?
    > Not to bad to get the basic mechanics working, but haven't really gotten into refactoring or making the devX nice yet. Prob should to assume we'll just always use chat models at this point - from a tiny sample, turbo's outputs do seem like they might be a bit easier to parse reliably than davinci's.
-check relative prices of new models
    > turbo is 0.2 cents per 1k tokens, equivalent to curie or 1/10 of davinci or 5x ada. Not too bad. Gpt4 is pretty expensive though, 3 cents per 1k prompt and 6 cents per 1k completions so ~20+ x turbo or 2x davinci. Takeaway: basically never makes sense to use davinci?
X -upgrade env to use py 3.8
    > Have to use py38 conda env now to get new-ish version of langchain. Also updated github workflow to py 3.8.

3/22/23 wed
-----------
X -revert jabberwocky to prev commit (before enginemap changes and version bump). Rules still seem to apply to completion endpoint.
~ -explore possibility of 2 diff types of user messages - 1 with full context, 1 with just question
    > Feel like if we're in debug mode and user asks a followup question, we don't need to waste tokens providing so much info again. Maybe can check if we're in the same frame as before to infer this.
    > Implemented part of this, the other part is that I need to figure out how user should specify the two types of messages or if they want to use 2 types. Ideally we'd probably have 1 file defining each prompt, same as I do in jabberwocky, but allow user to specify multiple types of human messages.
    > Whoops, actually not quite right. Frame only changes when a function/method is called. Hash kwargs instead. This at least makes the next_line field handling a little easier since I'm back to calling get_prompt_kwargs on every question.
~ -explore possibility of adding "prev_line" or "next_line" field to prompt kwargs
    > Finally managed to extract this, though logic might be a little fragile (?). Not the most important field but might be nice. But atm it's inside get_prompt_kwargs which doesn't execute if prev_frame_id is unchanged. Maybe that's ok though, since if any state changes I believe the frame will too?
    > Update: now using hash(kwargs) instead of id(frame), so above concern is not an issue.
-continue nb04 langchain
    -figure out a good way to define multiple chat prompt variants (e.g. py file, yaml file, what fields are needed, etc. Take inspo from langchain native method since they provide some out of the box)
    -Q: what is difference between what I do in nb04 manually appending to messages list vs what I did in nb02 with ChatHistory obj? Is that basically doing the same thing under the hood?
    -where to provide stop words? And would it be useful/effective to ask gpt to provide specific stop word when done to help it stop when appropriate?
    -Consider breaking down into chain? (Might be easier than trying to get it to generate valid YAML/JSON, also easier to parse. Could even do this concurrently, though prob better to not do that so code solution can see explanation first.)
        -EXPLANATION -> CODE
        -fancier: THOUGHT PROCESS -> EXPLANATION -> CODE

3/23/23 thurs
-------------
~ -continue nb04 langchain
    ~ -figure out a good way to define multiple chat prompt variants (e.g. py file, yaml file, what fields are needed, etc. Take inspo from langchain native method since they provide some out of the box)
        > Trying out py file, worried yaml would get messy with multiple user message templates needing to be indented. Not exactly loving python solution either so far but let's give it a shot for a bit longer.
        > Also started implementing wrapper cls. Maybe langchain already has something like this but I couldn't find it. Prob could gpt doc search the source code but /shrug. Still some stuff left to implement and UX could use a couple tweaks, but it's getting there.
-[BLOCKED: need to figure out how to define chat prompts in yaml/py/etc file first] debugger cls: change to expect chat prompt which (optionally?) has 2 diff types of user messages - 1 with full context, 1 with just question
    -consider how to let user specify these two types
    -implement (prob need to update how field_names are parsed in init and how they're used in ask_language_model)
    -Q: what is difference between what I do in nb04 manually appending to messages list vs what I did in nb02 with ChatHistory obj? Is that basically doing the same thing under the hood?
    -where to provide stop words? And would it be useful/effective to ask gpt to provide specific stop word when done to help it stop when appropriate?
    -Consider breaking down into chain? (Might be easier than trying to get it to generate valid YAML/JSON, also easier to parse. Could even do this concurrently, though prob better to not do that so code solution can see explanation first.)
        -EXPLANATION -> CODE
        -fancier: THOUGHT PROCESS -> EXPLANATION -> CODE

3/24/23 fri
-----------
-nb04 langchain: Chat class improvements
    _ -maybe specify model cls in config?
        > No, this is more of a runtime thing than a prompt thing. Let user specify it in init though.
    X -consider how user should pass model to chat class (obj or kwargs to instantiate it? Leaning obj and let classmethod construct obj first)
        > Current solution is pass in chat class and kwargs and instantiate inside.
    -figure out how user can define their own prompts without changing roboduck lib
    X -check if completion kwarg names need to change (just using jabberwocky names for now)
        > max_length -> max_tokens
        > Bit of a headache dealing with chat kwargs and model_kwargs arg to chat constructor. Seems like there's some magic going on that might let us pass everything in unpacked though, can look into tomorrow.
    X -check where to pass in completion kwargs (presumably in chatopenai constructor? But that also accepts something called model_kwargs separately, and IIRC `stop` or something similar might be specified elsewhere.)
         > Yes. `stop` can be passed in to __call__ but if not it falls back to model_kwargs. (If both, it throws an error rather than overriding - I'm used to the latter but I guess this works too.)
    -consider alternatives for how to select user message type (not loving key=x as first param. Also makes helpful fields in docstring harder.)
    X -write dummy chat model class to save $
    X -test end to end
        > New changes look good, though will need to do this again as I add more features.
    ~ -if I like this approach, add some other nice features:
        ~ -explore possibility of injecting available template fields into docstring
            > Prototyped an option that seems to work well. Still need to decide which interface I prefer: 1 reply method where docstring shows default user message fields, or 1 reply method for each user message type. Or both?
        -or at least make list of available fields easily viewable
        -debug mode (just view latest prompt)
        -docstrings
        -save/load transcript?
    -if I like this approach, port other jabberwocky debug prompts to roboduck

3/25/23 sat
-----------
~ -nb04 langchain: Chat class improvements
    X -consider alternatives for how to select user message type (not loving key=x as first param. Also makes helpful fields in docstring harder. At the very least, mangle name 'key' to allow prompts to define fields with that name.)
        > Got both working! A little hairy for a while but I think it's all good now. Bigger chunk of work than it looks here.
    -figure out how user can define their own prompts without changing roboduck lib
    -consider how to replace importlib code when loading prompt
    -test end to end
    -if I like this approach, add some other nice features:
        -debug mode (just view latest prompt)
        -docstrings
        -save/load transcript?
    -if I like this approach, port other jabberwocky debug prompts to roboduck

3/26/23 sun
-----------
~ -nb04 langchain: Chat class improvements
    X -figure out how user can define their own prompts without changing roboduck lib
        > Decided yaml specification would be a lot nicer that importlib.importing custom user py files. Multiline strings in nested fields actually work fine in yaml, just have to be mindful of indentation (that defines when str ends). Also updated manifest and setup.py to include yaml files (untested yet on fresh install, but works locally) and wrote new load_template and available_templates functions. Also updated chat class to use config approach. Also found "SOLUTION PART 2" was incorrectly included in roboduck version of debug prompt, removed it (jabberwock version is correct).
    X -consider how to replace importlib code when loading prompt
    X -simplify chat_kwargs vs model_kwargs parsing in chat cls (think not actually necessary)
    X -test end to end
    -if I like this approach, add some other nice features:
        -debug mode (just view latest prompt)
        -docstrings
        -save/load transcript?
    -if I like this approach, port other jabberwocky debug prompts to roboduck

3/27/23 mon
-----------
~ -nb04 langchain: Chat class improvements
    X -confirm stopwords are actually working (noticed yesterday I had "SOLUTION PART 2" in config which actually is supposed to be generated, given my current setup)
        X -where is this specified in langchain?
            > ChatOpenAI lets you pass it in to __call__ which passes it to _create_message_dicts. If you do that, you can't provide it at instantiation time. Other option is to pass it into init, in which case it ends up in _default_params which is used in _create_message_dicts.
        X -am I able to truncate solution very early (temporarily) by passing in a stopword str like SOLUTION PART 1?
            > Yes, specifying it at chat instantiation time.
    -chat cls improvements
        _ -debug mode (just view prompt, don't actually call api)
            > Skip for now. Might be slightly tricky to add as param to dynamically generated reply classes (think that should be supported already but have to check). Maybe this is really just equivalent to calling history() + new_message though?
        -get streaming mode working
            X -get messy version working
                > stdoutstreamer wasn't printing anything, realized it's because we have to set chat(verbose=True) for it to print anything. Hardcoded verbose=True for now; can consider if we want to allow user to disable this later.
            ~ -incorporate into class def
                > verbose setting is updated, but maybe provide default callback_manager arg, either in init signature or body. Need to think carefully how to handle this bc it can appear in chat_kwargs.
        -docstrings
        -save/load transcript?
    -port classes/funcs from nb04 to lib
    -port other jabberwocky debug prompts to roboduck
-add make commands
    -make prompt
    -make chat_prompt

3/28/23 tues
------------
-nb04 langchain: Chat class improvements
    -chat cls improvements
        -maybe add stdout streaming by default to cls? (could provide default callback_manager arg in init signature or body. Need to think carefully how to handle this bc it can appear in chat_kwargs. Default also depends on how customized I want this to be to roboduck - this affects both WHETHER we print by default and what handler cls we use. ATM IIRC I print response in diff color than user text, for example.)
        X -write callback that prints to stdout on new llm token with desired formatting (i.e. in color)
        X -add live typing functionality
        X -confirm if we need to change logic around extracting info from reply template
            > Yes we do. Python code seems to be helpfully enclosed in triple backticks more consistently. But SOLUTION PART X is no longer a single token, meaning I probably need to revert to my old more complex logic streaming from queue.
        ~ -implement changes if necessary
            > Options:
                > Use modified version of jabberwocky.stopwordstreamer (modified bc we don't want to stop streaming when we hit these phrases, just skip printing).
                    > Probably not much easier than just writing from scratch. Might get something cleaner from scratch anyway bc jabberwocky class was designed for a slightly different use case.
                > Try to find old version of this code in roboduck git history.
                    > Possible, but IIRC it got rather messy.
                > Write from scratch.
                    > Possible, it doesn't sound too bad and honestly would be fun, but I think I expected it to be simple last time and it still got messy. So probably would happen again, guessing it's more complex than I'm realizing atm.
                > Move to json/yaml response for easy parsing.
                    > Really like this mode of interaction but waiting for it to finish before typing anything would be a big loss of functionality.
                > Move to prompt chain that first asks just for natural language solution, then just for code solution.
                    > This does seem somewhat promising. My one concern is if chat and chaining are compatible entities in langchain. I'd assume so but I'm not sure - it seems like the assumption is that a chat is between a model and a user, so I don't know what happens if a bot message is last. Probably fine though.
                    > Seems like we have a few options here. 1) Maybe langchain.SequentialChain provides what I want? Need to test, unclear. 2) Subclass Chain to do what I want, probably very similar to SequentialChain. 3) Update my prompt/chat class to do this without chains. Concretely all we want to do is request two completions in a row, where the second one does not ask for any user input.
                    > Idiomatic langchain would probably use 1) or 2). Let's try 1 tomorrow and fallback to 2 if necessary.
                > Adjust system prompt instructing gpt to use unusual sep sequence to mark start and end of each section.
                    > This does seem like a reasonably simple option. Maybe a tad less generalizable than the above (picture a reply with 10 parts. As a 10 step chain, it should be easy to store the results of each section separately. With this approach, it gets a little hairier - do we access each result by index? If one section is missing, is it straightforward to tell which?
        -docstrings
        -save/load transcript?
    -port classes/funcs from nb04 to lib
    -port other jabberwocky debug prompts to roboduck
-add make commands
    -make prompt
    -make chat_prompt

3/29/23 wed
-----------
-nb04 langchain: Chat class improvements
    ~ -streaming support w/ gpt3.5+ (recall old solution relied on gpt3 tokenization which apparently changed ðŸ˜­)
        _ -try to implement desired behavior w/ chatmodel and SequentialChain (i.e. first prompt just asks for NL explanation, 2nd just asks for code)
            > Looked through code and it's not quite what I want.
        ~ -if above fails, try implementing custom cls. Prob very similar.
            > WIP. Finally getting it to do roughly what I want but it's not clear to me that the UX is actually any better. Need to refactor a bit, then try w/ chat models. Can use actual debug prompt as test.
        -if both above approaches aren't good for some reason, choose between 1) queue-based lagged streaming approach like before, still 1 prompt OR 2) figure out how to manually get multi-part completion by changing config and/or chat cls. (Don't choose now bc both of these approaches are rather complex, need to think through them a lot, and I suspect I can and should get one of the prev 2 options to work.)
    -docstrings
    -save/load transcript?
    -set streaming=True by default in chat cls? (could provide default callback_manager arg in init signature or body. Need to think carefully how to handle this bc it can appear in chat_kwargs. Default also depends on how customized I want this to be to roboduck - this affects both WHETHER we print by default and what handler cls we use. ATM IIRC I print response in diff color than user text, for example.)
    -port classes/funcs from nb04 to lib
    -port other jabberwocky debug prompts to roboduck
-add make commands
    -make prompt
    -make chat_prompt

3/30/23 thurs
-------------
~ -nb04 langchain: Chat class improvements
    ~ -custom chain cls
        X -streaming support w/ gpt3.5+ (recall old solution relied on gpt3 tokenization which apparently changed ðŸ˜­)
            ! -translate debug prompt into a series of chains (don't worry about config yet, just hardcode)
                > Was really struggling with this and finally wrote down what I wanted to do and realized this approach will be WAY slower and more expensive, O(N^2) wrt number of sections in output vs O(N) (or O(N) rather than O(1), depending on how you look at it). Combined with how un-user-friendly I'm finding the process of trying to define this, I think it's time to explore another option.
            _ -try multi-part debug prompt (above bullet) w/ chatOpenAI (prev only tested this class w/ regular LLM)
            _ -if promising, consider how to specify these types of responses in config
            -if not, revert to a different approach, 1 of:
                _ -use more pretty section titles and just display them to user (then str/regex afterwards to extract fields)
                X -use specific sep char and omit (hard to pick something that def won't show up in code though; use str/regex afterwards to extract fields)
                    > Just used newline and asked for section 2 to be in one pair of triple backticks.
                _ -make NL and code requests concurrently, stream NL but not code (lose potential chain of thought type benefits though. These answers should be dependent, don't want inconsistent answers.)
                _ -queue-based lagged streaming like initial approach
                _ -implement "followup question" in chat class/config (currently a bit fuzzy on how this would work exactly)
            > Revised prompt a bit to not ref section titles, moved some instructions about language to system message, more explicit instructions about formatting with backticks, wrote func to extract code from response. Also fixed bug where logit bias was wrong bc tokenizer changed (ended up removing entirely here bc 3.5+ models are better), tweaked some other settings to reflect these better models.
        X -integrate streaming callback
            > Need to pass streaming=True in as a chat_kwarg, set verbose=True, etc to get this to show up.
            > Working nicely now. I thinnk we're pretty much ready to start porting and incorporating into debugger cls.
    -docstrings
    -save/load transcript?
    X -set streaming=True by default in chat cls? (could provide default callback_manager arg in init signature or body. Need to think carefully how to handle this bc it can appear in chat_kwargs. Default also depends on how customized I want this to be to roboduck - this affects both WHETHER we print by default and what handler cls we use. ATM IIRC I print response in diff color than user text, for example.)
    -port classes/funcs from nb04 to lib
    -port other jabberwocky debug prompts to roboduck
-add make commands
    -make prompt
    -make chat_prompt

3/31/23 fri
-----------
~ -nb04 langchain: Chat class improvements
    X -custom chain cls
        X -consider making streaming/verbose attr set in init optional
            > Make streaming a separate configurable attr, rm verbose call bc not actually necessary (since callbacks can set always_verbose=True).
        X -enable streaming mode for dummychatmodel
            > More involved than expected, but it works now.
    -port all funcs/classes in nb04 to lib
        -port
        -[maybe wait] docstrings
    -port other jabberwocky debug prompts to roboduck
    -check for necessary updates in:
        -debugger module (main pdb subclass, cache)
        -errors module
        -logging module
        -magic module

4/1/23 sat
-----------
~ -nb04 langchain: Chat class improvements
    X -custom chain cls
        X -check if we can condense chat_kwargs and model_kwargs in constructor (think this might actually work and interface would be a lot nicer - don't like having to pass in model_kwargs as a dict)
            > Yes :)
    ~ -port all funcs/classes in nb04 to lib
        ~ -port
            > Ported callback and dummychatmodel classes, add_kwargs and extract_code functions.
        ~ -[maybe wait] docstrings
            > Documented the 4 classes/functions above reasonably thoroughly.
        > Just have Chat cls left. Maybe need to allow user to specify a custom `extract_code` function though - this one is fairly generic but users might want to do something more wild and I think I'd like to support that.
    -port other jabberwocky debug prompts to roboduck
    -check for necessary updates in:
        -debugger module (main pdb subclass, cache)
        -errors module
        -logging module
        -magic module

4/2/23 sun
-----------
-nb04 langchain: Chat class improvements
    X -allow prompt config and/or chat class to accept custom extract_code func?
        X -first consider use case
            > I (or user) might want to define a custom prompt that returns output that is formatted differently or has additional fields. E.g. returning json/yaml that also includes a boolean field is_solved. Easiest way to achieve this, I think, is to have a function that accepts the generated text as input and outputs a dict. We can use each kv pair to set an attr in completion cache (or do something similar to that).
            > Options for how to specify this function:
                > python snippet embedded in config yaml
                    > This could get a bit gnarly. No.
                > name of function or file containing function in yaml
                    > Possible. Or maybe should have each prompt be a directory containing both a yaml file and (optionally?) a python file.
                > don't specify it in config, leave it to user to pass in a function directly debugger class or something
                    > Don't love this, seems like it should be baked into config.
                    > Never mind, this doesn't belong in config, it's too specific to this use case. Might make sense to provide a set of post-completion asserts or postprocessing steps, but that's a bit different.
                > I guess this is more a property of the debugger cls than the prompt or chat. When we instantiate it, we could raise a warnning if a non-standard prompt is provided but a custom extract_attrs function is not.
        X -if above seems reasonable enough, figure out how to do this
        _ -implement
            > Not yet, do this at the debugger cls level as described above.
    X -port chat class in nb04 to lib
        X -port
        ~ -[maybe wait] docstrings
            > Documented the big stuff so I won't forget. Maybe return to the helper methods later.
    -port other jabberwocky debug prompts to roboduck
    ~ -check for necessary updates in:
        ~ -debugger module (main pdb subclass, cache)
            > WIP, non-exhaustive TODOs are sprinkled throughout file.
        -errors module
        -logging module
        -magic module
-reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/3/23 mon
-----------
-nb04 langchain: Chat class improvements
    -check for necessary updates in:
        -debugger module
            -pdb subclass
                X -update how field_names is set
                    > Wrote new chat method in the process and debugger helper method. Also changed how field names are retrieved and used by debugger cls.
                X -rm popping next_line from kwargs bc it IS now supported (in the 1 prompt I ported over to langchain, at least)
                -pass necessary kwargs to self.chat instantiation and update debugger init signature accordingly
                -enable self.silent mode w/ chat class (maybe can just set stream=False)?
                -allow passing in custom extract_kwargs func to map from completion -> cache (or should this be done elsewhere? Probably not)
            -update completion cache class if necessary
            -look for other todos in file
        -errors module
        -logging module
        -magic module
-reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/4/23 tues
-----------
-nb04 langchain: Chat class improvements
    -check for necessary updates in:
        -debugger module
            ~ -pdb subclass
                X -pass necessary kwargs to self.chat instantiation and update debugger init signature accordingly
                X -enable self.silent mode w/ chat class (maybe can just set stream=False)?
                X -enable printing resolved prompt in dev mode
                    > Maybe should move to a public method in chat cls though? Currently debugger cls calls chat internal method.
                -allow passing in custom extract_kwargs func to map from completion -> cache (or should this be done elsewhere? Probably not)
                -consider how we might add support for multiple reply types (perhaps user could define a number of prefixes like my use of [dev] to determine this?)
            -update completion cache class if necessary
            -look for other todos in file
        -errors module
        -logging module
        -magic module
-reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/5/23 wed
----------
-check for necessary updates (from langchain migration) in:
    ~ -debugger module
        ~ -pdb subclass
            X -allow passing in custom extract_kwargs func to map from completion -> cache (or should this be done elsewhere? Probably not)
                > Wrote new parse func in utils, kept "explanation" parsing logic very simple. We'll see how it does.
            -consider how we might add support for multiple reply types (perhaps user could define a number of prefixes like my use of [dev] to determine this?)
        X -update completion cache class if necessary
        -update duck func
        -look for other todos in file
    -errors module
    -logging module
    -magic module
-reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/6/23 thurs
------------
X -easy: document utils.parse_debug_response
-check for necessary updates in:
    ~ -debugger module
        X -consider better ways to specify defaults in cache cls so resetting them is cleaner
            > Wrote cls decorator that accomplishes this. Reset method is much prettier now.
        -pdb subclass
            -consider how we might add support for multiple reply types (perhaps user could define a number of prefixes like my use of [dev] to determine this?)

        -look for other todos in file
        -update duck func to match new signature
    -errors module
    -logging module
    -magic module
-reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/7/23 fri
----------
~ -check for necessary updates in:
    X -debugger module
        _ -pdb subclass
            _ -consider how we might add support for multiple reply types (perhaps user could define a number of prefixes like my use of [dev] to determine this?)
                > Idk, the whole point of conversational debugging is that you can ask for what you want. Can infer between two response types based on whether program state has changed. With arbitrary types, at some point you're just passing in a func to do everything.
            X -check what happens with cache when asking a followup (contextless) question and consider if that's what's desired (i.e. should last_code be reset or not?)
                > Will have empty str for last_code but after giving this some thought, I think that's desirable. Contextless response is only used in debugger mode (not errors/logging/magic) and Cache is only used in non-debugger modes.
        X -update duck func to match new signature
            > Tried using fastai.delegates, decided I like htools.add_docstring more here bc it includes documentation and does less weird hacking of the function signature.
        X -look for other todos in file
            > Changed chat._user_message to public and added some docs.
            > 2 unrelated todos still present in file, but that's a separate task.
    _ -magic module
        > Looks like no changes needed :).
    _ -logging module
        > Looks like no changes needed :).
    -errors module
-reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/8/23 sat
----------
X -consider renaming debugger module to debug
    > Yep, like the shorter name. Done.
X -check for visibly necessary updates in (consequence of jabberwocky -> langchain):
    X -errors module
    X -skim other modules for necessary updates as a result of errors module changes
-then test each module:
    -debugger
    -errors
    -magic
    -logging
X -reproduce jabberwocky.load_openai_key somewhere with langchain (make it so user has option of defining in ~/.openai or something, doesn't have to always set env var)

4/9/23 sun
----------
~ -manually test each module (post-langchain changes):
    X -debugger
        > Lots of missing imports (pycharm seems to have stopped highlighting them) but fixed that. A few small bugs around when to use contextful vs contextless prompt, prompt being a langchain obj instead of str, assuming presence of 'code' key in prompt kwargs when contextless prompt doesn't actually have that. Fixed and confirmed works with dummy cls and openai cls.
        > However, prompt seems to need some tweaking. Answer includes leading numbers per section ('1.' and '2.') which is a bit annoying but not disastrous. More problematic is that my logic failed to extract the code snippet, I think because it is prefixed by '```python' rather than simply '```'.
    -errors
    -magic
    -logging
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/10/23 mon
-----------
X -check if I ever reserved roboduck name on pypi (and get it if not)
    > Yes, already have it.
X -look into how to re-enable pycharm static analysis to show missing imports
    > Think key was setting python interpreter. Once I did that and pycharm finished indexing, it worked.
~ -revise debug prompt to try to avoid getting section numbers too (already requested no titles, but maybe can be even clearer)
    > Tweaked wording to include numbers and be a bit more forceful, but still not always effective. Also added moderate logit bias against "section" and "Section" - perhaps not ideal if you have a class called Section but it's not a huge bias. I'll think about this. Still getting sections prefixed with "1." and "2." fairly often.
    > Added instructions saying it should appear as 1 section to the user. First response looks good, but I'll have to keep an eye on this to get a larger sample size. No need to test extensively now ($; testing in UI is free, I think, but doesn't let me change temperature; bettergpt.chat lets me chang temperature but presumably makes api calls so is not free).
X -revise debug prompt OR parse func to avoid/handle case of '```python' vs '```'
    X -consider which strategy is preferable (more explicit instructions or more flexible/permissive parser)
        > More flexible parsing, unlike the section titles I don't really care if it uses ``` or ```python (even the section titles issue isn't particularly important, but this is even less so).
    X -implement
-port other prompts over to langchain config (have to do this before testing bc I think most non-debugging modes use this; need to confirm)
-manually test each module (post-langchain changes):
    -debugger
    -errors
    -magic
    -logging
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/11/23 tues
------------
X -port other prompts over to langchain config (have to do this before testing bc I think most non-debugging modes use this; need to confirm)
    X -debug_stack_track
    X -debug_full_stack_trace (if exists? I forget)
        > It does not. Added one since it's not too much lift.
    X -debug_full
    > Also made some other prompt tweaks: replaced stopword "SOLUTION PART 3" with "Section 3" and tweaked wording a bit in user instructions.
    > Also fixed a missing import now that pycharm linting is working again.
-manually test each module (post-langchain changes, need to port prompts first):
    -debugger
    -errors
    -magic
    -logging
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/12/23 wed
-----------
~ -manually test each module (post-langchain changes, need to port prompts first):
    X -debugger
    X -errors
        > Also added ability to customize color in debug cls and made errors module print in red by default.
    -magic
    -logging
-maybe adjust debugger prompt user instructions
    -realized I explicitly tell it what to provide in section 1 and 2 and that's NOT answering the user question
    -maybe move instructions about each section to system prompt? Or include it in contextless prompt?
-allow magic to pass in kwargs (e.g. model, prompt_name, etc.)
-maybe add more flexible prompt kwarg surgery in debug cls ask_language_model (e.g. warn if unrecognized fields are provided, make sure error message is good if they're missing)
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/13/23 thurs
-------------
-manually test each module (post-langchain changes):
    X -magic
        > Also add option for user to pass in custom prompt. Experimented a bit with allowing kwargs but that gets messy quickly. Main use case would be passing in a custom pdb class but maybe custom prompts allow enough customization (though I don't currently support passing in a custom parse_func, which constrains things a bit more). Added warning if user passes in custom prompt.
    -logging
-maybe adjust debugger prompt user instructions
    -realized I explicitly tell it what to provide in section 1 and 2 and that's NOT answering the user question
    -maybe move instructions about each section to system prompt? Or include it in contextless prompt?
X -allow magic to pass in kwargs (e.g. model, prompt_name, etc.)
-maybe add more flexible prompt kwarg surgery in debug cls ask_language_model (e.g. warn if unrecognized fields are provided, make sure error message is good if they're missing)
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/14/23 fri
-----------
X -manually test each module (post-langchain changes):
    X -logging
        > Ended up removing base logger *args and **kwargs, that class's signature only seems to accept a couple args so I'd rather use **kwargs for chat kwargs. Could have sworn it did accept kwargs though... ðŸ¤”
~ -maybe adjust debugger prompt user instructions
    ~ -realized I explicitly tell it what to provide in section 1 and 2 and that's NOT answering the user question
        > Adjusted both system and user prompt in debug prompt a bit, and seeing some promising early results. Still considering exact wording and format - thinking an "output format" section might do nicely at preventing section titles/numbers, so keep an eye out for that issue popping up again.
    -maybe move instructions about each section to system prompt? Or include it in contextless prompt?
-maybe add more flexible prompt kwarg surgery in debug cls ask_language_model (e.g. warn if unrecognized fields are provided, make sure error message is good if they're missing)
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/15/23 sat
-----------
X -maybe adjust debugger prompt user instructions
    X -[DO BEFORE UPDATING OTHER PROMPTS] additional changes to debug prompt
        X -maybe move instructions about each section to system prompt? Or include it in contextless prompt?
            > I now recall openai docs say system prompt doesn't have as much impact as user prompt so maybe best to keep the syntax-related stuff (which is a bit harder for gpt) out of there. Not clear that contextless prompt really needs that format atm - maybe need to test drive this a bit more in real or semi-real situations to get a better sense of what types of followup questions I'll ask. Did tweak system prompt a little to at least mention that this is a live debugging session, which contextless prompt has no mention of otherwise.
        X -maybe include sample output section to demo format? Might be less confusing and maybe even more concise.
            > I like this and first result worked well, keep an eye on this.
    X -update other prompts to match updated debug.yaml (except for intentional differences)
        X -debug_full
        X -debug_stack
        X -debug_full_stack
-maybe add more flexible prompt kwarg surgery in debug cls ask_language_model (e.g. warn if unrecognized fields are provided, make sure error message is good if they're missing)
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/16/23 sun
-----------
X -consider renaming langchain module in roboduck lib (maybe the wrong way to think of this?)
    > Keep name but make some core classes available in roboduck init so user doesn't need to explicitly reference this in imports. We DO have a heavy reliance on langchain so the fact that the naming reflects that is reasonable, IMO.
X -maybe add more flexible prompt kwarg surgery in debug cls ask_language_model (e.g. warn if unrecognized fields are provided, make sure error message is good if they're missing)
    > Raise error if receive unexpected kwargs OR fail to receive expected kwargs. (Langchain will error out in either of those cases so don't just warn.) Tested both variants on a dummy prompt.
-update deps
    > Added langchain (version will likely need to change before release due to their rapid dev speed), removed jabberwocky, removed some docstring refs to jabberwocky.
~ -look into support for models besides 3.5-turbo
    X -does gpt4 use same system/user instruction system?
        > 4 and 3.5-turbo both are chat only, no completion endpoint support. Should be same system/user setup.
    X -check if langchain has added any non-openai models yet or if there are discussions to do so
        > Added anthropic chatmodel support a couple days ago, no custom init method so hopefully its a dropin replacement for chatopenai cls. Need to confirm that though. Api key would obviously need to be set for it to work.
        > Issues does have open requests for OpenAssistant and Vicuna. Re the latter, someone notes that a HuggingFacePipeline abstraction already exists in langchain.
    -is it possible to make a single prompt template obj in chat obj w/ langchain? IIRC this might be possible. (This could unlock other models.)
    -what format do other open source systems expect?
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/17/23 mon
-----------
X -check if latest build passed
    > No, need to change htools install to speedup version. Might change those deps later anyway since it seems to no longer really be an extra for htools. However, plan is to probably remove htools dep anyway here, so it's not time sensitive.
-look into support for models besides 3.5-turbo
    X -is it possible to make a single prompt template obj in chat obj w/ langchain vs just a list of message dicts? IIRC this might be possible. (This could unlock other models.)
        > Looks like yes, there's langchain.prompts.chat.ChatPromptTemplate but it's not clear why it's necessary - docs create it from a list of messages but then convert it back to list of messages before calling chat. I think it's mostly useful if your system prompt also takes in variables, which mine doesn't.
    -does chat_class=langchain.chat_models.anthropic.ChatAnthropic work?
        ~ -get api key if necessary?
            > Submitted access request, may need to wait a bit.
        -check if works
        -fix if necessary
    ~ -what format do other open source systems expect?
        > Anthropic looks similar. Couldn't find public chat api for Open Assistant (api endpoints seem mostly for devs building the assistant, not building on top of it). I guess just trust that langchain will provide a consistent interface so if I conform to their UX it should be okay.
-groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)

4/18/23 tues
------------
X -check if build passes now
    > No, no openai api key in build. Added strict arg in api load key func and set to false by default.
    > EOD update: build passes!
-look into support for models besides 3.5-turbo
    -does chat_class=langchain.chat_models.anthropic.ChatAnthropic work?
        -check non-55 email for access, set up api key if so
        -check if can pass in langchain class w/out changes
        -fix if necessary
~ -groom backlog and take stock of project (what still needs to be done? Try partially autogenerated docs + tests?)
    > Removed some items from backlog.
X -add make command
    X -make chat_prompt

4/19/23 wed
-----------
X -check for possible bug - what happens if user message type is named "reply" when creating chat class (recall reply() method usually gets autogenerated but so does a method with the name of each user message type; possible collision and unclear what would happen with key_ arg)
    > Confirmed works both when defining 1 reply type named "reply" and multiple reply types where 1 is named "reply".
X -consider removing prompts/completion module? Or define a template and script there
    > Doesn't seem like we have an immediate need for this but also don't feel super compelled to remove it now. I guess I probably should remove it if I'm not going to use it (just adds clutter/noise to someone trying to use or contribute to the lib). But things change so fast, maybe some amazing open source non-chat model will pop up /shrug. Can always delete later.
X -consider if we have enough reason to warrant a cli (make_prompt/make_chat_prompt, duck as a "run with errors mode enabled" command, other?)
    > I think no need for multiple commands, just a single one to run a script like we would with "python script.py" with default settings.
    > Implemented sort of ugly version using a temporary file and str surgery. Alternatives seem like they might change the __file__ or __path__ attrs. Still need to document, consider other improvements or extensions.
-take stock of project and plan next steps
    -what still needs to be done? Try partially autogenerated docs + tests?
    -do I have concrete idea for more ambitious reloadium-type next steps?
    -and if so, should that be its own thing? I.e. do after a break vs. now?
    -write down next steps
-look into support for models besides 3.5-turbo
    -does chat_class=langchain.chat_models.anthropic.ChatAnthropic work?
        -check non-55 email for access, set up api key if so
        -check if can pass in langchain class w/out changes
        -fix if necessary

4/20/23 thurs
-------------
~ -easy:
    X -document cli.py
    ~ -consider moving make_prompt into cli? Probably not but consider.
        > Still thinking, but leaning no for now.
    ~ -consider allowing user to pass kwargs in to duck cli command to use diff models, prompts, auto explain mode, etc.
        > Updated to allow any kwargs, but importing classes isn't working (see notes in cli.py). For now just raise notimplementederror.
        > Still have to think about UX: might be better just to hardcode in a few common options like `auto` and possibly ignore the rest. Atm no error seems to be raised when passing in unrecognized kwargs, and detecting them could be nontrivial given the winding road we pass them down.
        > Also added `make reinstall` command and got `duck foo.py` working, but quicker to test with full python command to avoid frequent reinstalls.
    -other module docstrings
    -update readmes
-take stock of project and plan next steps
    -what still needs to be done? Try partially autogenerated docs + tests?
    -do I have concrete idea for more ambitious reloadium-type next steps?
    -and if so, should that be its own thing? I.e. do after a break vs. now?
    -write down next steps
-look into support for models besides 3.5-turbo
    -does chat_class=langchain.chat_models.anthropic.ChatAnthropic work?
        -check non-55 email for access, set up api key if so
        -check if can pass in langchain class w/out changes
        -fix if necessary

4/21/23 fri
-------------
-easy:
    -consider hardcoding a few kwargs in cli.py parser instead of allowing kwargs
    -OR do some arg validation to prevent typos slipping through the cracks
    -other module docstrings
    -update readmes
-take stock of project and plan next steps
    -what still needs to be done? Try partially autogenerated docs + tests?
    -do I have concrete idea for more ambitious reloadium-type next steps?
    -and if so, should that be its own thing? I.e. do after a break vs. now?
    -write down next steps
~ -look into support for models besides 3.5-turbo
    X -test gpt4 (pretty sure this should be fine)
        > Works, though I intially tried to use 'gpt4' when it's actually 'gpt-4'. Maybe should provide some enum to remind users what's available?
        > Decided to instead provide utility function that shows what models are available.
    -does chat_class=langchain.chat_models.anthropic.ChatAnthropic work?
        -check non-55 email for access, set up api key if so
        -check if can pass in langchain class w/out changes
        -fix if necessary

4/22/23 sat
-----------
-easy:
    -consider hardcoding a few kwargs in cli.py parser instead of allowing kwargs
    -OR do some arg validation to prevent typos slipping through the cracks
    -other module docstrings
    -update readmes
X -dig more into weird import issue in cli.py
    X -try building up minimal dummy script from scratch and see when import error appears, or try copying over cli.py to new location and gradually remove things
        > Identical script runs fine in ~/roboduck dir and ~/roboduck/lib dir. Breaks when we move it into lib/roboduck.
        > In lib/roboduck, even a blank file containing ONLY an import (e.g. requests.Session) breaks. Running other modules from the command line does too. Therefore, I think the issue is that my logging.py file is somehow clashing with the logging module that requests (for example) imports from.
    X -fix
        > Found a pleasingly simple solution - just move cli.py to inside a new cli subdir so it's not in the same location as my logging.py.
        > Also had to add imports to file (when classes are imported) and do some str surgery to make the errors.enable call use the class name as an object and not as a str (or as a class name repr).
-take stock of project and plan next steps
    -what still needs to be done? Try partially autogenerated docs + tests?
    -do I have concrete idea for more ambitious reloadium-type next steps?
    -and if so, should that be its own thing? I.e. do after a break vs. now?
    -write down next steps

4/23/23 sun
-----------
X -easy:
    _ -consider hardcoding a few kwargs in cli.py parser instead of allowing kwargs
    _ -OR do some arg validation to prevent typos slipping through the cracks
    X -other module docstrings
    X -update readmes
X -preserve newlines in cli help info
X -add more examples of calling CLI with available args
_ -take stock of project and plan next steps
    -what still needs to be done? Try partially autogenerated docs + tests?
    -do I have concrete idea for more ambitious reloadium-type next steps?
    -and if so, should that be its own thing? I.e. do after a break vs. now?
    -write down next steps
    > Basically know next steps already, writing tests.
~ -add more tests
    > Added a test for type_annotated_dict_str from gpt, but now pytest cmd throws error (can't import roboduck even though installed). Next step is to push to github and see if tests run successfully there, I think.

4/24/23 mon
-----------
X -get pytest working again (recall roboduck import error)
    > Tried pip installing pytest since it turned out it wasn't installed in py38 env, did not help. Tried removing init file in tests dir at stackoverflow's suggestion but didn't work either. What did work was running python -m pytest instead of pytest, also suggested by stackoverflow. GPT did not help here.
X -fix last test case for type_dict_str func if necessary
~ -add more tests
    X -utils.extract_code
        > Also pleasantly surprised to find that backticks in code already works fine - thought that might break my parsing func and I'd have to fix it.
    X -utils.store_class_defaults
    > Done (at least enough for now) with utils module. Other modules may be a bit trickier to test.
-look into how to add coverage % badge on github

4/25/23 tues
------------
~ -add more tests (intentionally excluding magic.py, shell.py; can revisit later if figure out a useful and effective way to do this)
    ~ -logging module
        > Mostly there, but still trying to figure out why we're still printing something to stdout when logging exceptions even when stdout=False. Some notable clues/notes:
            > Standard lib logging class does this too even after removing all handlers and setting propagate=False.
            > Easy to get tricked by logging level. Logging.info is often silent bc of default level, not bc we disabled stdout logging.
            > Error message is preceded by standard logging format when stdout=True but NOT when stdout=False. This makes me wonder if I inadvertently left some print statement somewhere (logging/debug/errors modules) and it's not actually coming from the logger. I haven't managed to uncover that yet though.
        > Other changes: documented a couple misc params I came across. Added check in excepthook to not print stack trace if kwargs['silent'] is True. Trying out making logger set `silent=not stdout` rather than hardcoded to true (forget my original logic here. Still considering. Don't want to halt program so logger can get explanation but maybe doesn't matter since it'll take just as long without streaming mode. Bain is kind of fried at this point, no use thinking about it more right now.)
    -errors module
    -debug module
    -prompts.utils module
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -callbacks
        -chat
        -utils
X -fix bug in truncated_repr that occurred when objects were iterable but didn't support obj[:i].
X -update makefile to run pytest in -s mode (show stdout)
-look into how to add coverage % badge on github

4/26/23 wed
-----------
X -update git ssh key to work w/ recent updates
X -figure out how to remove 'In' and 'Out' vars from vars included in prompt when in ipython
    > Updated htools is_ipy_name. Might eventually port this to roboduck as well. Exclude some other ipython vars by default too.
    > Considered removing things like __builtins__ but they're not actually ipython-specific so if I do that it should occur outside the is_ipy_name function. Not super harmful (like some of the ipy _ih vars which quietly add a dangerous number of tokens to our prompt) but maybe still worth excluding to save a few tokens and improve signal:noise ratio in prompt? No need to act now.
~ -add more tests (intentionally excluding magic.py, shell.py; can revisit later if figure out a useful and effective way to do this)
    X -fix failing test in logging module (still printing output even in stdout=False mode instead of being silent. See yesterday's notes for clues.)
        > Had to subclass debugger cls print_stack_entry method.
    -errors module
    -debug module
    -prompts.utils module
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -callbacks
        -chat
        -utils
-look into how to add coverage % badge on github

4/27/23 thurs
-------------
~ -write/generate more tests
    X -errors module
        > Only tested that enable/disable works. Not sure how to test components that require human interaction. /shrug
    -debug module
    X -prompts.utils module
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -callbacks
        -chat
        -utils
-look into how to add coverage % badge on github

4/28/23 fri
-------------
~ -write/generate more tests
    ~ -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        X -callbacks
        _ -utils
            > Only 1 function and it just loads api key. Skip.
        -chat
    -debug module (consider how to even test this?)
~ -fix failing build
    > Seems to be tied (at least partially) to missing openai dep. Assumed langchain included it but I guess not.
    > Still need to check if build passes - might be other issues.
-look into how to add coverage % badge on github

4/29/23 sat
-----------
~ -check if build passed (recall added openai dep)
    > Still no, available_templates() fails in remote build despite working locally. Did find some clues and made some improvements: error message now refers to key rather than mode to be more informative/less confusing, and fallback value is VALID_MODES rather than hardcoded tuple. Other clue (still unexplained) is that only "chat" mode is available on remote but locally we also have "completion".
X -remove htools dep
    X -identify all functions/classes used
        > Added sub-bullets in next bullet below.
        > Note: Updated version from cli.ReadmeUpdater has already been ported.
    X -port over each component
        X - core.random_str
            > Technically found you can use secrets.token_hex() instead.
        X - core.load
            > Just hardcode, we're only using it to load txt and py files.
        X - core.is_ipy_name
        X - meta.typecheck
        X - meta.add_docstring
    X -update import statements
    X -rm from requirements.txt
    X -make reinstall
X -fix cli entrypoint
    > Realized it was trying to import roboduck.cli.run but based on actual file location it should be roboduck.cli.cli.run. Fixed.
-write/generate more tests
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -chat
    -debug module (consider how to even test this?)
-look into how to add coverage % badge on github

4/30/23 sun
-----------
X -check if build passed (issue from sat re VALID_MODES should work, though still unclear why 'conversations' dir is missing remotely but not locally.)
    > Same test fails bc missing completion dir causes results to differ from expected vals. Now I'm thinking maybe bc the dir is empty, git is excluding it when running actions. Added gitkeep file.
    > Passes!
X -look into options for auto doc generation
    > Lots of progress. Noodled a bit on dynamic docs in misc.txt but decided a bit too ambitious for what's supposed to be a quick add on. Tentatively moving forward with mkdocs (fastapi's docs are pretty nice, and didn't have the greatest experience with sphinx). Got basic version running, though still lots of bugs. Guess maybe auto-doc generation is finicky in general.
-write/generate more tests
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -chat
    -debug module (consider how to even test this?)
-look into how to add coverage % badge on github

5/1/23 mon
----------
X -make commands
    X -make env (for devs)
    X -make docs
    X -make serve/serve_docs?
~ -docs
    ! -maybe restructure so mkdocs.yml isn't in repo root (i.e. have docs contain config file and docs and site subdirs. Might need to update some paths in configs?)
        > Tried this for a bit but it seems more complex than is worth it. Reverted to current structure.
    ! -figure out how to add nested py files to nav (e.g. cli, see docs/cli.md notes)
        > No luck yet, chatgpt and stackoverflow are both coming up short here.
    -look through docs and make changes as needed
    X -get gh pages deploy working
X -update readme
    X -link to docs
    X -misc dev notes: unit tests, build docs, etc.
X -update git remote repo url
-maybe break down utils module into multiple smaller modules
-write/generate more tests
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -chat
    -debug module (consider how to even test this?)
-look into how to add coverage % badge on github

5/2/23 tues
-----------
X -figure out how to rm Welcome to Mkdocs page on generated docs
    > Updated make commands to copy readme into docs dir before relevant commands. Should not use mkdocs commands anymore since they obviously don't do that.
X -flesh out readme
    X -brief description
    X -resize img
        > Still need to add demos/quickstarts, but save that for later.
X -maybe break down utils module into multiple smaller modules
~ -try adding deploy_docs step to github workflow
    > No idea if this will work, but updated workflow.
    > Fails with "mkdocs: no such file or directory.". Hmm.
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed
-write/generate more tests
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -chat
    -debug module (consider how to even test this?)
-look into how to add coverage % badge on github

5/3/23 wed
----------
X -add html files (or sites/* files) to .gitattributes
X -fix deploy_docs action in gh workflow
	> Some progress. Install from txt file rather than with script bc we already have python installed in workflow. Had to give gh actions permissions to read/write in repo settings. Tried rebuilding after that and failed due to remote having refs local lacks, but I wonder if that might be resolved by just pushing a new commit rather than rebuilding.
	> Update: above plan worked.
X -look into how to add various badges (e.g. coverage %) badge on github
	> Added docs, pypi version, and CI/build badges. gpt was quite helpful at knowing badge urls to add.
X -readme formatting tweaks
	> Place title/img/badges in table.
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed
-write/generate more tests
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -chat
    -debug module (consider how to even test this?)

5/4/23 thurs
------------
! -check if api key load happens on every debug cls init (if not, user could set env var and next call would still fail)
	> No, but actually I realized that func just sets the env var. If user does this manually, next chat cls instantiation should catch it, so should be fine.
~ -long prompt handling updates
	X -maybe can make warning length depend on current chat_class? (e.g. no need to warn if 4k tokens and using gpt4 w/ long context window)
		> Should be able to retrieve max_tokens in debug cls by self.chat.kwargs.get('max_tokens'). But confirmed max_tokens refers JUST to completion. However, the prompt length + completion length are still subject to the model context window.
		> Context window, however, is not automatically retrievable from chat (that I know of - maybe langchain tucks it away somewhere?). Could retrieve self.chat.kwargs.get('model_name') and compare it to a hardcoded dict of context lengths in lib, or just let things fail if prompt gets too long. OR maybe need param to control how many of last n user responses to include (like in jabberwocky conv manager) bc otherwise I think the debug prompt will just get longer and longer).
		> Do this in langchain.chat.Chat class now. Probably will adjust logic a bit later to allow the last_n_turns behavior I describe above.
	~ -confirm what happens if prompt is too long
		> Read that openai just raises an error and won't start the request. Now I preemptively error out to control error message.
	-vaguely recall ipy session often produce massive prompts. Maybe see if there's a more intelligent way to handle this.
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed
-write/generate more tests
    -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        -chat
    -debug module (consider how to even test this?)

5/5/23 fri
----------
~ -long prompt handling updates
	X -vaguely recall ipy session often produce massive prompts. Maybe see if there's a more intelligent way to handle this.
		> Turns out the ipython-specific issue I feared existed did not (at least in the sense that non-"full" prompts only use the last executed cell). However, my "local_vars" key filtering was overly permissive regardless of file type. Adjusted it to work like global_vars and only keep used tokens. A little worrisome since it seems like the difference in how they were handled might have been intentional, but the output looked more sensible when testing this time and as long as tests pass I guess it's fine.
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed
X -write/generate more tests
    X -langchain (in some cases, may be able to see how langchain tests their built in components and use that as a guide)
        X -chat
    _ -debug module (consider how to even test this?)

5/6/23 sat
----------
X -fix failing chat tests
	> Needed to set openai api key env var in fixture.
X -add instructions for setting api key
X -logging TODO re silent mode (see notes in py file; currently hardcoded to always silent but does stdout=True work then?)
	> Confirmed that the printing is caused by my super()._log call, NOT my excepthook call which is good. The question now is: why does that still print to stdout when I remove the stdout handler? Or do I?
	> Set propagate=False when stdout=False, prevents stdout logging while keeping file logging. Also updated comment so I remember to always keep live=False (even when stdout=True, we just want super()._log to print at end, not live typing). Also added error if user requests neither file nor stdout logging.
	> Added couple new tests too.
-debug todo re storing code vs full code
-long prompt handling updates
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed

5/7/23 sun
----------
X -debug todo re storing code vs full code
	> Considered inferring which to store but ultimately decided just to store the snippet. Added comment explaining why so I can reference that later if I'm thinking of changing it.
-long prompt handling updates
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed
X -write quickstart code snippets for readme/docs
	X -debugging
	X -errors
	X -logging
	X -magic
	X -cli
	> Also moved up api key section to top of quickstart, revised example snippet in debug module.

5/8/23 mon
----------
X -api key changes
	X -consider changing expected api key location from ~/.openai to ~/.roboduck/openai.conf or ~/.roboduck.yaml or something
	X -consider making set_openai_api_key write to this file if not exists?
	X -update readme if necessary
	> Involved writing new utils.update_yaml func too.
-write a short guide/example for defining a custom:
	-prompt
	-consider if any other custom guide needed? seems like there would be but not coming to mind. Chat class should typically be provided by langchain, I guess model_kwargs might need to be overridden but that's part of making a custom prompt.
-record demo gifs for readme
-long prompt handling updates
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed

5/9/23 tues
----------
X -write a short guide/example for defining a custom:
	X -prompt
	_ -consider if any other custom guide needed? seems like there would be but not coming to mind. Chat class should typically be provided by langchain, I guess model_kwargs might need to be overridden but that's part of making a custom prompt.
X -mv scripts to bin/ and update makefile accordingly
X -readme tweaks
X -add github "About" description/url/topics
-record demo gifs for readme
-long prompt handling updates
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed

5/10/23 wed
-----------
~ -consider major config improvements
	~ -allow specifying default model between gpt-3.5 and gpt-4? (So user doesn't have to always type duck('gpt-4') if they always want to use that. Have to think about how config would interact with prompt template default. Maybe set it to none by default and only override if `config['defaults']['default_model'] is truthy?)
		> Wrote utils funcs to update and load config. Did some thinking about resolution order and wrote notes in func docstring but didn't yet incorporate global config into resolution in Chat class.
		> Also added config module with config path and updated a couple other refs to it.
	-allow adding paths to custom prompt templates so users can use names rather than paths?
	-support loading/setting/writing arbitrary number of api keys (e.g. huggingface, anthropic, ???). (Perhaps more of an update of set_openai_api_key)
-record demo gifs for readme
-long prompt handling updates
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed

5/11/23 thurs
-------------
-consider major config improvements
	-incorporate global config into model_name resolution in chat instantiation
		-decide where this should occur. Inside init, or before?
		-implement
	-[WAIT: I think it's easier to implement just model_name first and then genenralize if necessary rather than generalizing upfront.] consider adding support for setting nested kwargs in config.yaml (e.g. have one group for api keys, one for things like model_name or temperature, one for roboudck kwargs like max_len_per_var/color/prompt_name)
		-decide
		-if yes:
			-update update_config func
			-update kwarg resolution in chat cls (extend from just model_name)
			-write more unit tests ðŸ˜­
	-allow adding paths to custom prompt templates so users can use names rather than paths?
	-support loading/setting/writing arbitrary number of api keys (e.g. huggingface, anthropic, ???). (Perhaps more of an update of set_openai_api_key)
-record demo gifs for readme
-long prompt handling updates
	-look into using last_n_turns in Chat history so prompt doesn't grow infinitely big (can reference jabberwocky conversation manager logic if desired)
-docs
    -consider what to do about nested py file problem (e.g. cli, has to be in nested dir to avoid import issues). List out possible options below
    -look through docs and make changes as needed

Backlog
-------
-other model support
	-does chat_class=langchain.chat_models.anthropic.ChatAnthropic work?
		-check non-55 email for access, set up api key if so
		-check if can pass in langchain class w/out changes
		-fix if necessary
	-is there some kind of ChatHuggingFace class in langchain? Seems like there must be, or will be soon if not yet.
-Chat class improvements
    -save/load transcript?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?
-passive:
    -finish distill post on augmenting human intellect
    -watch bret victor Future of Programming vid
    -read Engelbart paper on augmenting human intellect
-investigate: was it intentional that PromptManager supports passing in GPT obj but ConversationManager doesn't? (I think initially neither did but had an immediate use case for PromptManager so changed that one, but haven't had a need yet for ConversationManager so didn't prioritize it. Probably good to do eventually though.)

Easy Backlog
------------
-langchain.chat.chat various method docstrings

Keep an eye out for
-------------------
-debug prompt: hoping the section titles (e.g. "section 1." or "1.") are gone now, but see if they come back
-in nb05 testing components, sometimes running func w/ duck() call just errors as usual (think maybe autoreload related, seems to happen after editing source code?)
-dummychatmodel in debugger adds a couple newlines in the middle of the word "enclosed". Unsure why.
-sometimes openai obj seems to be from an outdated version, restarting kernel helps. Not sure if it's a jabberwocky thing, a langchain thing, an openai thing, or some weird interaction between two or more of those.
-debug slowness when using magic (is it calling query multiple times?) ~ - add option to add new cell w/ gpt-fixed function below (may need to adjust prompt a bit to encourage it to provide this)
    > 2/13/23: Haven't observed this in this round of work on the project, leftover from last time. Maybe just a transient issue w/ the codex API at the time?
-codex completion quality issues
    > 2/13/23: seeing a lot of repetition lately. Seems like it came after I upped the freq penalty so that seems odd, but maybe the more salient change is that at some point I removed docstring quotes as a stopword (bc we may need to generate it sometimes).

Won't do (for now)
------------------
-add make command `make prompt` (NOT chat_prompt, which exists)
	> Rationale: not used in lib atm.

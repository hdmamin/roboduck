2/1/23 wed
----------
X -settle on project
X -find all old relevant work
    > added 3 relevant files to this dir for now. The fourth (and perhaps most developed) effort is in the ~/nbextension-duckling dir.
~ -decide how to structure dir(s)/repo(s) (Will prob have multiple semi-related outputs, perhaps 1-5.)
    > Tentatively thinking monorepo. Easier to track everything that way.
-pick repo name
    > cairina, from "cairina moschata", one of the biggest, most powerful looking species of duck ("muscovy"). Think (rubber) duck on steroids.
X -add readme img
    > :D
~ -read distill AI-augmented human intelligence post

2/2/23 thurs
------------
X -premortem
X -map out very tentative plan for order to take things on (e.g. debugger -> jupyter magic -> interactive error messages -> nbextension -> reloading stuff -> LLM native programming language)
    > Lets start with debugger, then jupyter magic. The former is pretty far along and the latter should largely build on top of the debugger. We can take stock after that - depending on timing, can take a break or fit in error messages before break or move on to one of the bigger rocks (live reloading stuff or nbextension) if it went surprisingly fast.
X -consider whether notebook extension proj should continue or start over as jupyterlab extension
    -find stats on how commonly used each is?
        > In 2019 jetbrains python dev survey, DS use cases skewed twoards notebook (13% vs 5%; if we normalize to avoid non-jupyter options, that becomes 72% and 28%). By 2020 survey, it was 11% vs 6% (65% and 35% normalized). In 2021, they didn't break down results by DS users, so it's just 3% vs 2% (60% vs 40% normalized). In the "additional but not main IDE" question, it was 25% vs 12% notebooks (68% vs 32% normalized). In kaggle 2022 survey, 66.5% used nb while 23.7% used lab (74% vs 26% normalized). So (somewhat surprisingly) it seems like notebook is still considerably more popular, though perhaps slowly shifting towards lab. I like nb and started building in that so it's tempting to continue in that, but I guess I don't need to decide quite yet given that we're starting with the debugger.
~ -create cookiecutter template for each major project
    > Just start with debugger subdir (now "roboduck", i.e. a more powerful rubber duck). Uploaded to pypi to claim name.
-finish distill AI-augmented human intelligence post

2/3/23 fri
----------
X -remind myself where I left off w/ debugger work
    X -run existing code (do)
    -skim over existing code (read)
    X -read through done and outstanding todos
    X -write some todo items for what comes next
X -possible bug: seems like if we haven't saved nb since latest changes, load_ipynb func can load an outdated version (possibly? Though tit did that once but then I looked again and it seemed to be up to date so idk)
    > Confirmed that was happening but fixed it w/ help of a stackoverflow function.
X -consider adding option for "I don't know". Or maybe something like "If you don't know what's causing the bug, say "I don't know". Then write a list of 5 plausible causes that the developer can check for when debugging." (take advantage of its strength at generating lists, thinking of possibilities we might not)
X -update prompt to explain to gpt what the chat_db()/roboduck()/rbd() func call is
X -rename usages from llmdb to rdb or similar?
X -update func names to fit roboduck lib
X -add stop word to prompt to prevent starting another question
-finish distill AI-augmented human intelligence post

2/4/23 sat
----------
X -see if we can color user/model turns differently. (Bit hard to read atm, though partly bc I'm printing out full prompt for debugging purposes.)
X -add backup value when response is nothing
X -debug why I'm getting so many empty responses
    > Turns out I had docstring quotes in my stop phrases which can be problematic since that is a valid thing to generate at times. Also didn't start my prompt with docstring quotes which maybe degrades quality a bit. But if I add them back in, then I feel like I need it in stopwords again. Hmmm.
X -make codex respond in second person
X -figure out how to make multiline str start with quotes in yaml (or abandon their use)
    > Turns out my old method already supports that. Just don't escape them.
-consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
-hide user warning about using codex model name.

2/5/23 sun
----------
X -consider removing stop seq """ from 3 debug prompts (in case it actually needs to generate that w/in solution)
    > Yes, try it out. Just have roboduck rstrip them.
    > Update: with this format, codex seems to like closing the docstring before doing anything else, then jumping straight to code, meaning no explanation gets written. May need to revert to old no-docstring method.
    > Also realized I haven't been bumping the prompt versions. Whoops. I guess maybe this only needs to be done when I actually bump the jabberwocky pypi version and/or the roboduck pypi version.
_ -consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
    > I think for now let's leave this be - I'll be seeing a lot of query results over the next couple months so it will become apparent if this is something we need (i.e. if quality is an issue). I also suspect we may see a chatgpt/gpt4/codex2 release that makes this type of prompt engineering less necessary before the end of this project, so no need to optimize that dimension now.
X -hide user warning about using codex model name
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.

2/6/23 mon
----------
X -try reverting debug prompts to no opening docstring (see sun notes for why)
    > Nope, tried this again but it really seems like it needs the docstring otherwise it just jumps straight to code. Added logit bias instead of stopword. Also add another stopword since I observed some cases where it skipped repeating the DEVELOPER QUESTION but did repeat LOCAL VARIABLES and everything afterwards.
X -see how chatGPT does
    > As expected, noticeably better. Codex seems to like repeating things and writing lengthy answers here even though I added the word "concise" to the instructions. I bet a better code api will become available within the next few months.
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
X -stream completion instead of waiting for whole thing
    > Added streaming with .02s between characters. Maminbot used .015 but somehow even .02 seems really fast in jupyter. Maybe streamlit had extra latency so it wasn't really .015s there.

2/7/23 tues
-----------
~ -consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
    X -write draft of truncated_repr func
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/8/23 wed
-----------
X -add file from jabberwocky (I think?) to avoid counting jupyter notebooks in github code % count
-handle huge data structures (big df, long list, etc.) 
    X -handle case where seq[:1] still produces repr that's too long
    X -test behavior on dfs again (potential opportunity to do something custom like "pd.DataFrame([col1, col2, ...])"? As in literally that string, just call truncated_repr on cols.)
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/9/23 thurs
-----------
~ -handle huge data structures (big df, long list, etc.) 
    X -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it would be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
        > I guess I intentionally made dotdict have the same str and repr as a regular dict so it's tough to come up with a rule that would help here. I could change its str/repr, just leave it as the regular dict version, or update my debug prompt to show the type of each var too (could be a separate section like GLOBALS_TYPES, could be a comment after each value in GLOBALS like `'nums': [1, 3, 4...], # list`. Last idea is interesting.
    X -try out idea about specifying var types in comments
        > Wrote new func. I like this.
    X -fix bug: empty sets represented wrong
        > Because empty lists and tuples use literals like [] but sets use 'set()' so my logic around the last non-brace index failed. Added some hardcoded logic around setes.
    X -fix bug: dict-like objects have extra quotes
        > Must have added this mistakenly, fixed now.
    X -tweak debug prompt to acknowledge that roboduck func may not be present (in case we repurpose these)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/10/23 fri
-----------
X -handle huge data structures (big df, long list, etc.) 
    X -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
        > Just hardcode for now. There's not going to be some magic number here that's perfect.
    X -document (mention that length is more of a rough guideline, not strictly enforced)
_ -consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
    > Actually I think this is pretty good.
X -tweak prompts to hopefully reduce repetition a bit (been noticing this more lately)
    > Added frequency_penalty of 0.2 (pretty minor change for now, range is -2 to 2). Haven't tested yet, just keep an eye on things as I continue working on it. Don't want to sink too much time into selecting perfect prompt params now bc I think better models may be available soon.
~ -maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
    > Tried replacing load_prompt w/ PromptManager and realized the issue is that I wnat to check the resolved prompt, and pm.query() with verbose mode just prints. But found pm.prompt() and pm.kwargs() methods so it is possible. However, we have to pass GPT obj to PM on instantiation so if we want to support diff backends in roboduck init, we still need to create PM inside it. So not really saving any time. But maybe this work will help pave the way if I want to transition to ConversationManager at some point.
    > Haven't tested this yet but it's ready to be tried. I kept the old cls around in case I want to revert (w/out messing w/ jupyter/git interaction weirdness).
-try running magic again and see if problems arise

2/11/23 sat
-----------
X -handle pandas series separately in truncated_repr (noticed it was not handled that well, was truncating the metadata for some reason)
    > Also refactored func a bit.
X -try running new version of roboduck with PM (just run all cells from cls def down to debugging session cell)
    X -debug if necessary
    > Had to fix both prompt() and query() calls, they take slightly different args than GPT.query() which in hindsight is maybe not ideal but I guess part of the point is that it's supposed to simplify the call and if it had an identical interface it wouldn't be any easier. Works now, though codex's answer was very wrong.
-consider whether to try replacing PM with convmanager
-if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
-try running jupyter magic again and see if problems arise

2/12/23 sun
-----------
X -add my jabberwocky pre-commit hook to avoid pushing api key
_ -consider whether to try replacing PM with convmanager
    > Don't want to get too bogged down on exact interfact yet. Don't know if upcoming chatgpt API might have slightly different UX (e.g. conversation_id) - this could be used in place of my conversationmanager cls, which is more helpful for the alexa skill with its auto-persona generation via wikipedia.
_ -if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
~ -try running jupyter magic again and see if problems arise
    > Existing code worked fine. Taking another stab at setting shell attr instead of global var to help with code insertion but not working yet.
    > Confirmed that I CAN set a var ;ole get_ipython().xyz = 123 and it persists (just working in global scope for both).
    > BUT if I set it in RoboduckDB.ask_language_model, it does not seem to persist by the time the magic checks it. Need to investigate more. I think this is similar to the issue I ran into last time. Maybe shell obj is recreated every time we change scope or something?

2/13/23 mon
-----------
X -debug failure to set get_ipython() var in certain contexts (see yesterday's last few bullet points)
    > Realized there's no real reason this has to be set on the Ipython object. I ended up creating a cache class and setting a var there - not sure if this is meaningfully better than global (we're just editing a global class var, I suppose) but maybe it is, idk.
X -add "keep an eye on" and "won't do" sections to dailies
~ -extract code from gpt completion when inserting code cell
    > Tried ast.parse-based method, pretrained huggingface classifier method, considered ways of breaking down prompt into 2 to make extraction easier. Ended up implementing method that makes a second codex call but only when the magic exits, not on every conversational turn, that extracts code. Seems to work alright in playground but need to debug a bit more - code snippet is no longer being used to populate cell below. Also, without live typing it feels a lot slower.
-finish distill post on augmenting human intellect

2/14/23 tues
------------
X -debug code extraction in magic: why is it failing to populate next cell now?
    > First val returned by gpt.query is list, had to extract str.
X -consider other appraoches to code extraction (really feel like a simpler version relying on templating should be possible)
    > Have prompt ask for a two part solution and title the sections carefully to try to encourage this separation. Seems promising so far. Had to add some new stop words. Just updating debug and debug_full prompts atm, leave duckling prompt for later.
~ -update roboduck cls to avoid printing the SOLUTION PART 2 part
    > Did planning but no implementation yet. A bit tricky since we're printing one token at a time and SOLUTION tokenizes into ['S', 'OL', 'UTION']. It becomes very slightly less complex if we go lowercase (['s', 'olution']) but even so we probably need a solution that builds up a queue of tokens to print and prints with a slight delay (i.e. always check the last x tokens, prob 4-8, before printing. If we find ourselves in the SOLUTION PART 2 phrase, we pop those tokens off and don't print them until we hit a newline).
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/15/23 wed
-----------
X -move testing cells to bottom of nb to avoid re-run on kernel restart
    > Frequent restarts are necessary to make magic update, even if we manually delete it before registering it again. Weird that this issue just popped up. Wonder if related to deleting cookies somehow? Seems unlikely. Better solution is to do as much of debugging as possible in Roboduck cls and just update magic occasionally once a big chunk of progress has been made.
~ -update roboduck cls to avoid printing the SOLUTION PART 2 part
    ~ -implement algo from yesterday: SOLUTION tokenizes into ['S', 'OL', 'UTION']. It becomes very slightly less complex if we go lowercase (['s', 'olution']) but even so we probably need a solution that builds up a queue of tokens to print and prints with a slight delay (i.e. always check the last x tokens, prob 4-8, before printing. If we find ourselves in the SOLUTION PART 2 phrase, we pop those tokens off and don't print them until we hit a newline).
        > WIP. Quite tricky. Getting close now - remaining bug is that openai seems to (at least sometimes) return "SOLUTION PART 2" as a single token (or else I'm unintentionally converting it before adding it to the queue somehow?). Think the solution is to first confirm that that's actually happening (i.e. print out cur on each iteration). Then if that is true, update algo to grab items from start of queue until the desired char limit is met and track the index, and pop that many items. Or just add an extra check if first val is SOLUTION PART 2, though that wouldn't handle the case of them returning it as 2 tokens. (I initially thought maybe they add all stop words as single tokens to vocab somehow, but then I remembered only PART 1 and 3 versions are stop words, not 2.)
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/16/23 thurs
-------------
X -update roboduck cls to avoid printing the SOLUTION PART 2 part
    X -Debug algo to avoid printing SOLUTION PART 2
        X -print out cur on each iteration to confirm that openai actually sometimes returns SOLUTION PART 2 as a single token even though codex is supposed to tokenize it as 5 tokens
        _ -if ^ is true, update algo to grab items from start of queue until the desired char limit is met and track the index, and pop that many items. Or just add an extra check if first val is SOLUTION PART 2, though that wouldn't handle the case of them returning it as 2 tokens. (I initially thought maybe they add all stop words as single tokens to vocab somehow, but then I remembered only PART 1 and 3 versions are stop words, not 2.)
            > Decided to go with simpler approach (see below).
        X -implement simple approach
            > Wanted to keep this part simple for now. Might end up just having chatgpt/codex 3 api extract code portion once at end when magic is used in insert mode. For now, set colon right after SOLUTION PART N to try to avoid the issue of having a title of unknown content and variable size. Then just relied on assumption that gpt will continue to treat SOLUTION PART 2 as a single streaming step (it's still multiple tokens, but apparently they don't necessarily stream 1 token per step).
            X -tweak prompts
                > Added ANSWER KEY back to stop words and moved LOC (start of local variables) to logit bias.
            X -update magic to work with new solution
                > Had to keep SOLUTION PART 2 in codecompletioncache so we can split on it. Also stripped leading newline in inserted code cell.
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/17/23 fri
-----------
X -consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
    > Considered refactoring further to use ConversationManager but I think by project's end we'll switch over to the chatgpt api anyway which should add enough statefulness for us. Not expecting super longform conversations anyway.
X -cleanup funcs (lots of comments, todos, etc)
X -port everything to a lib
    X -consider if these will live in same lib, same module, etc.
        > Same lib, diff module. Jupyter extension stuff might go elsewhere but debugger and magic should be same lib because the magic just calls the debugger.
-document funcs/classes
-finish distill post on augmenting human intellect

2/18/23 sat
-----------
X -document funcs/classes
    > Documented three modules.
-finish distill post on augmenting human intellect
-start scoping out possible stack trace explainer
    -what is the exact use case - show better error messages all the time? Only if some env var is set? Only if we run a script with some special command? Or enter a conversational console on error? Static vs conversational is one of the first questions to answer.
    -write todos for next steps

2/19/23 sun
-----------
~ -install editable version of lib and test imports
    X -debugger works in jupyter
    X -magic works in jupyter
    ~ -debugger works in ipython
        > Live typing is kind of slow and choppy, but works.
    ! -magic works in ipython
        > We get AttributeError: can't set attribute when calling %duck magic. Caused by the line that sets shell.debugger_cls to RoboDuckDB, seems like ipython shell makes itself read only maybe? Weird that it works in jupyter though. I suspect we may also need to change debugger's method of reading in source code and possibly magic's method of handling insert mode.
-start scoping out possible stack trace explainer
    X -revisit exisitg debug_on_fail work
        > Actually ended up getting this working quite nicely. Ended up writing a nice monkeypatch and excepthook class that should probably end up in htools (currenly in debug_on_error.py). There's some interesting potential here but I'm not sure it's the right place to inject the LLM.
    X -what is the exact use case - show better error messages all the time? Only if some env var is set? Only if we run a script with some special command? Or enter a conversational console on error? Static vs conversational is one of the first questions to answer.
        > Key is we don't really want it to do this on every single error (esp in interactive shell), but want it to be as low friction as possible to use once we want to. I'm thinking maybe just catch errors using excepthook, print normal stack trace, and include a simple y/n input asking the user if they want it explained.
    X -look into prettyerrors source code
        > Actually looks fairly complex, may be easies after all to mess with sys.excepthook. Note that my current version of that doesn't seem to affect ipython, maybe it's already overridden sys.excepthook?
    X -write todos for next steps
-finish distill post on augmenting human intellect

2/20/23 mon
-----------
X -port new funcs from debug_on_error to htools
    X -monkeypatch
    X -excepthook (careful - as written, just importing it registers it. Maybe put in its own module or move to a method of a class that also lets you revert to default excepthook)
    X -get working in ipython
        > Not trivial, but got it working. Ipython does not use sys.excepthook.
-get editable version of lib working
    -magic works in ipython
        -fix line causing attributeerror in magic duck cls
        -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
        -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/21/23 tues
-----------
-get editable version of lib working
    ~ -magic works in ipython
        X -fix line causing attributeerror in magic duck cls
            > Still can't work in default ipython but we handle the error gracefully and provide a command that shows the user how to start an ipython shell that DOES make this magic available. Looks like minimal progress but this was actually a big win.
        -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
        -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/22/23 wed
-----------
X -get editable version of lib working
    X -magic works in ipython
        X -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
            > Only tries to load this if we set full_context=True. I added some documentation saying this is unavailable in ipython.
            > Update: realized it actually wasn't too crazy to make this work in ipython. Added utils func to load current ipython session and optionally format it and incorporated it into debugger cls, along with a new file_type "ipython session".
        ! -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
            > Interesting, it actually still works (i.e. inserts code into the new cell that appears when we exit the debugger).
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/23/23 thurs
-------------
~ -explore excepthook strategy for intelligent error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    ~ -see what default excepthook does
    ~ -try to reproduce that
    X -add in input asking for y/n
    X -color prompt to stand out from stack trace
    ~ -add in LM query if above is Y
        > Options:
            > 1. just enter debug session
                > Basically same as magic, though faster to enter would be nice. Still, maybe should differentiate this more.
            > 2. try to reuse roboduckdb cls but manually create a query (assume user question will basically be "what caused this error and how do I fix it?")
                > Probably ideal if possible, not sure if it is though.
            > 3. Manually call with jabberwocky.
                > Doable but clunky.
            > 4. Refactor jabberwocky call out of roboduckdb cls and call same helper func here.
                > Maybe the most likely solution ultimately.
            > For now, started with calling pdb.post_mortem with custom roboduck class. Works, though I do notice the answer was not good in this case and it looks like it *might* be caused by a missing global_var rather than just poor GPT.
                > Never mind, the global var is there. GPT is just wrong. Again, don't worry about this too much - chatgpt/codex3 would likely do better here.

2/24/23 fri
-----------
~ -figure out way to avoid making user type query. Recall main options are:
    -make new error prompt and just call gpt manually
    -refactor behavior out of roboduckdb cls
    X -figure out how to call roboduckdb.ask_language_model directly instead of making user do it (perhaps look into what that interact() method called inside post_mortem() actually does)
        > Essentially did this by adding commands to the debugger cmdqueue. This does what I envisioned but now I'm realizing it doesn't give gpt access to the stack trace. Pretty sure that should be possible though.
_ -provide shell shortcut command to launch ipython in mode that enables debug magic
    > Not sure this is actually better. Stick with current method for now.
    _ -implement
    X -update error message in debugger.py
        > Updated to include instructions on how to make setting persist (and to make it auto import the debug magic instead of just configuring ipython so that we're *allowed* to use it if we did import it).
X -allow user to pass kwargs into roboduck()
X -add get_ipython import in magic module (technically unnecessary, I think, but the missing import makes me a bit uneasy0

2/25/23 sat
-----------
~ -easy:
    X -document shell.RoboDuckTerminalInteractiveShell cls (very short, just explaining why this is necessary: ipython frozen attr etc.)
    -finish distill post on augmenting human intellect
    -watch bret victor Future of Programming vid
    -read Engelbart paper on augmenting human intellect
~ -talky errors 
    -goal: avoid user having to type question, while still passing in both traceback/error + state to gpt
        X -dive into traceback.print_exception to see where it's getting the error message + stack trace from
        -update funcs in scratch_errors.py to pass that info down chain of funcs so it can be included in prompt
        -update prompt.yaml or create new one to include (possibly optional) stack trace
        X -print out prompt and confirm it includes everything I want
        X -incorporate y/n prompt from nb01 excepthook (i.e. don't query for EVERY error)
        X -port over logic from htools.autodebug to work in non-ipython contexts too
        -revisit magic and see if we should use this new logic there too (bc it's always used when an error's just occurred)

2/26/23 sun
-----------
-talky errors 
    ~ -goal: avoid user having to type question, while still passing in both traceback/error + state to gpt
        ~ -update funcs in scratch_errors.py to pass stack trace down chain of funcs so it can be included in prompt
            X -write new stack trace debug prompt that includes stack trace var
        -update prompt.yaml or create new one to include (possibly optional) stack trace
        -revisit magic and see if we should use this new logic there too (bc it's always used when an error's just occurred)
X -replace some roboduck names w/ duck (duck is taken on pypi and I'm reasonably pleased w/ roboduck as a lib name, but it would be nice for the natural language debugger to be as easy to invoke as possible and duck is fewer chars)
    > Had to do this in jabberwocky prompts too.
X -generate roboduck mascot/image with midjourney/stable diffusion/dalle 2
    > Could still use some touchups - change green body to yellow, remove mystery object on ground (watering can?)

2/27/23 mon
-----------
X -talky errors 
    X -goal: avoid user having to type question, while still passing in both traceback/error + state to gpt
        X -update funcs in scratch_errors.py to pass stack trace down chain of funcs so it can be included in prompt
            X -debugger cls updates
                X -make cls allow user to specify prompt name (to support stack trace debug prompt) instead of just full=T/F
                X -make sure correct vars are passed in (depending on prompt's available kwargs?)
                    > Along the way, wrote new jabberwocky field_names utils func and added wrapper method to PromptManager. Haven't bumped version and uploaded to pypi yet though, because more changes are probably coming.
            X -update scratch_errors script to specify correct prompt (stack trace version)
        X -update prompt.yaml or create new one to include (possibly optional) stack trace
        > Works, although logic might be a little fragile. Also observed more annoying repetition because I'm limited to 4 stop words and STACK_TRACE adds another (and looks like logit bias isn't as effective as I'd hoped at avoiding LOCAL VARIABLES). But again, don't get bogged down here - chatgpt probably fixes this problem.
-revisit magic and see if we should use this new stack trace prompt there too (bc it's always used when an error's just occurred)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

2/28/23 tues
------------
X -talky errors 
    X -port to lib
    X -document
    X -check works in ipython
    X -check works in python script
    ! -check works w/ 'python -c "my code"'
        > Not yet, can't access source code.
~ -look into debugger bug when running code from terminal with 'python -c {my_code}'
    > Looks like this won't be trivial to solve - can't easily access source code w/ inspect or sys.argv, for example. Might be able to get it from appropriate frame object. Look into this more tomorrow.
X -add error handling to htools autodebug ipython exception registration
-revisit magic and see if we should use new stack trace prompt there too (bc it's always used when an error's just occurred; alternatively, could maybe make prompt depend on flag)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/1/23 wed
----------
~ -revisit magic and see if we should use new stack trace prompt there too (bc it's always used when an error's just occurred; alternatively, could maybe make prompt depend on flag)
    > Wrote new partial stack_trace() to get stack trace as str manually.
    > Added auto flag in magic to allow user to skip the interactive session and just ask the default question.
    > Added require_confirmation flag in excepthook to allow us to skip user confirmation, like in the magic auto mode (bc user already requested it).
    > Tested magic auto flag in jupyter and it works!
    > Still need to add support for using auto mode and insert mode at the same time in magic - currently auto mode does not insert. Could also just not support this and raise warning - could be rather tricky given how differently I implemented the different modes.
-figure out how to access source code when running code from terminal with 'python -c {my_code}'. (Gpt suggests maybe can extract from frame obj.)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/2/23 thurs
------------
X -add support for using auto mode and insert mode at the same time in magic (currently auto mode does not insert. Could also just not support this and raise warning - could be rather tricky given how differently I implemented the different modes.)
    X -look into how difficult it might be to support this
    X -implement (either add support or raise warning)
    > Implemented. Not hard after all - since I managed to reuse debugger cls in errors module, it already sets CompletionCache attr.
    > Also renamed some flags, confirmed they can be called together (e.g. "-ip" instead of "-i -p"), and revised docstring accordingly.
X -filter warning in magic when registering excepthook func due to errors module import
! -figure out how to access source code when running code from terminal with 'python -c {my_code}'. (Gpt suggests maybe can extract from frame obj.)
    > Continued trying but really seems difficult (maybe impossible? Surely there's SOME way but it might not be doable without some non-python hacks, idk). Tried to hack something together to use ~/bash_history but that seems unreliable and it doesn't work anyway.
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/3/23 fri
----------
X -prototype json/yaml version of prompt w/ chatgpt
    > Prototype in roboduck/data/scratch/debug_prompt_yaml_response. Still need to test with api though. I initially wanted json response but chatgpt was having a really hard time keeping python indentation in json - I'm guessin because dumping code to json removes tabs by default and so a lot of code in json training data probably didn't have much indentation. But yaml works pretty well so far and the human readable aspect is actually pretty nice given than the response is basically a bunch of natural language.
    > ðŸ¤” Now I'm wondering if the increased parsability of a highly structured response is worth it - it might require us to wait for the whole response rather than showing live words, and more waiting is annoying. Could put the "solved" field first but the whole point was to try to make it "think" first before reaching a diagnosis.
_ -debug why completions often include leading newline when I thought I stripped them (maybe need to check on first few i rather than just i=0?)
    > Wait on this - would probably become a non issue if I switch to yaml. Same with addressing the repeated newlines at the end of a prompt - gpt3 did this a lot lately but chatgpt shows no signs of doing that.
X -cleanup repo a bit
    > Moved some old scratch code files to new roboduck/data/scratch dir.
-think about some early stage ideas:
    -maybe error module should add gpt response to stack trace like raise RuntimeError(gpt_response) from e?
    -potential logging module: what would this look like? More informative logs would be nice, but maybe this could just entail importing error module and logging the new stack trace that involves gpt's response (see bullet above)
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/4/23 sat
----------
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI
~ -think about some early stage ideas:
    ~ -maybe error module should add gpt response to stack trace like raise RuntimeError(gpt_response) from e?
        > Took first stab at implementing this but there are still some possible kinks to work out. I tried to make post_mortem do this but I realized in error mode, we still require the user to hit -y so it's hard to test automatically. Also, sys.last_value etc doesn't seem to be getting updated anymore - maybe bc the default excepthook does that somewhere and we overwrote that?
    ~ -potential logging module: what would this look like? More informative logs would be nice, but maybe this could just entail importing error module and logging the new stack trace that involves gpt's response (see bullet above)
        > Looked into logger module a bit. A few ways to do this (custom logger, custom LogRecord, etc.) but I think a better way might be to start by having our custom error mode update the exception obj to include the explanation so it automatically gets logged.
X -change or document that question is ignored by default in post_mortem
    > Was going to remove but realized prompt is still a WIP and we can specify different prompts, so it's plausible that a future change could make this useful. Just documented.
X -document debugger precmd method
-touch up roboduck mascot image
    -change green body to yellow
    -remove mystery object on ground (watering can?)

3/5/23 sun
----------
X -Look into error when importing roboduck.errors in ipython session without special flag (AttributeError: module 'sys' has no attribute 'last_type'. Caused by the stack_trace partial def. Maybe just wrap that w/ better error message?)
    > Change from partial to function with better error message. Sys.last_type is often not defined at import time and partial definition requires it to exist already.
X -exploring support for auto updating exception str content after writing completion
    _ -look into way to allow user to set requires_confirmation=False (for possible logging in production as opposed to interactive dev)
        > I think the main use case is for logging, so maybe just make a custom logger that works a bit like the jupyter magic and calls excepthook manually.
    X -look into how sys.last_value etc. are set. Maybe I can manually update that rather than my current exc.args mutation in post_mortem? Prob would be ideal to do this in the debugger cls rather than postmortem bc postmortem only affects error mode
        > I set these 3 values in sys.excepthook which seems to be where this occurs normally (indirectly, in a called function). I update the error message in post_mortem, not debugger cls, because exiting debugger sets pdb related values for sys.last_type etc anyway.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/6/23 mon
----------
~ -logging module
    X -write errors.enable() func
        > Decided I didn't agree with yesterday's decision to avoid this, I think it as mostly out of a desire to avoid refactoring errors module. Rewrote this and confirmed it works in ipython.
        > Also let user pass in Pdb cls.
    X -look into base logger class to see what options are for where to slot in the gpt completion
    X -select first option to try (can always revisit decision if it ends up not working as nicely as expected)
        > Override _log in Logger subclass.
    ~ -implement
        > Sort of works, but sys.last_value etc aren't getting updated. Need to debug further.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/7/23 tues
----------
~ -logging module
    X -investigate why sys.last_value etc aren't being updated when using new logger cls in logger.py
        > Actually, issue was that logger called my excepthook with sys.last_value etc and that was wrong because it now only gets updated INSIDE the excepthook itself. Managed to extract those args from the exception obj.
    X -fix
    > Parametrized sleep between chars in debugger and added option for silent output. Logger seems to work, though could use a couple refinements/default tweaking.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/8/23 wed
----------
X -document new silent param in debugger cls (and possibly in errors module kwargs?)
X -logging module
    X -custom logger seems to lose some default formatting, e.g. starting messages with time or INFO/WARNING/etc. See if we can fix that.
    > Also added similar functionality as MultiLogger to make it easy to configure logging to stdout and/or file.
X -removed old commented out quickmail func version from htools
    > Also uploaded 7.5.0 to pypi.
-jabberwocky changes to support chatgpt
    -think openai lib version needs to be updated, make sure things stil work afterwards
    -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
    -update enginemap engines if necessary
    -update GPTBackend.query if necessary
    -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
    -try out new debug prompt (data/scratch/) from api instead of UI

3/9/23 thurs
------------
X -easy:
    X -document logger cls + init
    X -document other classes
~ -start playing around with langchain
    > Realistically, I should probably eventually replace my jabberwocky usage with langchain. Don't do that yet though - go through some more tutorials and toy around a bit more to make sure it will be able to do the things I want.
X -medium:
    _ -create a more minimal install for jabberwocky (no youtube transcription stuff, for example)
        > Hold off while considering move to langchain.
    X -touch up roboduck mascot image
        ~ -change green body to yellow
        _ -remove mystery object on ground (watering can?)
        > Experimented a bit in gimp. I'd need to get a lot better to really do what I want, and I'm running into a known fuzzy select bug on mac. Made a more blue and yellow variant but the original is kind of growing on me. Leaning towards just keeping it.
        > Also made a more grayish version online. Now I'm kind of digging the blue one. And I realized the watering can thing may be supposed to be something a mechanic would use to oil robots? That sort of makes sense. Changed readme to use blue version for now, will see if I like it.
-hard:
    -jabberwocky changes to support chatgpt
        -think openai lib version needs to be updated, make sure things stil work afterwards
        -review docs on Chat completion vs completions. How much does jabberwocky api need to be updated? Maybe could largely revert to just openai api to minimize deps?
        -update enginemap engines if necessary
        -update GPTBackend.query if necessary
        -try out chat endpoint. Interface is a little different (e.g. takes list of messages?)
        -try out new debug prompt (data/scratch/) from api instead of UI

3/10/23 fri
-----------
-play around with langchain more with eventual eye towards possibly replacing jabberwocky
~ -start adding tests
    X -pick framework (prob pytest)
    X -look up examples
    ~ -write tests
        > Was starting to question if I really want to do this but testing first module already paid off - second half of truncated_repr seemingly got lost when copy/pasting from jupyter to lib ðŸ˜³. Seems fixed now.
    -add CI/CD on push/merge?
~ -brief glimpse of old reload.py script
    > Lots of todos here, still very WIP. Think it is worth trying to scavenge some things from this, but could consider designing new solution first and then seeing how it differs. Might find something better or more elegant, like how my autodebug module worked way better by starting over.
-passive:
    -finish distill post on augmenting human intellect
    -watch bret victor Future of Programming vid
    -read Engelbart paper on augmenting human intellect

3/11/23 sat
-----------
X -peruse saved git repos around auto refactoring, stack overflow errors, live program tracing
    > Livepython tool is interesting, seemingly executes code line by line and lets us pause in between. Seems like it might allow for something like: try to execute this line, if it fails debug it and try to fix it, then run the fixed code. But maybe it's unnecessary - sys.excepthook already executes on errors, and sys.settrace seems very problematic in ipython.
X -func to color new parts of a str differently (similar to colordiff)
    > Still need to incorporate this into...somewhere. Might lend itself better to a json/yaml response where we know exactly where the code starts and ends. (Granted, we could probably ask gpt to add a field called "diff" but I guess that introduces more chances of inconsistencies.)
-continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/12/23 sun
-----------
X -consider where (and how) we might incorporate new colordiff func
    > Errors module (enabled by default) and logging module (disabled by default). Not compatible with live typing and probably not ideal when logging to file, but works nicely in error message. Note that both of these modules previously ignored code completion entirely, but I decided it's worth keeping.
    > Note: do I need to update response color in debugger and/or errors module? I realized whole response is currently printed in green, hiding the code diffs. It's visible later when logging/printing the exception. But I guess diffs aren't supported for live typing anyway. Could enter color mode once we hit SOLUTION PART 2, I suppose, but that feels like it will require some custom queue/trie logic that will probably disappear once I update the prompt anyway. Hold off for now.
    > Ended up adding a bunch of new attrs to cache, which does make it feel more useful. Also added a reset() method to set all of them back to empty strs, which magic does.
-continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/13/23 mon
-----------
~ -continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
    > Messed around a bit with basic chains, tuned my cs agent -> PR -> json pipeline a little bit, tried writing a chain that parses a hopefully json str response to a dict, toyed around with some Memory implementations (perhaps less necessary due to chatGPT, though still useful for long conversations).
    > One learning: a Chain actually seems to be somewhat synonymous with the heavier weight Prompt class I envisioned for jabberwocky 3.0. E.g. you import an entirely different class to perform a Summarization task. Don't necessarily love that but I guess it helps define pre/postprocessing logic (but still...couldn't you do something like Task('summarize') and then have Task create a Summarize obj? Guess that's similar functionally but the UX feels a lot nicer to me. No forest of imports.).
    > Looked a little bit at async stuff too but didn't try this yet. For some reason I'm kind of surprised they didn't go with threads - seems like using async has major repercussions bc IIRC async funcs must be called by other async funcs, and in my experience they can be quite finicky when working with CLIs (or maybe that's fire-specific).
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/14/23 tues
------------
~ -continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
    X -vector DB/doc search
    X -youtube
    > Spent >2 hours playing around with this, lots of fun. Doc search may be less necessary now given increased context length, but still a nice cost saving measure AND gives potential to scale up to more ambitious use cases, perhaps.
    -agent
    -tracing
    -chatgpt (messed with this a little bit but not enough, only tried single turn)
-consider: 
    -how much time/effort to replace jabberwock with langchain in roboduck?
    -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
    -key question: whether jabberwocky or langchain, do I want to sacrifice live typing for cleaner structured yaml/json output?
        -and do I want/need to change davinci to a chat model? RLHF probably improves quality quite a bit.
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/15/23 wed
-----------
 X -continue langchain colab nb with eventual eye towards possibly replacing jabberwocky in roboduck
    X -agent
    ~ -tracing
        > Read through demo nb, looks like in practice this just refers to running an agent (or perhaps a generic chain) with verbose=True. NB doesn't actually import anything from a tracing module.
    ~ -index (is this that different from the doc search? Maybe a way to speed that up?)
        > Seems like index is the higher level api wrapped around document stores/vector dbs. Read through demo nb but no need to run code rn.
    X -chatgpt (messed with this a little bit but not enough, only tried single turn)
        > Since I've used their chatmodel a bit at this point, I took the opportunity to lump this in with exploring their streaming capabilities and so far it's not super encouraging - not really built to yield values.
        > Actually maybe it's fine, just a different interface. Should be able to implement whatever logic you want in the on_new_token cls, presumably - may just need to pass in more vars from global state.
    > Think we've done enough langchain aimless exploration for now. Starting tomorrow we need to consider whether to rewrite roboduck with this or not and whether to use this in next part of project (or next project entirely). Currently leaning towards just keeping jabberwocky for existing roboduck stuff and then using next proj as an opportunity to transition to langchain. The main drawback is that jabberwocky doesn't currently support chat models and those would probably be a lot better than what I'm currently using. But I think we could get 99% of the way their with the 3.5 models and perhaps a hacky use of ConversationManager for the NL debugger. Errors/logging/magic prob don't really need any conversational state, really.
-consider: 
    -how much time/effort to replace jabberwock with langchain in roboduck?
    -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
    -key question: whether jabberwocky or langchain, do I want to sacrifice live typing for cleaner structured yaml/json output?
        -and do I want/need to change davinci to a chat model? RLHF probably improves quality quite a bit.
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/16/23 thurs
-------------
X -add getLogger() func to logging module to match builtin logging interface
X -rename RoboDuckDB to DuckDB in all modules
X -document lib modules 
X -add missing notebook summaries
X -make readmes
X -cp langchain colab to scratch nbs in repo
    X -rm ALL api keys
    X -download and move to repo
-consider: 
    -langchain usage
        -how much time/effort to replace jabberwock with langchain in roboduck?
        -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
        -confirm how easily jabberwocky works w/ 3.5 models (at least prob should update some name maps in EngineMap? And support for engine int values will probably start fading away)
        -briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
    -do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
        -consider tradeoffs
        -make decision
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/17/23 fri
-----------
-consider: 
    X -langchain usage
        X -how much time/effort to replace jabberwock with langchain in roboduck?
            > Changes would at least be pretty well contained to the debugger module (the debugger class, in fact). Not huge lift I suppose. The bigger issue may be that langchain seems to define jabberwocky prompts as classes that need to be imported and perhaps instantiated with different kwargs (?). Probably could recover my interface if I really wanted and just do something like getattr(langchain.prompts, name) or whatever their current interface is. I suppose I'd probably need to implement some custom prompts there but that's not a huge lift.
            > Implementing streaming their way might be a little bit annoying - maybe it would be fine but I could imagine having to implement a different callback for each module that uses a prompt (maybe?). Again, probably not that bad given how self contained the debugger module/cls are.
            > Estimate: might add on the order of 1 month? With a pretty wide prediction interval around that, say 1-12 weeks.
            > Buuuut if I don't do this I may need to update jabberwocky to work w/ chatgpt, gpt4, etc. That might take a similar amount of time if not longer.
        -are there any features I'd lose in langchain w/ default functionality? Would those be implementable via custom code?
            > Pretty sure but not certain that streaming would still work. Pretty sure but not certain that my interface of passing prompt name to debugger class or calling functions would work (though in practice the user should probably never do that).
        X -confirm how easily jabberwocky works w/ recent openai updates
			X -3.5 models
			X -4 models
			X -chat models
			> Details in nb. Not supported but not too crazy to add that.
        -briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
    -do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
        -consider tradeoffs
        -make decision
-confirm if I remove duck() call from source code
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/18/23 sat
------------
X -confirm whether I currently remove duck() call from source code in debugger cls source code loading
	> Looked through code briefly and didn't see it. Confirmed by testing dev mode in ipython shell, probably more reliable/comprehensive check than reading code. Uncovered new issue that updated openai api breaks jabberwocky, though.
X -if necessary, remove duck() call from source code (full and/or partial, as necessary)
	> Confirmed works in ipython.
X -look into newly discovered openai error on query
	> Seems to be resolved by restarting ipython. Guessing that was a long running session and autoreload didn't work, just like in jupyter. I imported jabberwocky afterwards in the same new session (after importing openai and confirming that query worked) and querying with it worked so I think we're good.
-consider: 
	-briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
	-briefly try out new debugging prompt in langchain. How to do this, how much work?
	-make decision on first approach to work on: expanding jabberwocky functionality or replacing jabberwocky usage w/ langchain in roboduck
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/19/23 sun
-----------
X -briefly try out my idea of using ConversationManager with a non-wiki bio, but rather an AI tutor prompt
	> Does seem promising. Some kinks to work out around when to pass in code context vs. just user question, when to use conv manager vs prompt manager vs actual chat model, also need to support new 3.5 and 4 model names. But I like the experience of being able to question gpt about its solution, which requires us to maintain some state.
~ -consider: 
	-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
	-briefly try out new debugging prompt in langchain. How to do this, how much work?
	X -make decision on first approach to work on: expanding jabberwocky functionality or replacing jabberwocky usage w/ langchain in roboduck
			> Start w/ updating jabberwocky, I guess. Would be nice to have that up to date for alexa too, potentially.
~ -jabberwocky updates
	X -support 3.5 engine names
	X -support 4.0 engine names
	>>> Tentatively added support but realized those new models don't seem to support the completions endpoint, only chat. Could get a little hairy auto-delegating to them.
	-support chatcompletion usage
		-via convmanager?
		-or via new cls?
		-or leave out of jabberwocky but use native openai implementation in roboduck?
		-rename gpt3 refs to gpt?
-add CI/CD to run tests on push/merge?
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?

3/20/23 mon
-----------
 -consider: 
	-how to support chatcompletion usage in jabberwocky (recall 3.5/4/turbo models do not support completions endpoint after all, just chat)
		-via convmanager, delegate to another helper method when recognizing a chat-like model name in kwargs?
		-or via new chatmanager cls?
		-or leave out of jabberwocky but use native openai implementation in roboduck?
		-or leave jabberwocky for older versions of openai and bite the bullet and shift over to langchain now
	-rename gpt3 refs to gpt? (If so, remember to update both lib and alexa. Ignore gui)
	-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
	-briefly try out new debugging prompt in langchain. How to do this, how much work?
X -add CI/CD to run tests on push/merge?
	> Added yml file. Not sure yet why not running.
	> $default-branch variable didn't work. Needed to hardcode to 'main'.
	> Build passes in <2 minutes. Will get slower as (if?) I add tests. But I'll probably also pare down requirements which should help speed things up a little. Also could consider pushing to dev branch and only merging periodically since it kind of annoys me that it runs tests on every single push atm.
X -flatten dir structure

3/21/23 tues
------------
 ~ -consider: 
	X -how to support chatcompletion usage in jabberwocky (recall 3.5/4/turbo models do not support completions endpoint after all, just chat)
		! -via convmanager, delegate to another helper method when recognizing a chat-like model name in kwargs?
            > Bc of how we select query_func in GPT.query, that doesn't quite seem like the right UX. GPT._get_query_func would need to select different func based on model name (probably), OR make query_gpt3 func perform the delegation to chat_model. Would need to repurpose prompt param for `messages` which would be a little weird and not sure if all params carry over. Thinking it might be cleanest to just leave jabberwocky as is supporting pre-chat models and move to langchain from now on? (Though really, most if not all queries here COULD be done w/ non-chat models. But the more powerful models seem to be only available as chat endpoint so it's probably still very much worth using them even if the stateful aspect is relatively unimportant.)
		! -or via new chatmanager cls?
            > I guess we could do this but I think at this point it would be easier to just ignore existing jabberwocky stuff and wrap openai ChatCompletion itself, in which case we might as well not even put it in jabberwocky.
		! -or leave out of jabberwocky but use native openai implementation in roboduck?
            > Possible, but using roboduck as a reason to get more familiar with langchain isn't such a bad idea.
		X -or leave jabberwocky for older versions of openai and bite the bullet and shift over to langchain now
            > Let's at least give a custom prompt/chain a quick try.
        > Leaning no chat support in jabberwocky. Langchain or something similar will probably be big, no use resisting.
	-rename gpt3 refs to gpt? (If so, remember to update both lib and alexa. Ignore gui)
	-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
~ -briefly try out new debugging prompt in langchain. How to do this, how much work?
    > Not to bad to get the basic mechanics working, but haven't really gotten into refactoring or making the devX nice yet. Prob should to assume we'll just always use chat models at this point - from a tiny sample, turbo's outputs do seem like they might be a bit easier to parse reliably than davinci's.
-check relative prices of new models
    > turbo is 0.2 cents per 1k tokens, equivalent to curie or 1/10 of davinci or 5x ada. Not too bad. Gpt4 is pretty expensive though, 3 cents per 1k prompt and 6 cents per 1k completions so ~20+ x turbo or 2x davinci. Takeaway: basically never makes sense to use davinci?
X -upgrade env to use py 3.8
    > Have to use py38 conda env now to get new-ish version of langchain. Also updated github workflow to py 3.8.

3/22/23 wed
-----------
X -revert jabberwocky to prev commit (before enginemap changes and version bump). Rules still seem to apply to completion endpoint.
-explore possibility of 2 diff types of user messages - 1 with full context, 1 with just question
    > Feel like if we're in debug mode and user asks a followup question, we don't need to waste tokens providing so much info again. Maybe can check if we're in the same frame as before to infer this.
-continue nb04 langchain
    -figure out a good way to define multiple chat prompt variants (e.g. py file, yaml file, what fields are needed, etc. Take inspo from langchain native method since they provide some out of the box)
    -Q: what is difference between what I do in nb04 manually appending to messages list vs what I did in nb02 with ChatHistory obj? Is that basically doing the same thing under the hood?
    -where to provide stop words? And would it be useful/effective to ask gpt to provide specific stop word when done to help it stop when appropriate?
    -Consider breaking down into chain? (Might be easier than trying to get it to generate valid YAML/JSON, also easier to parse. Could even do this concurrently, though prob better to not do that so code solution can see explanation first.)
        -EXPLANATION -> CODE
        -fancier: THOUGHT PROCESS -> EXPLANATION -> CODE


Backlog
-------
-maybe rename gpt3 refs to gpt? (If so, remember to update both lib and alexa. Ignore gui)
-do I want to sacrifice live typing for cleaner structured yaml/json output? (Possibly orthogonal to langchain decision.)
-think about what I want to achieve with auto bug-fixer POC and/or LLM native programming language
    -what is the desired experience?
    -what are some possible approaches to achieve this?
    -what is a realistic timeframe? Should I leave this for a future project?
-passive:
    -finish distill post on augmenting human intellect
    -watch bret victor Future of Programming vid
    -read Engelbart paper on augmenting human intellect
-investigate: was it intentional that PromptManager supports passing in GPT obj but ConversationManager doesn't? (I think initially neither did but had an immediate use case for PromptManager so changed that one, but haven't had a need yet for ConversationManager so didn't prioritize it. Probably good to do eventually though.)

Easy Backlog
------------
-write more tests

Keep an eye out for
-------------------
-debug slowness when using magic (is it calling query multiple times?) ~ - add option to add new cell w/ gpt-fixed function below (may need to adjust prompt a bit to encourage it to provide this)
    > 2/13/23: Haven't observed this in this round of work on the project, leftover from last time. Maybe just a transient issue w/ the codex API at the time?
-codex completion quality issues
    > 2/13/23: seeing a lot of repetition lately. Seems like it came after I upped the freq penalty so that seems odd, but maybe the more salient change is that at some point I removed docstring quotes as a stopword (bc we may need to generate it sometimes).

Won't do (for now)
------------------

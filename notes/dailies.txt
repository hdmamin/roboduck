2/1/23 wed
----------
X -settle on project
X -find all old relevant work
    > added 3 relevant files to this dir for now. The fourth (and perhaps most developed) effort is in the ~/nbextension-duckling dir.
~ -decide how to structure dir(s)/repo(s) (Will prob have multiple semi-related outputs, perhaps 1-5.)
    > Tentatively thinking monorepo. Easier to track everything that way.
-pick repo name
    > cairina, from "cairina moschata", one of the biggest, most powerful looking species of duck ("muscovy"). Think (rubber) duck on steroids.
X -add readme img
    > :D
~ -read distill AI-augmented human intelligence post

2/2/23 thurs
------------
X -premortem
X -map out very tentative plan for order to take things on (e.g. debugger -> jupyter magic -> interactive error messages -> nbextension -> reloading stuff -> LLM native programming language)
    > Lets start with debugger, then jupyter magic. The former is pretty far along and the latter should largely build on top of the debugger. We can take stock after that - depending on timing, can take a break or fit in error messages before break or move on to one of the bigger rocks (live reloading stuff or nbextension) if it went surprisingly fast.
X -consider whether notebook extension proj should continue or start over as jupyterlab extension
    -find stats on how commonly used each is?
        > In 2019 jetbrains python dev survey, DS use cases skewed twoards notebook (13% vs 5%; if we normalize to avoid non-jupyter options, that becomes 72% and 28%). By 2020 survey, it was 11% vs 6% (65% and 35% normalized). In 2021, they didn't break down results by DS users, so it's just 3% vs 2% (60% vs 40% normalized). In the "additional but not main IDE" question, it was 25% vs 12% notebooks (68% vs 32% normalized). In kaggle 2022 survey, 66.5% used nb while 23.7% used lab (74% vs 26% normalized). So (somewhat surprisingly) it seems like notebook is still considerably more popular, though perhaps slowly shifting towards lab. I like nb and started building in that so it's tempting to continue in that, but I guess I don't need to decide quite yet given that we're starting with the debugger.
~ -create cookiecutter template for each major project
    > Just start with debugger subdir (now "roboduck", i.e. a more powerful rubber duck). Uploaded to pypi to claim name.
-finish distill AI-augmented human intelligence post

2/3/23 fri
----------
X -remind myself where I left off w/ debugger work
    X -run existing code (do)
    -skim over existing code (read)
    X -read through done and outstanding todos
    X -write some todo items for what comes next
X -possible bug: seems like if we haven't saved nb since latest changes, load_ipynb func can load an outdated version (possibly? Though tit did that once but then I looked again and it seemed to be up to date so idk)
    > Confirmed that was happening but fixed it w/ help of a stackoverflow function.
X -consider adding option for "I don't know". Or maybe something like "If you don't know what's causing the bug, say "I don't know". Then write a list of 5 plausible causes that the developer can check for when debugging." (take advantage of its strength at generating lists, thinking of possibilities we might not)
X -update prompt to explain to gpt what the chat_db()/roboduck()/rbd() func call is
X -rename usages from llmdb to rdb or similar?
X -update func names to fit roboduck lib
X -add stop word to prompt to prevent starting another question
-finish distill AI-augmented human intelligence post

2/4/23 sat
----------
X -see if we can color user/model turns differently. (Bit hard to read atm, though partly bc I'm printing out full prompt for debugging purposes.)
X -add backup value when response is nothing
X -debug why I'm getting so many empty responses
    > Turns out I had docstring quotes in my stop phrases which can be problematic since that is a valid thing to generate at times. Also didn't start my prompt with docstring quotes which maybe degrades quality a bit. But if I add them back in, then I feel like I need it in stopwords again. Hmmm.
X -make codex respond in second person
X -figure out how to make multiline str start with quotes in yaml (or abandon their use)
    > Turns out my old method already supports that. Just don't escape them.
-consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
-hide user warning about using codex model name.

2/5/23 sun
----------
X -consider removing stop seq """ from 3 debug prompts (in case it actually needs to generate that w/in solution)
    > Yes, try it out. Just have roboduck rstrip them.
    > Update: with this format, codex seems to like closing the docstring before doing anything else, then jumping straight to code, meaning no explanation gets written. May need to revert to old no-docstring method.
    > Also realized I haven't been bumping the prompt versions. Whoops. I guess maybe this only needs to be done when I actually bump the jabberwocky pypi version and/or the roboduck pypi version.
_ -consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
    > I think for now let's leave this be - I'll be seeing a lot of query results over the next couple months so it will become apparent if this is something we need (i.e. if quality is an issue). I also suspect we may see a chatgpt/gpt4/codex2 release that makes this type of prompt engineering less necessary before the end of this project, so no need to optimize that dimension now.
X -hide user warning about using codex model name
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.

2/6/23 mon
----------
X -try reverting debug prompts to no opening docstring (see sun notes for why)
    > Nope, tried this again but it really seems like it needs the docstring otherwise it just jumps straight to code. Added logit bias instead of stopword. Also add another stopword since I observed some cases where it skipped repeating the DEVELOPER QUESTION but did repeat LOCAL VARIABLES and everything afterwards.
X -see how chatGPT does
    > As expected, noticeably better. Codex seems to like repeating things and writing lengthy answers here even though I added the word "concise" to the instructions. I bet a better code api will become available within the next few months.
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
X -stream completion instead of waiting for whole thing
    > Added streaming with .02s between characters. Maminbot used .015 but somehow even .02 seems really fast in jupyter. Maybe streamlit had extra latency so it wasn't really .015s there.

2/7/23 tues
-----------
~ -consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
    X -write draft of truncated_repr func
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/8/23 wed
-----------
X -add file from jabberwocky (I think?) to avoid counting jupyter notebooks in github code % count
-handle huge data structures (big df, long list, etc.) 
    X -handle case where seq[:1] still produces repr that's too long
    X -test behavior on dfs again (potential opportunity to do something custom like "pd.DataFrame([col1, col2, ...])"? As in literally that string, just call truncated_repr on cols.)
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/9/23 thurs
-----------
~ -handle huge data structures (big df, long list, etc.) 
    X -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it would be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
        > I guess I intentionally made dotdict have the same str and repr as a regular dict so it's tough to come up with a rule that would help here. I could change its str/repr, just leave it as the regular dict version, or update my debug prompt to show the type of each var too (could be a separate section like GLOBALS_TYPES, could be a comment after each value in GLOBALS like `'nums': [1, 3, 4...], # list`. Last idea is interesting.
    X -try out idea about specifying var types in comments
        > Wrote new func. I like this.
    X -fix bug: empty sets represented wrong
        > Because empty lists and tuples use literals like [] but sets use 'set()' so my logic around the last non-brace index failed. Added some hardcoded logic around setes.
    X -fix bug: dict-like objects have extra quotes
        > Must have added this mistakenly, fixed now.
    X -tweak debug prompt to acknowledge that roboduck func may not be present (in case we repurpose these)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/10/23 fri
-----------
X -handle huge data structures (big df, long list, etc.) 
    X -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
        > Just hardcode for now. There's not going to be some magic number here that's perfect.
    X -document (mention that length is more of a rough guideline, not strictly enforced)
_ -consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
    > Actually I think this is pretty good.
X -tweak prompts to hopefully reduce repetition a bit (been noticing this more lately)
    > Added frequency_penalty of 0.2 (pretty minor change for now, range is -2 to 2). Haven't tested yet, just keep an eye on things as I continue working on it. Don't want to sink too much time into selecting perfect prompt params now bc I think better models may be available soon.
~ -maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
    > Tried replacing load_prompt w/ PromptManager and realized the issue is that I wnat to check the resolved prompt, and pm.query() with verbose mode just prints. But found pm.prompt() and pm.kwargs() methods so it is possible. However, we have to pass GPT obj to PM on instantiation so if we want to support diff backends in roboduck init, we still need to create PM inside it. So not really saving any time. But maybe this work will help pave the way if I want to transition to ConversationManager at some point.
    > Haven't tested this yet but it's ready to be tried. I kept the old cls around in case I want to revert (w/out messing w/ jupyter/git interaction weirdness).
-try running magic again and see if problems arise

2/11/23 sat
-----------
X -handle pandas series separately in truncated_repr (noticed it was not handled that well, was truncating the metadata for some reason)
    > Also refactored func a bit.
X -try running new version of roboduck with PM (just run all cells from cls def down to debugging session cell)
    X -debug if necessary
    > Had to fix both prompt() and query() calls, they take slightly different args than GPT.query() which in hindsight is maybe not ideal but I guess part of the point is that it's supposed to simplify the call and if it had an identical interface it wouldn't be any easier. Works now, though codex's answer was very wrong.
-consider whether to try replacing PM with convmanager
-if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
-try running jupyter magic again and see if problems arise

2/12/23 sun
-----------
X -add my jabberwocky pre-commit hook to avoid pushing api key
_ -consider whether to try replacing PM with convmanager
    > Don't want to get too bogged down on exact interfact yet. Don't know if upcoming chatgpt API might have slightly different UX (e.g. conversation_id) - this could be used in place of my conversationmanager cls, which is more helpful for the alexa skill with its auto-persona generation via wikipedia.
_ -if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
~ -try running jupyter magic again and see if problems arise
    > Existing code worked fine. Taking another stab at setting shell attr instead of global var to help with code insertion but not working yet.
    > Confirmed that I CAN set a var ;ole get_ipython().xyz = 123 and it persists (just working in global scope for both).
    > BUT if I set it in RoboduckDB.ask_language_model, it does not seem to persist by the time the magic checks it. Need to investigate more. I think this is similar to the issue I ran into last time. Maybe shell obj is recreated every time we change scope or something?

2/13/23 mon
-----------
X -debug failure to set get_ipython() var in certain contexts (see yesterday's last few bullet points)
    > Realized there's no real reason this has to be set on the Ipython object. I ended up creating a cache class and setting a var there - not sure if this is meaningfully better than global (we're just editing a global class var, I suppose) but maybe it is, idk.
X -add "keep an eye on" and "won't do" sections to dailies
~ -extract code from gpt completion when inserting code cell
    > Tried ast.parse-based method, pretrained huggingface classifier method, considered ways of breaking down prompt into 2 to make extraction easier. Ended up implementing method that makes a second codex call but only when the magic exits, not on every conversational turn, that extracts code. Seems to work alright in playground but need to debug a bit more - code snippet is no longer being used to populate cell below. Also, without live typing it feels a lot slower.
-finish distill post on augmenting human intellect

2/14/23 tues
------------
X -debug code extraction in magic: why is it failing to populate next cell now?
    > First val returned by gpt.query is list, had to extract str.
X -consider other appraoches to code extraction (really feel like a simpler version relying on templating should be possible)
    > Have prompt ask for a two part solution and title the sections carefully to try to encourage this separation. Seems promising so far. Had to add some new stop words. Just updating debug and debug_full prompts atm, leave duckling prompt for later.
~ -update roboduck cls to avoid printing the SOLUTION PART 2 part
    > Did planning but no implementation yet. A bit tricky since we're printing one token at a time and SOLUTION tokenizes into ['S', 'OL', 'UTION']. It becomes very slightly less complex if we go lowercase (['s', 'olution']) but even so we probably need a solution that builds up a queue of tokens to print and prints with a slight delay (i.e. always check the last x tokens, prob 4-8, before printing. If we find ourselves in the SOLUTION PART 2 phrase, we pop those tokens off and don't print them until we hit a newline).
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/15/23 wed
-----------
X -move testing cells to bottom of nb to avoid re-run on kernel restart
    > Frequent restarts are necessary to make magic update, even if we manually delete it before registering it again. Weird that this issue just popped up. Wonder if related to deleting cookies somehow? Seems unlikely. Better solution is to do as much of debugging as possible in Roboduck cls and just update magic occasionally once a big chunk of progress has been made.
~ -update roboduck cls to avoid printing the SOLUTION PART 2 part
    ~ -implement algo from yesterday: SOLUTION tokenizes into ['S', 'OL', 'UTION']. It becomes very slightly less complex if we go lowercase (['s', 'olution']) but even so we probably need a solution that builds up a queue of tokens to print and prints with a slight delay (i.e. always check the last x tokens, prob 4-8, before printing. If we find ourselves in the SOLUTION PART 2 phrase, we pop those tokens off and don't print them until we hit a newline).
        > WIP. Quite tricky. Getting close now - remaining bug is that openai seems to (at least sometimes) return "SOLUTION PART 2" as a single token (or else I'm unintentionally converting it before adding it to the queue somehow?). Think the solution is to first confirm that that's actually happening (i.e. print out cur on each iteration). Then if that is true, update algo to grab items from start of queue until the desired char limit is met and track the index, and pop that many items. Or just add an extra check if first val is SOLUTION PART 2, though that wouldn't handle the case of them returning it as 2 tokens. (I initially thought maybe they add all stop words as single tokens to vocab somehow, but then I remembered only PART 1 and 3 versions are stop words, not 2.)
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/16/23 thurs
-------------
X -update roboduck cls to avoid printing the SOLUTION PART 2 part
    X -Debug algo to avoid printing SOLUTION PART 2
        X -print out cur on each iteration to confirm that openai actually sometimes returns SOLUTION PART 2 as a single token even though codex is supposed to tokenize it as 5 tokens
        _ -if ^ is true, update algo to grab items from start of queue until the desired char limit is met and track the index, and pop that many items. Or just add an extra check if first val is SOLUTION PART 2, though that wouldn't handle the case of them returning it as 2 tokens. (I initially thought maybe they add all stop words as single tokens to vocab somehow, but then I remembered only PART 1 and 3 versions are stop words, not 2.)
            > Decided to go with simpler approach (see below).
        X -implement simple approach
            > Wanted to keep this part simple for now. Might end up just having chatgpt/codex 3 api extract code portion once at end when magic is used in insert mode. For now, set colon right after SOLUTION PART N to try to avoid the issue of having a title of unknown content and variable size. Then just relied on assumption that gpt will continue to treat SOLUTION PART 2 as a single streaming step (it's still multiple tokens, but apparently they don't necessarily stream 1 token per step).
            X -tweak prompts
                > Added ANSWER KEY back to stop words and moved LOC (start of local variables) to logit bias.
            X -update magic to work with new solution
                > Had to keep SOLUTION PART 2 in codecompletioncache so we can split on it. Also stripped leading newline in inserted code cell.
-consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
-finish distill post on augmenting human intellect

2/17/23 fri
-----------
X -consider refactoring promptmanager and gpt out to module level vars (avoid repeated instantiation)
    > Considered refactoring further to use ConversationManager but I think by project's end we'll switch over to the chatgpt api anyway which should add enough statefulness for us. Not expecting super longform conversations anyway.
X -cleanup funcs (lots of comments, todos, etc)
X -port everything to a lib
    X -consider if these will live in same lib, same module, etc.
        > Same lib, diff module. Jupyter extension stuff might go elsewhere but debugger and magic should be same lib because the magic just calls the debugger.
-document funcs/classes
-finish distill post on augmenting human intellect

2/18/23 sat
-----------
X -document funcs/classes
    > Documented three modules.
-finish distill post on augmenting human intellect
-start scoping out possible stack trace explainer
    -what is the exact use case - show better error messages all the time? Only if some env var is set? Only if we run a script with some special command? Or enter a conversational console on error? Static vs conversational is one of the first questions to answer.
    -write todos for next steps

2/19/23 sun
-----------
~ -install editable version of lib and test imports
    X -debugger works in jupyter
    X -magic works in jupyter
    ~ -debugger works in ipython
        > Live typing is kind of slow and choppy, but works.
    ! -magic works in ipython
        > We get AttributeError: can't set attribute when calling %duck magic. Caused by the line that sets shell.debugger_cls to RoboDuckDB, seems like ipython shell makes itself read only maybe? Weird that it works in jupyter though. I suspect we may also need to change debugger's method of reading in source code and possibly magic's method of handling insert mode.
-start scoping out possible stack trace explainer
    X -revisit exisitg debug_on_fail work
        > Actually ended up getting this working quite nicely. Ended up writing a nice monkeypatch and excepthook class that should probably end up in htools (currenly in debug_on_error.py). There's some interesting potential here but I'm not sure it's the right place to inject the LLM.
    X -what is the exact use case - show better error messages all the time? Only if some env var is set? Only if we run a script with some special command? Or enter a conversational console on error? Static vs conversational is one of the first questions to answer.
        > Key is we don't really want it to do this on every single error (esp in interactive shell), but want it to be as low friction as possible to use once we want to. I'm thinking maybe just catch errors using excepthook, print normal stack trace, and include a simple y/n input asking the user if they want it explained.
    X -look into prettyerrors source code
        > Actually looks fairly complex, may be easies after all to mess with sys.excepthook. Note that my current version of that doesn't seem to affect ipython, maybe it's already overridden sys.excepthook?
    X -write todos for next steps
-finish distill post on augmenting human intellect

2/20/23 mon
-----------
X -port new funcs from debug_on_error to htools
    X -monkeypatch
    X -excepthook (careful - as written, just importing it registers it. Maybe put in its own module or move to a method of a class that also lets you revert to default excepthook)
    X -get working in ipython
        > Not trivial, but got it working. Ipython does not use sys.excepthook.
-get editable version of lib working
    -magic works in ipython
        -fix line causing attributeerror in magic duck cls
        -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
        -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/21/23 tues
-----------
-get editable version of lib working
    ~ -magic works in ipython
        X -fix line causing attributeerror in magic duck cls
            > Still can't work in default ipython but we handle the error gracefully and provide a command that shows the user how to start an ipython shell that DOES make this magic available. Looks like minimal progress but this was actually a big win.
        -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
        -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/22/23 wed
-----------
X -get editable version of lib working
    X -magic works in ipython
        X -see if we need to change how source code is obtained (actually, how did debugger do this? Must be possible already.)
            > Only tries to load this if we set full_context=True. I added some documentation saying this is unavailable in ipython.
            > Update: realized it actually wasn't too crazy to make this work in ipython. Added utils func to load current ipython session and optionally format it and incorporated it into debugger cls, along with a new file_type "ipython session".
        ! -see if we need to change insert mode (is there a concept of a new cell here? Probably, but idk if interface is same)
            > Interesting, it actually still works (i.e. inserts code into the new cell that appears when we exit the debugger).
-explore excepthook strategy for error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    -see what default excepthook does
    -try to reproduce that
    -add in input asking for y/n
    -add in LM query if above is Y

2/23/23 thurs
-------------
~ -explore excepthook strategy for intelligent error messages (recall: plan is to just print default stack trace, then have y/n input asking if we want LLM to explain)
    ~ -see what default excepthook does
    ~ -try to reproduce that
    X -add in input asking for y/n
    X -color prompt to stand out from stack trace
    ~ -add in LM query if above is Y
        > Options:
            > 1. just enter debug session
                > Basically same as magic, though faster to enter would be nice. Still, maybe should differentiate this more.
            > 2. try to reuse roboduckdb cls but manually create a query (assume user question will basically be "what caused this error and how do I fix it?")
                > Probably ideal if possible, not sure if it is though.
            > 3. Manually call with jabberwocky.
                > Doable but clunky.
            > 4. Refactor jabberwocky call out of roboduckdb cls and call same helper func here.
                > Maybe the most likely solution ultimately.
            > For now, started with calling pdb.post_mortem with custom roboduck class. Works, though I do notice the answer was not good in this case and it looks like it *might* be caused by a missing global_var rather than just poor GPT.
                > Never mind, the global var is there. GPT is just wrong. Again, don't worry about this too much - chatgpt/codex3 would likely do better here.

2/24/23 fri
-----------
-figure out way to avoid making user type query. Recall main options are:
    -make new error prompt and just call gpt manually
    -refactor behavior out of roboduckdb cls
    -figure out how to call roboduckdb.ask_language_model directly instead of making user do it (perhaps look into what that interact() method called inside post_mortem() actually does)
-provide shell shortcut command to launch ipython in mode that enables debug magic
    -implement
    -update error message in debugger.py


Backlog
-------
-finish distill post on augmenting human intellect
-equivalent of jupyter magic but for scripts? (i.e. if we hit an error, enter debugging session)
-investigate: was it intentional that PromptManager supports passing in GPT obj but ConversationManager doesn't? (I think initially neither did but had an immediate use case for PromptManager so changed that one, but haven't had a need yet for ConversationManager so didn't prioritize it. Probably good to do eventually though.)
-[WAIT: near end of proj, see which features were really necessary] create a more minimal install for jabberwocky (no youtube transcription stuff, for example)
-[WAIT: maybe unnecessary w/ chatgpt/codex 3 api] prototype some "prompt by proxy" prompts (e.g. "conversation with a powerful AI embedded in the interpreter" or "Donald Knuth")
-watch bret victor Future of Programming vid
-read Engelbart paper on augmenting human intellect

Easy Backlog
------------

Keep an eye out for
-------------------
-debug slowness when using magic (is it calling query multiple times?) ~ - add option to add new cell w/ gpt-fixed function below (may need to adjust prompt a bit to encourage it to provide this)
    > 2/13/23: Haven't observed this in this round of work on the project, leftover from last time. Maybe just a transient issue w/ the codex API at the time?
-codex completion quality issues
    > 2/13/23: seeing a lot of repetition lately. Seems like it came after I upped the freq penalty so that seems odd, but maybe the more salient change is that at some point I removed docstring quotes as a stopword (bc we may need to generate it sometimes).

Won't do (for now)
------------------

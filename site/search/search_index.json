{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"RoboDuck <p>rubber duck debugging: a method of debugging code by articulating a problem in spoken or written natural language. The name is a reference to a story in the book The Pragmatic Programmer in which a programmer would carry around a rubber duck and debug their code by forcing themselves to explain it, line-by-line, to the duck. [1]</p> <p>robo duck debugging: like rubber duck debugging, but the duck talks back.</p>"},{"location":"#about","title":"About","text":"<p>Many AI-powered dev tools help you write boilerplate more quickly, but the hardest and most time-consuming part of programming is often the last mile. Roboduck's goal is to help you understand and fix those bugs. It essentially embeds an LLM (large language model) in the Python interpreter, providing drop-in natural language replacements for Python's standard approaches to: - debugging - error handling - logging  </p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#todo-add-gifs","title":"TODO add gifs","text":""},{"location":"#install","title":"Install","text":"<pre><code>pip install roboduck\n</code></pre>"},{"location":"#api-key-setup","title":"API Key Setup","text":"<p>You need an openai API key to begin using roboduck. Once you have an account (sign up here), you can visit https://platform.openai.com/account/api-keys to retrieve your key. Your simplest option is then to call <code>roboduck.set_openai_api_key(api_key, update_config=True)</code> which essentially does the following: </p> <pre><code>mkdir ~/.roboduck\necho \"openai_api_key: your_api_key\" &gt; ~/.roboduck/config.yaml\n</code></pre> <p>Manually setting an OPENAI_API_KEY environment variable also works.</p> <p>Roboduck does not store your API key or collect any usage data.</p>"},{"location":"#debugger","title":"Debugger","text":"<p>We provide a natural language equivalent of python's built-in <code>breakpoint</code> function. Once you're in an interactive session, you can use the standard pdb commands to navigate your code (cmd+f \"debugger commands\" here. TLDR: type <code>n</code> to execute the next line, a variable name to view its current value, or <code>q</code> to quit the debugging session). However, you can also type a question like \"Why do we get an index error when j changes from 3 to 4?\" or \"Why does nums have three 9s in it when the input list only had one?\". Concretely, any time you type something including a question mark, an LLM will try to answer. This is not just performing static analysis - the LLM can access information about the current state of your program.</p> <pre><code>from roboduck import duck\n\ndef bubble_sort(nums):\n    for i in range(len(nums)):\n        for j in range(len(nums)):\n            if nums[j] &gt; nums[j + 1]:\n                nums[j + 1] = nums[j]\n                nums[j] = nums[j + 1]\n                duck()   # &lt;--------------------------- instead of breakpoint()\n    return nums\n\nnums = [3, 1, 9, 2, 1]\nbubble_sort(nums)\n</code></pre>"},{"location":"#errors","title":"Errors","text":"<p>Roboduck is also good at explaining error messages.  Importing the errors module automatically enables optional error explanations. <code>errors.disable()</code> reverts to python's regular behavior on errors. <code>errors.enable()</code> can be used to re-enable error explanations or to change settings. For example, setting auto=True automatically explains all errors rather than asking the user if they want an explanation (y/n) when an error occurs (this is probably excessive for most use cases, but you're free to do it).</p> <pre><code>from roboduck import errors\n\ndata = {'x': 0}\ny = data.x\n\nerrors.disable()\ny = data.x\n\nerrors.enable(auto=True)\ny = data.x\n</code></pre>"},{"location":"#jupyter-magic","title":"Jupyter Magic","text":"<p>Jupyter provides a <code>%debug</code> magic that can be used after an error occurs to enter a postmortem debugging session. Roboduck's <code>%duck</code> magic works similarly, but with all of our debugging module's conversational capabilities:</p> <pre><code># cell 1\nfrom roboduck import magic\n\nnums = [1, 2, 3]\nnums.add(4)\n</code></pre> <pre><code># cell 2\n%duck\n</code></pre>"},{"location":"#logging","title":"Logging","text":"<p>Roboduck also provides a logger that can write to stdout and/or a file. Whenever you log an Exception object, an LLM will try to diagnose and suggest a fix for the problem. (Unlike the debug module, the logger does not type responses live because we assume logs will typically be viewed after the fact.)</p> <pre><code>from roboduck import logging\n\nlogger = logging.getLogger(path='/tmp/log.txt')\ndata = {'x': 0}\ntry:\n    x = data.x\nexcept Exception as e:\n    logger.error(e)\n</code></pre>"},{"location":"#cli","title":"CLI","text":"<p>You can also run a python script with error explanations enabled:</p> <pre><code>duck my_script.py\n</code></pre> <p>Run <code>duck --help</code> for more info.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>To create a virtual environment and install relevant packages: <pre><code>make dev_env\n</code></pre></p> <p>To run unit tests: <pre><code>make test\n</code></pre></p> <p>To rebuild the docs locally: <pre><code>make docs\n</code></pre></p>"},{"location":"#repo-structure","title":"Repo Structure","text":"<pre><code>roboduck/\n\u251c\u2500\u2500 lib          # Python package source code\n\u251c\u2500\u2500 bin          # Executable scripts to be run from the project root directory.\n\u251c\u2500\u2500 docs         # Markdown templates that mkdocs uses to build automatic documentation.\n\u251c\u2500\u2500 site         # The autogenerated content that makes up our documentation, deployed with github pages.\n\u251c\u2500\u2500 .github      # Build that executes when pushing or merging to main.\n\u251c\u2500\u2500 tests        # Pytest unit tests.\n\u2514\u2500\u2500 data         # Contains images for our README. Used locally to store other miscellaneous files that are excluded from github.\n</code></pre>"},{"location":"config/","title":"Config","text":"<p>Allow us to easily read from and write to roboduck's config file.</p> <p>Roboduck creates a config file at <code>~/.roboduck/config.yaml</code>. This currently supports only two fields:</p> <ul> <li> <p><code>openai_api_key</code>: See the Quickstart for setup help.</p> </li> <li> <p><code>model_name</code> (optional): Roboduck is configured to use gpt-3.5-turbo by default. This field lets you change that (e.g. to gpt-4). If present in the config file, this will take priority over any model_name field specified in a chat template (e.g. our default debug prompt template). You can view valid options with <code>roboduck.available_models()</code>. You can still override the config default by manually passing a value into a function, e.g. <code>duck(model_name='gpt-4-32k')</code>.</p> </li> </ul> <p>You can manually edit your config file or use a command like <code>roboduck.update_config(model_name='gpt-4')</code>. Passing in a value of None (e.g. <code>roboduck.update_config(model_name=None)</code>) will delete that field from your config file.</p>"},{"location":"config/#lib.roboduck.config-attributes","title":"Attributes","text":""},{"location":"config/#lib.roboduck.config.config_path","title":"<code>config_path = Path('~/.roboduck/config.yaml').expanduser()</code>  <code>module-attribute</code>","text":""},{"location":"config/#lib.roboduck.config-functions","title":"Functions","text":""},{"location":"config/#lib.roboduck.config.update_config","title":"<code>update_config(config_path=config_path, **kwargs)</code>","text":"<p>Update roboduck config file with settings that persist for future sessions.</p> <p>Other fields may be configurable here in the future, but as of v1 this should really only be used to set openai_api_key and/or model_name.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str or Path</code> <p>Location of the roboduck config file.</p> <code>config_path</code> <code>kwargs</code> <code>any</code> <p>Available fields include: - openai_api_key - model_name: name like 'gpt-3.5-turbo' that controls what model to use for completions. Model_name is resolved as follows: 1. kwargs explicitly passed in by user (e.g. <code>duck(model_name='gpt-4')</code> always override everything else. 2. if global config file (which this function updates) has a model_name, it is the next highest priority. 3. specific chat template (e.g. roboduck/prompts/chat/debug.yaml) model name is used if neither #1 or #2 are provided.</p> <p>The reason for the global config taking priority over specific templates is that we want to make it easy for a user to always use a specific model that is not the roboduck default (i.e. without having to pass in a model_name in every single duck() call). This does come with the tradeoff of making it hard to define both a different default model AND a custom prompt template with yet another model, but that seems like a less common use case.</p> <p>Passing in a value of None indicates that the corresponding key should be deleted from the config file, NOT that we will explicitly set {field}: None.</p> <code>{}</code> Source code in <code>lib/roboduck/config.py</code> <pre><code>def update_config(config_path=config_path, **kwargs):\n\"\"\"Update roboduck config file with settings that persist for future\n    sessions.\n\n    Other fields may be configurable here in the future, but as of v1 this\n    should really only be used to set openai_api_key and/or model_name.\n\n    Parameters\n    ----------\n    config_path : str or Path\n        Location of the roboduck config file.\n    kwargs : any\n        Available fields include:\n            - openai_api_key\n            - model_name: name like 'gpt-3.5-turbo' that controls what model\n            to use for completions. Model_name is resolved as follows:\n            1. kwargs explicitly passed in by user (e.g.\n            `duck(model_name='gpt-4')` always override everything else.\n            2. if global config file (which this function updates) has a\n            model_name, it is the next highest priority.\n            3. specific chat template (e.g. roboduck/prompts/chat/debug.yaml)\n            model name is used if neither #1 or #2 are provided.\n\n            The reason for the global config taking priority over specific\n            templates is that we want to make it easy for a user to always use\n            a specific model that is not the roboduck default (i.e. without\n            having to pass in a model_name in every single duck() call). This\n            does come with the tradeoff of making it hard to define both a\n            different default model AND a custom prompt template with yet\n            another model, but that seems like a less common use case.\n\n            Passing in a value of None indicates that the corresponding key\n            should be deleted from the config file, NOT that we will explicitly\n            set {field}: None.\n    \"\"\"\n    recognized_keys = {'openai_api_key', 'model_name'}\n    if set(kwargs) - recognized_keys:\n        warnings.warn(f'You are setting unrecognized key(s): '\n                      f'{set(kwargs) - recognized_keys}.')\n    update_yaml(path=config_path, delete_if_none=True, **kwargs)\n</code></pre>"},{"location":"config/#lib.roboduck.config.load_config","title":"<code>load_config(config_path=config_path)</code>","text":"<p>Load roboduck config.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str or Path</code> <p>Location of the roboduck config file.</p> <code>config_path</code> <p>Returns:</p> Type Description <code>dict</code> Source code in <code>lib/roboduck/config.py</code> <pre><code>def load_config(config_path=config_path):\n\"\"\"Load roboduck config.\n\n    Parameters\n    ----------\n    config_path : str or Path\n        Location of the roboduck config file.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    config_path = Path(config_path)\n    if not config_path.is_file():\n        config_path.parent.mkdir(parents=True, exist_ok=True)\n        config_path.touch()\n    return load_yaml(path=config_path)\n</code></pre>"},{"location":"config/#lib.roboduck.config.apply_config_defaults","title":"<code>apply_config_defaults(chat_kwargs, template_only, config_path=config_path)</code>","text":"<p>Help resolve model_name in place. Recall we prioritize sources in this order:</p> <ol> <li>value a user specified explicitly, e.g. Chat(..., model_name='gpt-4').</li> <li>value specified in roboduck config file</li> <li>value specified in a prompt template (can be native to roboduck or user-defined)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>chat_kwargs</code> <code>dict</code> <p>Kwargs to pass to our langchain.chat.Chat constructor. May include a model_name str field.</p> required <code>template_only</code> <code>bool</code> <p>Specifies whether chat_kwargs are passed in directly from a prompt template (template_only=True) or include kwargs that a user passed in explicitly (template_only=False).</p> required <code>config_path</code> <code>str or Path</code> <p>Location of the roboduck config file.</p> <code>config_path</code> <p>Returns:</p> Type Description <code>None</code> <p>Update happens in place (if at all).</p> Source code in <code>lib/roboduck/config.py</code> <pre><code>def apply_config_defaults(chat_kwargs, template_only, config_path=config_path):\n\"\"\"Help resolve model_name in place. Recall we prioritize sources in this\n    order:\n\n    1. value a user specified explicitly, e.g. Chat(..., model_name='gpt-4').\n    2. value specified in roboduck config file\n    3. value specified in a prompt template (can be native to roboduck or\n    user-defined)\n\n    Parameters\n    ----------\n    chat_kwargs : dict\n        Kwargs to pass to our langchain.chat.Chat constructor. May include a\n        model_name str field.\n    template_only : bool\n        Specifies whether chat_kwargs are passed in directly from a prompt\n        template (template_only=True) or include kwargs that a user passed in\n        explicitly (template_only=False).\n    config_path : str or Path\n        Location of the roboduck config file.\n\n    Returns\n    -------\n    None\n        Update happens in place (if at all).\n    \"\"\"\n    # If both are true, it means the user has already explicitly passed in a\n    # model name so we should NOT override it with our config default.\n    if 'model_name' in chat_kwargs and not template_only:\n        return\n\n    cfg = load_config(config_path=config_path)\n    config_model_name = cfg.get('model_name', '')\n    # We also don't want to add something like model_name='' if no default is\n    # specified in the config. Better to revert to langchain class default than\n    # set it to None, which could break things.\n    if config_model_name:\n        chat_kwargs['model_name'] = config_model_name\n</code></pre>"},{"location":"config/#lib.roboduck.config.set_openai_api_key","title":"<code>set_openai_api_key(key=None, config_path=config_path, strict=False, update_config_=False)</code>","text":"<p>Set OPENAI_API_KEY environment variable for langchain.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str or None</code> <p>Optionally pass in openai api key (str). If not provided, we check the config path and try to load a key. If it is provided, we don't check config_path.</p> <code>None</code> <code>config_path</code> <code>str or Path</code> <p>Local yaml file containing the field openai_api_key. We only try to load the key from it if <code>key</code> is not provided. We do not write to this file by default.</p> <code>config_path</code> <code>strict</code> <code>bool</code> <p>Determines what happens when key is None and config path does not exist. Strict=True raises a runtime error, False just warns user.</p> <code>False</code> <code>update_config_</code> <code>bool</code> <p>If True, we update the yaml config file with that api key.</p> <code>False</code> Source code in <code>lib/roboduck/config.py</code> <pre><code>def set_openai_api_key(key=None, config_path=config_path,\n                       strict=False, update_config_=False):\n\"\"\"Set OPENAI_API_KEY environment variable for langchain.\n\n    Parameters\n    ----------\n    key : str or None\n        Optionally pass in openai api key (str). If not provided, we check the\n        config path and try to load a key. If it is provided, we don't check\n        config_path.\n    config_path : str or Path\n        Local yaml file containing the field openai_api_key. We only try to\n        load the key from it if `key` is not provided. We do not write to\n        this file by default.\n    strict : bool\n        Determines what happens when key is None and config path does not\n        exist. Strict=True raises a runtime error, False just warns user.\n    update_config_ : bool\n        If True, we update the yaml config file with that api key.\n    \"\"\"\n    config_path = Path(config_path).expanduser()\n    var_name = 'OPENAI_API_KEY'\n    key = key or os.environ.get(var_name)\n    if not key:\n        try:\n            data = load_config(config_path)\n            key = data[var_name.lower()]\n        except Exception as e:\n            msg = 'Openai api key must either be passed into this function ' \\\n                  f'or stored in {config_path} with field name ' \\\n                  f'{var_name.lower()}. No key found.'\n            if strict:\n                raise RuntimeError(msg)\n            else:\n                warnings.warn(msg + ' Not raising error because strict=False, '\n                              'but openai API will not be available.')\n                return\n    os.environ[var_name] = key\n    if update_config_:\n        update_config(config_path, **{var_name.lower(): key})\n</code></pre>"},{"location":"custom_prompts/","title":"Custom Prompts","text":"<p>Roboduck provides prompts for interactive debugging and stack trace analysis, but you can also define your own. This involves a few steps:</p> <ol> <li> <p>Construct a prompt template. Roboduck defines prompt templates using yaml files. You can find an example here. The kwargs are passed to langchain and correspond to common model hyperaparameters you can find in the openai api docs. Roboduck typically defines two types of user messages: first is the default message type (called \"contextful\" in the example linked above) which is used whenever program state has changed since the user's last question. The second (\"contextless\" in our example) is used when state has not changed, e.g. when asking a followup question during an interactive debugging session. You can create a similar yaml file locally containing whatever instructions you want to show the model. We expect model output to consist of a natural language explanation followed by an optional code snippet - it's technically possible to override this expectation by writing a custom replacement for roboduck's <code>utils.parse_completion</code> function, but we don't anticipate that being a common workflow.</p> </li> <li> <p>[Optional] Subclass <code>roboduck.DuckDB</code> and implement a custom <code>_get_prompt_kwargs</code> method. If your custom prompt expects different fields than our default prompt, you need to provide the debugger with a way to access them. <code>_get_prompt_kwargs</code> must return a dictionary with values for all of these fields. You can find roboduck's default implementation here. If your prompt contains the same fields and merely changes the instruction wording, you can skip this step.</p> </li> <li> <p>Specify desired <code>prompt_name</code>. Use your custom template as follows:</p> </li> </ol> <pre><code># In a debugger:\nfrom roboduck import duck\n\nduck(prompt_name=your_template_path)\n</code></pre> <pre><code># In a logger:\nfrom roboduck import logging\n\nlogger = logging.getLogger(prompt_name=your_template_path)\n</code></pre> <pre><code># In error explanation mode:\nfrom roboduck import errors\n\nerrors.enable(prompt_name=your_template_path)\n</code></pre>"},{"location":"debug/","title":"Debug","text":"<p>A conversational debugger and drop-in replacement for pdb. Python's default interactive debugging session is already a crude conversation with your program or interpreter, in a sense - this just lets your program communicate to you more effectively.</p>"},{"location":"debug/#lib.roboduck.debug--quickstart","title":"Quickstart","text":"<p>Here's a broken version of bubble sort that places a <code>duck()</code> call on the second to last line where you might normally call <code>breakpoint()</code>.</p> <pre><code>from roboduck import duck\n\ndef bubble_sort(nums):\n    for i in range(len(nums)):\n        for j in range(len(nums)):\n            if nums[j] &gt; nums[j + 1]:\n                nums[j + 1] = nums[j]\n                nums[j] = nums[j + 1]\n                duck()   # &lt;--------------------------- instead of breakpoint()\n    return nums\n\nnums = [3, 1, 9, 2, 1]\nbubble_sort(nums)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug-classes","title":"Classes","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache","title":"<code>CodeCompletionCache</code>","text":"<p>Just stores the last completion from DuckDB in a way that our <code>duck</code> jupyter magic can access (without relying on global variable, though not sure if this is meaningfully different). The magic only needs to access it in insert mode (-i flag) to insert the fixed code snippet into a new code cell.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>@store_class_defaults(attr_filter=lambda x: x.startswith('last_'))\nclass CodeCompletionCache:\n\"\"\"Just stores the last completion from DuckDB in a way that our\n    `duck` jupyter magic can access (without relying on global variable, though\n    not sure if this is meaningfully different). The magic only needs to access\n    it in insert mode (-i flag) to insert the fixed code snippet into a new\n    code cell.\n    \"\"\"\n\n    last_completion = ''\n    last_explanation = ''\n    last_code = ''\n    last_new_code = ''\n    last_code_diff = ''\n    last_extra = {}\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache-attributes","title":"Attributes","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache.last_completion","title":"<code>last_completion = ''</code>  <code>class-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache.last_explanation","title":"<code>last_explanation = ''</code>  <code>class-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache.last_code","title":"<code>last_code = ''</code>  <code>class-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache.last_new_code","title":"<code>last_new_code = ''</code>  <code>class-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache.last_code_diff","title":"<code>last_code_diff = ''</code>  <code>class-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache.last_extra","title":"<code>last_extra = {}</code>  <code>class-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB","title":"<code>DuckDB</code>","text":"<p>         Bases: <code>Pdb</code></p> <p>Conversational debugger powered by LLM (e.g. gpt-3.5-turbo or gpt-4). Once you're in a debugging session, regular pdb commands will work as usual but any user command containing a question mark will be interpreted as a question for the lLM. Prefixing your question with \"[dev]\" will print out the full prompt before making the query (mostly useful when working on the library).</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>class DuckDB(Pdb):\n\"\"\"Conversational debugger powered by LLM (e.g. gpt-3.5-turbo or gpt-4).\n    Once you're in a debugging session, regular pdb commands will work as usual\n    but any user command containing a question mark will be interpreted as a\n    question for the lLM. Prefixing your question with \"[dev]\" will print out\n    the full prompt before making the query (mostly useful when working on the\n    library).\n    \"\"\"\n\n    def __init__(self, prompt_name='debug', max_len_per_var=79, silent=False,\n                 pdb_kwargs=None, parse_func=parse_completion, color='green',\n                 **chat_kwargs):\n\"\"\"\n        Parameters\n        ----------\n        prompt_name : str\n            Name of prompt template to use when querying chatGPT. Roboduck\n            currently provides several builtin options\n            (see roboduck.prompts.chat):\n                debug - for interactive debugging sessions on the relevant\n                    snippet of code.\n                debug_full - for interactive debugging sessions on the whole\n                    notebook (no difference from \"debug\" for scripts). Risks\n                    creating a context that is too long.\n                debug_stack_trace - for automatic error explanations or\n                    logging.\n            Alternatively, can also define your own template in a yaml file\n            mimicking the format of the builtin templates and pass in the\n            path to that file as a string.\n        max_len_per_var : int\n            Limits number of characters per variable when communicating\n            current state (local or global depending on `full_context`) to\n            gpt. If unbounded, that section of the prompt alone could grow\n            very big . I somewhat arbitrarily set 79 as the default, i.e.\n            1 line of python per variable. I figure that's usually enough to\n            communicate the gist of what's happening.\n        silent : bool\n            If True, print gpt completions to stdout. One example of when False\n            is appropriate is our logging module - we want to get the\n            explanation and update the exception message which then gets\n            logged, but we don't care about typing results in real time.\n        pdb_kwargs : dict or None\n            Additional kwargs for base Pdb class.\n        parse_func : function\n            This will be called on the generated text each time gpt provides a\n            completion. It returns a dictionary whose values will be stored\n            in CodeCompletionCache in this module. See the default function's\n            docstring for guidance on writing a custom function.\n        color : str\n            Color to print gpt completions in. Sometimes we want to change this\n            to red, such as in the errors module, to make it clearer that an\n            error occurred.\n        chat_kwargs : any\n            Additional kwargs to configure our Chat class (passed to\n            its `from_config` factory). Common example would be setting\n            `chat_class=roboduck.langchain.chat.DummyChatModel`\n            which mocks api calls (good for development, saves money).\n        \"\"\"\n        super().__init__(**pdb_kwargs or {})\n        # These are prompts in the pdb sense, not the LLM sense. I.e. they\n        # are shown at the start of the line, right before the place where the\n        # user or the LLM will begin typing.\n        self.prompt = '&gt;&gt;&gt; '\n        self.duck_prompt = '[Duck] '\n        self.query_kwargs = {}\n        chat_kwargs['name'] = prompt_name\n        if silent:\n            chat_kwargs['streaming'] = False\n        else:\n            chat_kwargs['streaming'] = True\n            chat_kwargs['callback_manager'] = CallbackManager(\n                [LiveTypingCallbackHandler(color=color)]\n            )\n        # Dev color is what we print the prompt in when user asks a question\n        # in dev mode.\n        self.color = color\n        self.dev_color = 'blue' if self.color == 'red' else 'red'\n        # Must create self.chat before setting _chat_prompt_keys,\n        # and full_context after both of those.\n        self.chat = Chat.from_config(**chat_kwargs)\n        self.default_user_key, self.backup_user_key = self._chat_prompt_keys()\n        self.full_context = 'full_code' in self.field_names()\n        self.prompt_name = prompt_name\n        self.repr_func = partial(truncated_repr, max_len=max_len_per_var)\n        self.silent = silent\n        self.parse_func = parse_func\n        # This gets updated every time the user asks a question.\n        self.prev_kwargs_hash = None\n\n    def _chat_prompt_keys(self):\n\"\"\"Retrieve default and backup user reply prompt keys (names) from\n        self.chat object. If the prompt template has only one reply type,\n        the backup key will equal the default key.\n        \"\"\"\n        keys = list(self.chat.user_templates)\n        default = keys[0]\n        backup = default\n        if len(keys) &gt; 1:\n            backup = keys[1]\n            if len(keys) &gt; 2:\n                warnings.warn(\n                    'You\\'re using a chat prompt template with &gt;2 types or '\n                    'user replies. This is not recommended because it\\'s '\n                    'not clear how to determine which reply type to use. We '\n                    'arbitrarily choose the first non-default key as the '\n                    f'backup reply type (\"{backup}\").'\n                )\n        return default, backup\n\n    def field_names(self, key=''):\n\"\"\"Get names of variables that are expected to be passed into default\n        user prompt template.\n\n        Parameters\n        ----------\n        key : str\n            Determines which user prompt type to use. By default, roboduck\n            provides \"contextful\" (which will include the source code, variable\n            values, and the stack trace when appropriate) and \"contextless\"\n            (which includes only the user question). We default to\n            \"contextful\" here.\n\n        Returns\n        -------\n        set[str]\n        \"\"\"\n        return self.chat.input_variables(key)\n\n    def _get_next_line(self, code_snippet):\n\"\"\"Retrieve next line of code that will be executed. Must call this\n        before we remove the duck() call. We use this in `_get_prompt_kwargs`\n        during interactive debugging sessions.\n\n        Parameters\n        ----------\n        code_snippet : str\n        \"\"\"\n        lines = code_snippet.splitlines()\n        max_idx = len(lines) - 1\n\n        # Adjust f_lineno because it's 1 - indexed by default.\n        # Set default next_line in case we don't find any valid line.\n        line_no = self.curframe.f_lineno - 1\n        next_line = ''\n        while line_no &lt;= max_idx:\n            if lines[line_no].strip().startswith('duck('):\n                line_no += 1\n            else:\n                next_line = lines[line_no]\n                break\n        return next_line\n\n    def _get_prompt_kwargs(self):\n\"\"\"Construct a dictionary describing the current state of our code\n        (variable names and values, source code, file type). This will be\n        passed to our langchain chat.reply() method to fill in the debug prompt\n        template.\n\n        Returns\n        -------\n        dict\n            contains keys 'code', 'local_vars', 'global_vars', 'file_type'.\n            If we specified full_context=True on init, we also include the key\n            'full_code'.\n        \"\"\"\n        res = {}\n\n        # Get current code snippet.\n        # Fails when running code from cmd line like:\n        # 'python -c \"print(x)\"'.\n        # Haven't been able to find a way around this yet.\n        try:\n            # Find next line before removing duck call to avoid messing up our\n            # index.\n            code_snippet = inspect.getsource(self.curframe)\n            res['next_line'] = self._get_next_line(code_snippet)\n            res['code'] = self._remove_debugger_call(code_snippet)\n        except OSError as err:\n            self.error(err)\n\n        # Get full source code if necessary.\n        if self.full_context:\n            # File is a string, either a file name or something like\n            # &lt;ipython-input-50-e97ed612f523&gt;.\n            file = inspect.getsourcefile(self.curframe.f_code)\n            if file.startswith('&lt;ipython'):\n                # If we're in ipython, ipynbname.path() throws a\n                # FileNotFoundError.\n                try:\n                    full_code = load_ipynb(ipynbname.path())\n                    res['file_type'] = 'jupyter notebook'\n                except FileNotFoundError:\n                    full_code = load_current_ipython_session()\n                    res['file_type'] = 'ipython session'\n            else:\n                with open(file, 'r') as f:\n                    full_code = f.read()\n                res['file_type'] = 'python script'\n            res['full_code'] = self._remove_debugger_call(full_code)\n            used_tokens = set(res['full_code'].split())\n        else:\n            # This is intentionally different from the used_tokens line in the\n            # if clause - we only want to consider local code here.\n            used_tokens = set(res['code'].split())\n\n        # Namespace is often polluted with lots of unused globals (htools is\n        # very much guilty of this \ud83d\ude2c) and we don't want to clutter up the\n        # prompt with these.\n        res['local_vars'] = type_annotated_dict_str(\n            {k: v for k, v in self.curframe_locals.items()\n             if k in used_tokens and not is_ipy_name(k)},\n            self.repr_func\n        )\n        res['global_vars'] = type_annotated_dict_str(\n            {k: v for k, v in self.curframe.f_globals.items()\n             if k in used_tokens and not is_ipy_name(k)},\n            self.repr_func\n        )\n        return res\n\n    @staticmethod\n    def _remove_debugger_call(code_str):\n\"\"\"Remove `duck` function call (our equivalent of `breakpoint` from\n        source code string. Including it introduces a slight risk that gpt\n        will fixate on this mistery function as a potential bug cause.\n\n        Parameters\n        ----------\n        code_str : str\n            Source code snippet. We want to remove the `duck()` call (which\n            sometimes includes kwargs) to prevent this from distracting the\n            LLM.\n\n        Returns\n        -------\n        str\n        \"\"\"\n        return '\\n'.join(line for line in code_str.splitlines()\n                         if not line.strip().startswith('duck('))\n\n    def onecmd(self, line):\n\"\"\"Base class describes this as follows:\n\n        Interpret the argument as though it had been typed in response to the\n        prompt. Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n\n        We add an extra check in the if block to check if the user asked a\n        question. If so, we ask gpt. If not, we treat it as a regular pdb\n        command.\n\n        Parameters\n        ----------\n        line : str or tuple\n            If str, this is a regular line like in the standard debugger.\n            If tuple, this contains (line str, stack trace str - see\n            roboduck.errors.post_mortem for the actual insertion into the\n            cmdqueue). This is for use with the debug_stack_trace mode.\n        \"\"\"\n        if isinstance(line, tuple):\n            line, stack_trace = line\n        else:\n            stack_trace = ''\n        if not self.commands_defining:\n            if '?' in line:\n                return self.ask_language_model(\n                    line,\n                    stack_trace=stack_trace,\n                    verbose=line.startswith('[dev]')\n                )\n            return cmd.Cmd.onecmd(self, line)\n        else:\n            return self.handle_command_def(line)\n\n    def ask_language_model(self, question, stack_trace='', verbose=False):\n\"\"\"When the user asks a question during a debugging session, query\n        gpt for the answer and type it back to them live.\n\n        Parameters\n        ----------\n        question : str\n            User question, e.g. \"Why are the first three values in nums equal\n            to 5 when the input list only had a single 5?\". (Example is from\n            a faulty bubble sort implementation.)\n        stack_trace : str\n            When using the \"debug_stack_trace\" prompt, we need to pass a\n            stack trace string into the prompt.\n        verbose : bool\n            If True, print the full gpt prompt in red before making the api\n            call. User activates this mode by prefixing their question with\n            '[dev]'. This overrides self.silent.\n        \"\"\"\n        # Don't provide long context-laden prompt if nothing has changed since\n        # the user's last question. This is often a followup/clarifying\n        # question.\n        prompt_kwargs = self._get_prompt_kwargs()\n        kwargs_hash = hash(str(prompt_kwargs))\n        if kwargs_hash == self.prev_kwargs_hash:\n            prompt_kwargs.clear()\n            prompt_key = self.backup_user_key\n        else:\n            prompt_key = self.default_user_key\n\n        # Perform surgery on kwargs depending on what fields are expected.\n        field_names = self.field_names(prompt_key)\n        if 'question' in field_names:\n            prompt_kwargs['question'] = question\n        if stack_trace:\n            prompt_kwargs['stack_trace'] = stack_trace\n\n        # Validate that expected fields are present and provide interpretable\n        # error message if not.\n        kwargs_names = set(prompt_kwargs)\n        only_in_kwargs = kwargs_names - field_names\n        only_in_expected = field_names - kwargs_names\n        error_msg = 'If you are using a custom prompt, you may need to ' \\\n                    'subclass roboduck.debug.DuckDB and override the ' \\\n                    '_get_prompt_kwargs method.'\n        if only_in_kwargs:\n            raise RuntimeError(\n                f'Received unexpected kwarg(s): {only_in_kwargs}. {error_msg} '\n            )\n        if only_in_expected:\n            raise RuntimeError(\n                f'Missing required kwarg(s): {only_in_expected}. {error_msg}'\n            )\n\n        prompt = self.chat.user_message(key_=prompt_key,\n                                        **prompt_kwargs).content\n        if verbose:\n            print(colored(prompt, 'red'))\n\n        if not self.silent:\n            print(colored(self.duck_prompt, self.color), end='')\n\n        # The actual LLM call.\n        res = self.chat.reply(**prompt_kwargs, key_=prompt_key)\n\n        answer = res.content.strip()\n        if not answer:\n            answer = 'Sorry, I don\\'t know. Can you try ' \\\n                     'rephrasing your question?'\n            # This is intentionally nested in if statement because if answer is\n            # truthy, we will have already printed it via our callback if not\n            # in silent mode.\n            if not self.silent:\n                print(colored(answer, self.color))\n\n        parsed_kwargs = self.parse_func(answer)\n        # When using the `duck` jupyter magic in \"insert\" mode, we reference\n        # the CodeCompletionCache to populate the new code cell.\n        CodeCompletionCache.last_completion = answer\n        CodeCompletionCache.last_explanation = parsed_kwargs['explanation']\n        # Built-in prompts always ask for a fixed version of the relevant\n        # snippet, not the whole code, so that's what we store here and use for\n        # the diff operation.\n        # Contextless prompt has no `code` key.\n        old_code = prompt_kwargs.get('code', '')\n        new_code = parsed_kwargs['code']\n        CodeCompletionCache.last_code_diff = colordiff_new_str(old_code,\n                                                               new_code)\n        CodeCompletionCache.last_code = old_code\n        CodeCompletionCache.last_new_code = new_code\n        CodeCompletionCache.last_extra = parsed_kwargs.get('extra', {})\n        self.prev_kwargs_hash = kwargs_hash\n\n    def precmd(self, line):\n\"\"\"We need to define this to make our errors module work. Our\n        post_mortem function sometimes places a tuple in our debugger's\n        cmdqueue and precmd is called as part of the default cmdloop method.\n        Technically it calls postcmd too but we don't need to override that\n        because it does nothing with its line argument.\n\n        Parameters\n        ----------\n        line : str or tuple\n            If a tuple, it means roboduck.errors.excepthook is being called\n            and an error has occurred. The stack trace is passed in as the\n            second of two items, where the first item is the same object that\n            is normally passed in.\n        \"\"\"\n        if isinstance(line, tuple):\n            line, trace = line\n            return super().precmd(line), trace\n        return super().precmd(line)\n\n    def print_stack_entry(self, frame_lineno, prompt_prefix='\\n-&gt; '):\n\"\"\"This is called automatically when entering a debugger session\n        and it prints a message to stdout like\n\n        ```\n        &gt; &lt;ipython-input-20-9c67d40d0f93&gt;(2)&lt;module&gt;()\n        -&gt; print + 6\n        ```\n\n        In silent mode (like when using the roboduck logger with stdout=False),\n        we want to disable that message. When silent=False, this behaves\n        identically to the standard pdb equivalent.\n        \"\"\"\n        if self.silent:\n            return\n        frame, lineno = frame_lineno\n        if frame is self.curframe:\n            prefix = '&gt; '\n        else:\n            prefix = '  '\n        self.message(prefix +\n                     self.format_stack_entry(frame_lineno, prompt_prefix))\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB-attributes","title":"Attributes","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.prompt","title":"<code>prompt = '&gt;&gt;&gt; '</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.duck_prompt","title":"<code>duck_prompt = '[Duck] '</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.query_kwargs","title":"<code>query_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.color","title":"<code>color = color</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.dev_color","title":"<code>dev_color = 'blue' if self.color == 'red' else 'red'</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.chat","title":"<code>chat = Chat.from_config(None=chat_kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.full_context","title":"<code>full_context = 'full_code' in self.field_names()</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.prompt_name","title":"<code>prompt_name = prompt_name</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.repr_func","title":"<code>repr_func = partial(truncated_repr, max_len=max_len_per_var)</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.silent","title":"<code>silent = silent</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.parse_func","title":"<code>parse_func = parse_func</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.prev_kwargs_hash","title":"<code>prev_kwargs_hash = None</code>  <code>instance-attribute</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB-functions","title":"Functions","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.field_names","title":"<code>field_names(key='')</code>","text":"<p>Get names of variables that are expected to be passed into default user prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Determines which user prompt type to use. By default, roboduck provides \"contextful\" (which will include the source code, variable values, and the stack trace when appropriate) and \"contextless\" (which includes only the user question). We default to \"contextful\" here.</p> <code>''</code> <p>Returns:</p> Type Description <code>set[str]</code> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def field_names(self, key=''):\n\"\"\"Get names of variables that are expected to be passed into default\n    user prompt template.\n\n    Parameters\n    ----------\n    key : str\n        Determines which user prompt type to use. By default, roboduck\n        provides \"contextful\" (which will include the source code, variable\n        values, and the stack trace when appropriate) and \"contextless\"\n        (which includes only the user question). We default to\n        \"contextful\" here.\n\n    Returns\n    -------\n    set[str]\n    \"\"\"\n    return self.chat.input_variables(key)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.onecmd","title":"<code>onecmd(line)</code>","text":"<p>Base class describes this as follows:</p> <p>Interpret the argument as though it had been typed in response to the prompt. Checks whether this line is typed at the normal prompt or in a breakpoint command list definition.</p> <p>We add an extra check in the if block to check if the user asked a question. If so, we ask gpt. If not, we treat it as a regular pdb command.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str or tuple</code> <p>If str, this is a regular line like in the standard debugger. If tuple, this contains (line str, stack trace str - see roboduck.errors.post_mortem for the actual insertion into the cmdqueue). This is for use with the debug_stack_trace mode.</p> required Source code in <code>lib/roboduck/debug.py</code> <pre><code>def onecmd(self, line):\n\"\"\"Base class describes this as follows:\n\n    Interpret the argument as though it had been typed in response to the\n    prompt. Checks whether this line is typed at the normal prompt or in\n    a breakpoint command list definition.\n\n    We add an extra check in the if block to check if the user asked a\n    question. If so, we ask gpt. If not, we treat it as a regular pdb\n    command.\n\n    Parameters\n    ----------\n    line : str or tuple\n        If str, this is a regular line like in the standard debugger.\n        If tuple, this contains (line str, stack trace str - see\n        roboduck.errors.post_mortem for the actual insertion into the\n        cmdqueue). This is for use with the debug_stack_trace mode.\n    \"\"\"\n    if isinstance(line, tuple):\n        line, stack_trace = line\n    else:\n        stack_trace = ''\n    if not self.commands_defining:\n        if '?' in line:\n            return self.ask_language_model(\n                line,\n                stack_trace=stack_trace,\n                verbose=line.startswith('[dev]')\n            )\n        return cmd.Cmd.onecmd(self, line)\n    else:\n        return self.handle_command_def(line)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.ask_language_model","title":"<code>ask_language_model(question, stack_trace='', verbose=False)</code>","text":"<p>When the user asks a question during a debugging session, query gpt for the answer and type it back to them live.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>User question, e.g. \"Why are the first three values in nums equal to 5 when the input list only had a single 5?\". (Example is from a faulty bubble sort implementation.)</p> required <code>stack_trace</code> <code>str</code> <p>When using the \"debug_stack_trace\" prompt, we need to pass a stack trace string into the prompt.</p> <code>''</code> <code>verbose</code> <code>bool</code> <p>If True, print the full gpt prompt in red before making the api call. User activates this mode by prefixing their question with '[dev]'. This overrides self.silent.</p> <code>False</code> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def ask_language_model(self, question, stack_trace='', verbose=False):\n\"\"\"When the user asks a question during a debugging session, query\n    gpt for the answer and type it back to them live.\n\n    Parameters\n    ----------\n    question : str\n        User question, e.g. \"Why are the first three values in nums equal\n        to 5 when the input list only had a single 5?\". (Example is from\n        a faulty bubble sort implementation.)\n    stack_trace : str\n        When using the \"debug_stack_trace\" prompt, we need to pass a\n        stack trace string into the prompt.\n    verbose : bool\n        If True, print the full gpt prompt in red before making the api\n        call. User activates this mode by prefixing their question with\n        '[dev]'. This overrides self.silent.\n    \"\"\"\n    # Don't provide long context-laden prompt if nothing has changed since\n    # the user's last question. This is often a followup/clarifying\n    # question.\n    prompt_kwargs = self._get_prompt_kwargs()\n    kwargs_hash = hash(str(prompt_kwargs))\n    if kwargs_hash == self.prev_kwargs_hash:\n        prompt_kwargs.clear()\n        prompt_key = self.backup_user_key\n    else:\n        prompt_key = self.default_user_key\n\n    # Perform surgery on kwargs depending on what fields are expected.\n    field_names = self.field_names(prompt_key)\n    if 'question' in field_names:\n        prompt_kwargs['question'] = question\n    if stack_trace:\n        prompt_kwargs['stack_trace'] = stack_trace\n\n    # Validate that expected fields are present and provide interpretable\n    # error message if not.\n    kwargs_names = set(prompt_kwargs)\n    only_in_kwargs = kwargs_names - field_names\n    only_in_expected = field_names - kwargs_names\n    error_msg = 'If you are using a custom prompt, you may need to ' \\\n                'subclass roboduck.debug.DuckDB and override the ' \\\n                '_get_prompt_kwargs method.'\n    if only_in_kwargs:\n        raise RuntimeError(\n            f'Received unexpected kwarg(s): {only_in_kwargs}. {error_msg} '\n        )\n    if only_in_expected:\n        raise RuntimeError(\n            f'Missing required kwarg(s): {only_in_expected}. {error_msg}'\n        )\n\n    prompt = self.chat.user_message(key_=prompt_key,\n                                    **prompt_kwargs).content\n    if verbose:\n        print(colored(prompt, 'red'))\n\n    if not self.silent:\n        print(colored(self.duck_prompt, self.color), end='')\n\n    # The actual LLM call.\n    res = self.chat.reply(**prompt_kwargs, key_=prompt_key)\n\n    answer = res.content.strip()\n    if not answer:\n        answer = 'Sorry, I don\\'t know. Can you try ' \\\n                 'rephrasing your question?'\n        # This is intentionally nested in if statement because if answer is\n        # truthy, we will have already printed it via our callback if not\n        # in silent mode.\n        if not self.silent:\n            print(colored(answer, self.color))\n\n    parsed_kwargs = self.parse_func(answer)\n    # When using the `duck` jupyter magic in \"insert\" mode, we reference\n    # the CodeCompletionCache to populate the new code cell.\n    CodeCompletionCache.last_completion = answer\n    CodeCompletionCache.last_explanation = parsed_kwargs['explanation']\n    # Built-in prompts always ask for a fixed version of the relevant\n    # snippet, not the whole code, so that's what we store here and use for\n    # the diff operation.\n    # Contextless prompt has no `code` key.\n    old_code = prompt_kwargs.get('code', '')\n    new_code = parsed_kwargs['code']\n    CodeCompletionCache.last_code_diff = colordiff_new_str(old_code,\n                                                           new_code)\n    CodeCompletionCache.last_code = old_code\n    CodeCompletionCache.last_new_code = new_code\n    CodeCompletionCache.last_extra = parsed_kwargs.get('extra', {})\n    self.prev_kwargs_hash = kwargs_hash\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.precmd","title":"<code>precmd(line)</code>","text":"<p>We need to define this to make our errors module work. Our post_mortem function sometimes places a tuple in our debugger's cmdqueue and precmd is called as part of the default cmdloop method. Technically it calls postcmd too but we don't need to override that because it does nothing with its line argument.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str or tuple</code> <p>If a tuple, it means roboduck.errors.excepthook is being called and an error has occurred. The stack trace is passed in as the second of two items, where the first item is the same object that is normally passed in.</p> required Source code in <code>lib/roboduck/debug.py</code> <pre><code>def precmd(self, line):\n\"\"\"We need to define this to make our errors module work. Our\n    post_mortem function sometimes places a tuple in our debugger's\n    cmdqueue and precmd is called as part of the default cmdloop method.\n    Technically it calls postcmd too but we don't need to override that\n    because it does nothing with its line argument.\n\n    Parameters\n    ----------\n    line : str or tuple\n        If a tuple, it means roboduck.errors.excepthook is being called\n        and an error has occurred. The stack trace is passed in as the\n        second of two items, where the first item is the same object that\n        is normally passed in.\n    \"\"\"\n    if isinstance(line, tuple):\n        line, trace = line\n        return super().precmd(line), trace\n    return super().precmd(line)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.print_stack_entry","title":"<code>print_stack_entry(frame_lineno, prompt_prefix='\\n-&gt; ')</code>","text":"<p>This is called automatically when entering a debugger session and it prints a message to stdout like</p> <pre><code>&gt; &lt;ipython-input-20-9c67d40d0f93&gt;(2)&lt;module&gt;()\n-&gt; print + 6\n</code></pre> <p>In silent mode (like when using the roboduck logger with stdout=False), we want to disable that message. When silent=False, this behaves identically to the standard pdb equivalent.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def print_stack_entry(self, frame_lineno, prompt_prefix='\\n-&gt; '):\n\"\"\"This is called automatically when entering a debugger session\n    and it prints a message to stdout like\n\n    ```\n    &gt; &lt;ipython-input-20-9c67d40d0f93&gt;(2)&lt;module&gt;()\n    -&gt; print + 6\n    ```\n\n    In silent mode (like when using the roboduck logger with stdout=False),\n    we want to disable that message. When silent=False, this behaves\n    identically to the standard pdb equivalent.\n    \"\"\"\n    if self.silent:\n        return\n    frame, lineno = frame_lineno\n    if frame is self.curframe:\n        prefix = '&gt; '\n    else:\n        prefix = '  '\n    self.message(prefix +\n                 self.format_stack_entry(frame_lineno, prompt_prefix))\n</code></pre>"},{"location":"debug/#lib.roboduck.debug-functions","title":"Functions","text":""},{"location":"debug/#lib.roboduck.debug.duck","title":"<code>duck(**kwargs)</code>","text":"<p>Roboduck equivalent of native python breakpoint(). The DuckDB docstring is below. Any kwargs passed in to this function will be passed to its constructor.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>@add_docstring(DuckDB.__init__)\ndef duck(**kwargs):\n\"\"\"Roboduck equivalent of native python breakpoint().\n    The DuckDB docstring is below. Any kwargs passed in to this function\n    will be passed to its constructor.\n    \"\"\"\n    DuckDB(**kwargs).set_trace(sys._getframe().f_back)\n</code></pre>"},{"location":"decorators/","title":"Decorators","text":"<p>Miscellaneous decorators used throughout the library.</p>"},{"location":"decorators/#lib.roboduck.decorators-functions","title":"Functions","text":""},{"location":"decorators/#lib.roboduck.decorators.typecheck","title":"<code>typecheck(func_=None, **types)</code>","text":"<p>Decorator to enforce type checking for a function or method. There are two ways to call this: either explicitly passing argument types to the decorator, or letting it infer them using type annotations in the function that will be decorated. We allow both usage methods since older versions of Python lack type annotations, and also because I feel the annotation syntax can hurt readability.</p> <p>Ported from htools to avoid extra dependency.</p> <p>Parameters:</p> Name Type Description Default <code>func_</code> <code>function</code> <p>The function to decorate. When using decorator with manually-specified types, this is None. Underscore is used so that <code>func</code> can still be used as a valid keyword argument for the wrapped function.</p> <code>None</code> <code>types</code> <code>type</code> <p>Optional way to specify variable types. Use standard types rather than importing from the typing library, as subscripted generics are not supported (e.g. typing.List[str] will not work; typing.List will but at that point there is no benefit over the standard <code>list</code>).</p> <code>{}</code> <p>Examples:</p> <p>In the first example, we specify types directly in the decorator. Notice that they can be single types or tuples of types. You can choose to specify types for all arguments or just a subset.</p> <pre><code>@typecheck(x=float, y=(int, float), iters=int, verbose=bool)\ndef process(x, y, z, iters=5, verbose=True):\n    print(f'z = {z}')\n    for i in range(iters):\n        if verbose: print(f'Iteration {i}...')\n        x *= y\n    return x\n</code></pre> <pre><code>&gt;&gt;&gt; process(3.1, 4.5, 0, 2.0)\nTypeError: iters must be &lt;class 'int'&gt;, not &lt;class 'float'&gt;.\n</code></pre> <pre><code>&gt;&gt;&gt; process(3.1, 4, 'a', 1, False)\nz = a\n12.4\n</code></pre> <p>Alternatively, you can let the decorator infer types using annotations in the function that is to be decorated. The example below behaves equivalently to the explicit example shown above. Note that annotations regarding the returned value are ignored.</p> <pre><code>@typecheck\ndef process(x:float, y:(int, float), z, iters:int=5, verbose:bool=True):\n    print(f'z = {z}')\n    for i in range(iters):\n        if verbose: print(f'Iteration {i}...')\n        x *= y\n    return x\n</code></pre> <pre><code>&gt;&gt;&gt; process(3.1, 4.5, 0, 2.0)\nTypeError: iters must be &lt;class 'int'&gt;, not &lt;class 'float'&gt;.\n</code></pre> <pre><code>&gt;&gt;&gt; process(3.1, 4, 'a', 1, False)\nz = a\n12.4\n</code></pre> Source code in <code>lib/roboduck/decorators.py</code> <pre><code>def typecheck(func_=None, **types):\n\"\"\"Decorator to enforce type checking for a function or method. There are\n    two ways to call this: either explicitly passing argument types to the\n    decorator, or letting it infer them using type annotations in the function\n    that will be decorated. We allow both usage methods since older\n    versions of Python lack type annotations, and also because I feel the\n    annotation syntax can hurt readability.\n\n    Ported from [htools](https://github.com/hdmamin/htools) to avoid extra\n    dependency.\n\n    Parameters\n    ----------\n    func_ : function\n        The function to decorate. When using decorator with\n        manually-specified types, this is None. Underscore is used so that\n        `func` can still be used as a valid keyword argument for the wrapped\n        function.\n    types : type\n        Optional way to specify variable types. Use standard types rather than\n        importing from the typing library, as subscripted generics are not\n        supported (e.g. typing.List[str] will not work; typing.List will but at\n        that point there is no benefit over the standard `list`).\n\n    Examples\n    --------\n    In the first example, we specify types directly in the decorator. Notice\n    that they can be single types or tuples of types. You can choose to\n    specify types for all arguments or just a subset.\n\n    ```\n    @typecheck(x=float, y=(int, float), iters=int, verbose=bool)\n    def process(x, y, z, iters=5, verbose=True):\n        print(f'z = {z}')\n        for i in range(iters):\n            if verbose: print(f'Iteration {i}...')\n            x *= y\n        return x\n    ```\n\n    &gt;&gt;&gt; process(3.1, 4.5, 0, 2.0)\n    TypeError: iters must be &lt;class 'int'&gt;, not &lt;class 'float'&gt;.\n\n    &gt;&gt;&gt; process(3.1, 4, 'a', 1, False)\n    z = a\n    12.4\n\n    Alternatively, you can let the decorator infer types using annotations\n    in the function that is to be decorated. The example below behaves\n    equivalently to the explicit example shown above. Note that annotations\n    regarding the returned value are ignored.\n\n    ```\n    @typecheck\n    def process(x:float, y:(int, float), z, iters:int=5, verbose:bool=True):\n        print(f'z = {z}')\n        for i in range(iters):\n            if verbose: print(f'Iteration {i}...')\n            x *= y\n        return x\n    ```\n\n    &gt;&gt;&gt; process(3.1, 4.5, 0, 2.0)\n    TypeError: iters must be &lt;class 'int'&gt;, not &lt;class 'float'&gt;.\n\n    &gt;&gt;&gt; process(3.1, 4, 'a', 1, False)\n    z = a\n    12.4\n    \"\"\"\n    # Case 1: Pass keyword args to decorator specifying types.\n    if not func_:\n        return partial(typecheck, **types)\n    # Case 2: Infer types from annotations. Skip if Case 1 already occurred.\n    elif not types:\n        types = {k: v.annotation\n                 for k, v in signature(func_).parameters.items()\n                 if not v.annotation == Parameter.empty}\n\n    @wraps(func_)\n    def wrapper(*args, **kwargs):\n        fargs = signature(wrapper).bind(*args, **kwargs).arguments\n        for k, v in types.items():\n            if k in fargs and not isinstance(fargs[k], v):\n                raise TypeError(\n                    f'{k} must be {str(v)}, not {type(fargs[k])}.'\n                )\n        return func_(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"decorators/#lib.roboduck.decorators.add_kwargs","title":"<code>add_kwargs(func, fields, hide_fields=(), strict=False)</code>","text":"<p>Decorator that adds parameters into the signature and docstring of a function that accepts **kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>function</code> <p>Function to decorate.</p> required <code>fields</code> <code>list[str]</code> <p>Names of params to insert into signature + docstring.</p> required <code>hide_fields</code> <code>list[str]</code> <p>Names of params that are already in the function's signature that we want to hide. To use a non-empty value here, we must set strict=True and the param must have a default value, as this is what will be used in all subsequent calls.</p> <code>()</code> <code>strict</code> <code>bool</code> <p>If true, we do two things: 1. On decorated function call, check that the user provided all expected arguments. 2. Enable the use of the <code>hide_fields</code> param.</p> <code>False</code> <p>Returns:</p> Type Description <code>function</code> Source code in <code>lib/roboduck/decorators.py</code> <pre><code>def add_kwargs(func, fields, hide_fields=(), strict=False):\n\"\"\"Decorator that adds parameters into the signature and docstring of a\n    function that accepts **kwargs.\n\n    Parameters\n    ----------\n    func : function\n        Function to decorate.\n    fields : list[str]\n        Names of params to insert into signature + docstring.\n    hide_fields : list[str]\n        Names of params that are *already* in the function's signature that\n        we want to hide. To use a non-empty value here, we must set strict=True\n        and the param must have a default value, as this is what will be used\n        in all subsequent calls.\n    strict : bool\n        If true, we do two things:\n        1. On decorated function call, check that the user provided all\n        expected arguments.\n        2. Enable the use of the `hide_fields` param.\n\n    Returns\n    -------\n    function\n    \"\"\"\n    # Hide_fields must have default values in existing function. They will not\n    # show up in the new docstring and the user will not be able to pass in a\n    # value when calling the new function - it will always use the default.\n    # To set different defaults, you can pass in a partial rather than a\n    # function as the first arg here.\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n\n    if hide_fields and not strict:\n        raise ValueError(\n            'You must set strict=True when providing one or more '\n            'hide_fields. Otherwise the user can still pass in those args.'\n        )\n    sig = signature(wrapper)\n    params_ = {k: v for k, v in sig.parameters.items()}\n\n    # Remove any fields we want to hide.\n    for field in hide_fields:\n        if field not in params_:\n            warnings.warn(f'No need to hide field {field} because it\\'s not '\n                          'in the existing function signature.')\n        elif params_.pop(field).default == Parameter.empty:\n            raise TypeError(\n                f'Field \"{field}\" is not a valid hide_field because it has '\n                'no default value in the original function.'\n            )\n\n    if getattr(params_.pop('kwargs', None), 'kind') != Parameter.VAR_KEYWORD:\n        raise TypeError(f'Function {func} must accept **kwargs.')\n    new_params = {\n        field: Parameter(field, Parameter.KEYWORD_ONLY)\n        for field in fields\n    }\n    overlap = set(new_params) &amp; set(params_)\n    if overlap:\n        raise RuntimeError(\n            f'Some of the kwargs you tried to inject into {func} already '\n            'exist in its signature. This is not allowed because it\\'s '\n            'unclear how to resolve default values and parameter type.'\n        )\n\n    params_.update(new_params)\n    wrapper.__signature__ = sig.replace(parameters=params_.values())\n    if strict:\n        # In practice langchain checks for this anyway if we ask for a\n        # completion, but outside of that context we need typecheck\n        # because otherwise we could provide no kwargs and _func wouldn't\n        # complain. Just use generic type because we only care that a value is\n        # provided.\n        wrapper = typecheck(wrapper, **{f: object for f in fields})\n    return wrapper\n</code></pre>"},{"location":"decorators/#lib.roboduck.decorators.store_class_defaults","title":"<code>store_class_defaults(cls=None, attr_filter=None)</code>","text":"<p>Class decorator that stores default values of class attributes (can be all or a subset). Default here refers to the value at class definition time.</p> <p>Examples:</p> <pre><code>@store_class_defaults(attr_filter=lambda x: x.startswith('last_'))\nclass Foo:\n    last_bar = 3\n    last_baz = 'abc'\n    other = True\n</code></pre> <pre><code>&gt;&gt;&gt; Foo._class_defaults\n</code></pre> <p>{'last_bar': 3, 'last_baz': 'abc'}</p> <p>Or use the decorator without parentheses to store all values at definition time. This is usually unnecessary. If you do provide an attr_filter, it must be a named argument.</p> <p>Foo.reset_class_vars() will reset all relevant class vars to their default values.</p> Source code in <code>lib/roboduck/decorators.py</code> <pre><code>def store_class_defaults(cls=None, attr_filter=None):\n\"\"\"Class decorator that stores default values of class attributes (can be\n    all or a subset). Default here refers to the value at class definition\n    time.\n\n    Examples\n    --------\n    ```\n    @store_class_defaults(attr_filter=lambda x: x.startswith('last_'))\n    class Foo:\n        last_bar = 3\n        last_baz = 'abc'\n        other = True\n    ```\n\n    &gt;&gt;&gt; Foo._class_defaults\n\n    {'last_bar': 3, 'last_baz': 'abc'}\n\n    Or use the decorator without parentheses to store all values at definition\n    time. This is usually unnecessary. If you do provide an attr_filter, it\n    must be a named argument.\n\n    Foo.reset_class_vars() will reset all relevant class vars to their\n    default values.\n    \"\"\"\n    if cls is None:\n        return partial(store_class_defaults, attr_filter=attr_filter)\n    if not isinstance(cls, type):\n        raise TypeError(\n            f'cls arg in store_class_defaults decorator has type {type(cls)} '\n            f'but expected type `type`, i.e. a class. You may be passing in '\n            f'an attr_filter as a positional arg which is not allowed - it '\n            f'must be a named arg if provided.'\n        )\n    if not attr_filter:\n        def attr_filter(x):\n            return True\n    defaults = {}\n    for k, v in vars(cls).items():\n        if attr_filter(k):\n            defaults[k] = v\n\n    name = '_class_defaults'\n    if hasattr(cls, name):\n        raise AttributeError(\n            f'Class {cls} already has attribute {name}. store_class_defaults '\n            'decorator would overwrite that. Exiting.'\n        )\n    setattr(cls, name, defaults)\n\n    @classmethod\n    def reset_class_vars(cls):\n\"\"\"Reset all default class attributes to their defaults.\"\"\"\n        for k, v in cls._class_defaults.items():\n            try:\n                setattr(cls, k, v)\n            except Exception as e:\n                warnings.warn(f'Could not reset class attribute {k} to its '\n                              f'default value:\\n\\n{e}')\n\n    meth_name = 'reset_class_vars'\n    if hasattr(cls, meth_name):\n        raise AttributeError(\n            f'Class {cls} already has attribute {meth_name}. '\n            f'store_class_defaults decorator would overwrite that. Exiting.'\n        )\n    setattr(cls, meth_name, reset_class_vars)\n    return cls\n</code></pre>"},{"location":"decorators/#lib.roboduck.decorators.add_docstring","title":"<code>add_docstring(func)</code>","text":"<p>Add the docstring from another function/class to the decorated function/class.</p> <p>Ported from htools to avoid extra dependency.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>function</code> <p>Function to decorate.</p> required <p>Examples:</p> <pre><code>@add_docstring(nn.Conv2d)\nclass ReflectionPaddedConv2d(nn.Module):\n    # ...\n</code></pre> Source code in <code>lib/roboduck/decorators.py</code> <pre><code>def add_docstring(func):\n\"\"\"Add the docstring from another function/class to the decorated\n    function/class.\n\n    Ported from [htools](https://github.com/hdmamin/htools) to avoid extra\n    dependency.\n\n    Parameters\n    ----------\n    func : function\n        Function to decorate.\n\n    Examples\n    --------\n    ```\n    @add_docstring(nn.Conv2d)\n    class ReflectionPaddedConv2d(nn.Module):\n        # ...\n    ```\n    \"\"\"\n    def decorator(new_func):\n        new_func.__doc__ = f'{new_func.__doc__}\\n\\n{func.__doc__}'\n        @wraps(new_func)\n        def wrapper(*args, **kwargs):\n            return new_func(*args, **kwargs)\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"errors/","title":"Errors","text":"<p>Errors that explain themselves! Or more precisely, errors that are explained to you by a gpt-esque model. Simply importing this module will change python's default behavior when it encounters an error.</p>"},{"location":"errors/#lib.roboduck.errors--quickstart","title":"Quickstart","text":"<p>Importing the errors module automatically enables optional error explanations. <code>disable()</code> reverts to python's regular behavior on errors. <code>enable()</code> can be used to re-enable error explanations or to change settings. For example, setting auto=True automatically explains all errors rather than asking the user if they want an explanation (y/n) when an error occurs. <pre><code>from roboduck import errors\n\ndata = {'x': 0}\ny = data.x\n\nerrors.disable()\ny = data.x\n\nerrors.enable(auto=True)\ny = data.x\n</code></pre></p>"},{"location":"errors/#lib.roboduck.errors-attributes","title":"Attributes","text":""},{"location":"errors/#lib.roboduck.errors.default_excepthook","title":"<code>default_excepthook = sys.excepthook</code>  <code>module-attribute</code>","text":""},{"location":"errors/#lib.roboduck.errors.ipy","title":"<code>ipy = get_ipython()</code>  <code>module-attribute</code>","text":""},{"location":"errors/#lib.roboduck.errors-functions","title":"Functions","text":""},{"location":"errors/#lib.roboduck.errors.post_mortem","title":"<code>post_mortem(t=None, Pdb=DuckDB, trace='', prompt_name='debug_stack_trace', colordiff=True, interactive=False, **kwargs)</code>","text":"<p>Drop-in replacement (hence the slightly odd arg order, where trace is required but third positionally) for pdb.post_mortem that allows us to get both the stack trace AND global/local vars from the program state right before an exception occurred.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>traceback</code> <p>Error traceback if one is available. Single character name is a holdover from the default post_mortem.</p> <code>None</code> <code>Pdb</code> <code>type</code> <p>Debugger class. Name is capitalized to provide consistent interface with default post_mortem function.</p> <code>DuckDB</code> <code>trace</code> <code>str</code> <p>Stack trace formatted as a single string. Required - default value just helps us maintain a consistent interface with pdb.post_mortem.</p> <code>''</code> <code>prompt_name</code> <code>str</code> <p>The prompt name that will be passed to our debugger class. Usually should leave this as the default. We expect the name to contain 'debug' and will warn if it doesn't.</p> <code>'debug_stack_trace'</code> <code>colordiff</code> <code>bool</code> <p>If True, the new code snippet in the exception will print new parts in green.</p> <code>True</code> <code>interactive</code> <code>bool</code> <p>If False, LLM will just explain the error and exit. If True, user instead will be thrown into an interactive debugging session where they can ask followup questions.</p> <code>False</code> <code>kwargs</code> <code>any</code> <p>Additional kwargs to pass to debugger class constructor. The docstring of the default class is included below for reference.</p> <code>{}</code> Source code in <code>lib/roboduck/errors.py</code> <pre><code>@add_docstring(DuckDB.__init__)\ndef post_mortem(t=None, Pdb=DuckDB, trace='', prompt_name='debug_stack_trace',\n                colordiff=True, interactive=False, **kwargs):\n\"\"\"Drop-in replacement (hence the slightly odd arg order, where trace is\n    required but third positionally) for pdb.post_mortem that allows us to get\n    both the stack trace AND global/local vars from the program state right\n    before an exception occurred.\n\n    Parameters\n    ----------\n    t : traceback\n        Error traceback if one is available. Single character name is a\n        holdover from the default post_mortem.\n    Pdb : type\n        Debugger class. Name is capitalized to provide consistent interface\n        with default post_mortem function.\n    trace : str\n        Stack trace formatted as a single string. Required - default value\n        just helps us maintain a consistent interface with pdb.post_mortem.\n    prompt_name : str\n        The prompt name that will be passed to our debugger class. Usually\n        should leave this as the default. We expect the name to contain\n        'debug' and will warn if it doesn't.\n    colordiff : bool\n        If True, the new code snippet in the exception will print new\n        parts in green.\n    interactive : bool\n        If False, LLM will just explain the error and exit. If True, user\n        instead will be thrown into an interactive debugging session where\n        they can ask followup questions.\n    kwargs : any\n        Additional kwargs to pass to debugger class constructor. The docstring\n        of the default class is included below for reference.\n    \"\"\"\n    if t is None:\n        t = sys.exc_info()[2]\n        assert t is not None, \"post_mortem outside of exception context\"\n    if 'debug' not in prompt_name:\n        warnings.warn(\n            f'You passed an unexpected prompt_name ({prompt_name}) to '\n            f'post_mortem. Are you sure you didn\\'t mean to use '\n            f'debug_stack_trace?'\n        )\n    assert trace, 'Trace passed to post_mortem should be truthy.'\n\n    # This serves almost like a soft assert statement - if user defines some\n    # custom debugger class and the question leaks through, gpt should\n    # hopefully warn us.\n    dummy_question = (\n        'This is a fake question to ensure that our ask_language_model '\n        'method gets called. Our debugger class should remove this from the '\n        'prompt kwargs before calling gpt. If you can read this, can you '\n        'indicate that in your response?'\n    )\n    kwargs['color'] = kwargs.get('color', 'red')\n    p = Pdb(prompt_name=prompt_name, **kwargs)\n    p.reset()\n    p.cmdqueue.insert(0, (dummy_question, trace))\n    if interactive:\n        print('When done, enter `q` to quit.\\n')\n    else:\n        p.cmdqueue.insert(1, 'q')\n    p.interaction(None, t)\n\n    # Make gpt explanation available as part of last error message,\n    # accessible via sys.last_value.\n    last_value = getattr(sys, 'last_value', None)\n    if CodeCompletionCache.last_completion and last_value:\n        code_name = 'last_code_diff' if colordiff else 'last_new_code'\n        last_value.args = tuple(\n            arg if i else f'{arg}\\n\\n{CodeCompletionCache.last_explanation}'\n                          f'\\n\\n{getattr(CodeCompletionCache, code_name)}'\n            for i, arg in enumerate(last_value.args)\n        )\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.print_exception","title":"<code>print_exception(etype, value, tb, limit=None, file=None, chain=True)</code>","text":"<p>Replacement for traceback.print_exception() that returns the whole stack trace as a single string. Used in roboduck's custom excepthook to allow us to show the stack trace to gpt. The original function's docstring is below:</p> <p>Print exception up to 'limit' stack trace entries from 'tb' to 'file'.</p> <p>This differs from print_tb() in the following ways: (1) if traceback is not None, it prints a header \"Traceback (most recent call last):\"; (2) it prints the exception type and value after the stack trace; (3) if type is SyntaxError and value has the appropriate format, it prints the line where the syntax error occurred with a caret on the next line indicating the approximate position of the error.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def print_exception(etype, value, tb, limit=None, file=None, chain=True):\n\"\"\"Replacement for traceback.print_exception() that returns the\n    whole stack trace as a single string. Used in roboduck's custom excepthook\n    to allow us to show the stack trace to gpt. The original function's\n    docstring is below:\n\n    Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.\n    \"\"\"\n    # format_exception has ignored etype for some time, and code such as cgitb\n    # passes in bogus values as a result. For compatibility with such code we\n    # ignore it here (rather than in the new TracebackException API).\n    if file is None:\n        file = sys.stderr\n    trace = ''.join(\n        TracebackException(type(value), value, tb, limit=limit)\n        .format(chain=chain)\n    )\n    if file != sys.stderr:\n        with open(file, 'w') as f:\n            f.write(trace)\n    return trace\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.excepthook","title":"<code>excepthook(etype, val, tb, prompt_name='debug_stack_trace', auto=False, cls=DuckDB, **kwargs)</code>","text":"<p>Replaces sys.excepthook when module is imported. When an error is thrown, the user is asked whether they want an explanation of what went wrong. If they enter 'y' or 'yes', it will query gpt for help. Unlike roboduck.debug.duck(), the user does not need to manually type a question. By default we don't linger in the debugger - we just provide an explanation and exit. Passing in interactive=True allows the user to ask followup questions.</p> <p>Disable by calling roboduck.errors.disable().</p> <p>Parameters are the same as the default sys.excepthook function. Kwargs are forwarded to our custom post_mortem function.</p> <p>Parameters:</p> Name Type Description Default <code>etype</code> <code>type</code> <p>The class of exception that just occurred.</p> required <code>val</code> <code>Exception</code> <p>The error that just occurred.</p> required <code>tb</code> <code>traceback</code> <p>The traceback from the error that just occurred.</p> required <code>prompt_name</code> <code>str</code> <p>The roboduck prompt to use (can be a builtin option in roboduck.prompts.chat or a path to a user-defined yaml file).</p> <code>'debug_stack_trace'</code> <code>auto</code> <code>bool</code> <p>If True, automatically start explaining every error that occurs (usually not recommended). If False, user will be asked to type y/n before asking an LLM.</p> <code>False</code> <code>cls</code> <code>type</code> <p>The debugger class to use. By default it's the roboduck equivalent of pdb.Pdb, but you could also subclass DuckDB and do something custom.</p> <code>DuckDB</code> <code>kwargs</code> <code>any</code> <p>Additional kwargs to pass to the debugger cls, e.g. \"model_name='gpt-4'\".</p> <code>{}</code> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def excepthook(etype, val, tb, prompt_name='debug_stack_trace',\n               auto=False, cls=DuckDB, **kwargs):\n\"\"\"Replaces sys.excepthook when module is imported. When an error is\n    thrown, the user is asked whether they want an explanation of what went\n    wrong. If they enter 'y' or 'yes', it will query gpt for help. Unlike\n    roboduck.debug.duck(), the user does not need to manually type a\n    question. By default we don't linger in the debugger - we just provide an\n    explanation and exit. Passing in interactive=True allows the user to ask\n    followup questions.\n\n    Disable by calling roboduck.errors.disable().\n\n    Parameters are the same as the default sys.excepthook function. Kwargs\n    are forwarded to our custom post_mortem function.\n\n    Parameters\n    ----------\n    etype : type\n        The class of exception that just occurred.\n    val : Exception\n        The error that just occurred.\n    tb : traceback\n        The traceback from the error that just occurred.\n    prompt_name : str\n        The roboduck prompt to use (can be a builtin option in\n        roboduck.prompts.chat or a path to a user-defined yaml file).\n    auto : bool\n        If True, automatically start explaining every error that occurs\n        (usually not recommended). If False, user will be asked to type y/n\n        before asking an LLM.\n    cls : type\n        The debugger class to use. By default it's the roboduck equivalent of\n        pdb.Pdb, but you could also subclass DuckDB and do something custom.\n    kwargs : any\n        Additional kwargs to pass to the debugger cls,\n        e.g. \"model_name='gpt-4'\".\n    \"\"\"\n    sys.last_type, sys.last_value, sys.last_traceback = etype, val, tb\n    trace = print_exception(etype, val, tb)\n    if not kwargs.get('silent', False):\n        print(trace)\n    kwargs.update(prompt_name=prompt_name, trace=trace, t=tb, Pdb=cls)\n    if auto:\n        return post_mortem(**kwargs)\n    while True:\n        cmd = input('Explain error message? [y/n]\\n').lower().strip()\n        if cmd in ('y', 'yes'):\n            return post_mortem(**kwargs)\n        if cmd in ('n', 'no'):\n            return\n        print('Unrecognized command. Valid choices are \"y\" or \"n\".\\n')\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.enable","title":"<code>enable(**kwargs)</code>","text":"<p>Enable conversational debugging mode. This is called automatically on module import. However, users may wish to make changes, e.g. set auto=True or pass in a custom debugger cls, and this function makes that possible.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>any</code> <p>These are passed to our custom debugger class. Some common args are below:</p> <p>auto (bool) - if True, automatically have an LLM explain every error that occurs without asking the user for confirmation. This is used in our logging module, for instance. You almost certainly want to keep this as False (the default) for any interactive development. interactive (bool) - if True, error explanations will leave the user in an interactive session where they can ask followup questions. If False (the default), we will simply explain the error and exit the session (more similar to standard python error handling). cls (type) - the debugger class to use. prompt_name (str) - determines what prompt/prompt_name the custom debugger uses, e.g. \"debug_stack_trace\". Users can also define their own custom prompt (https://hdmamin.github.io/roboduck/custom_prompts/) and pass in the file path here. colordiff (bool) - if True, new code snippet will print new parts in green.</p> <code>{}</code> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def enable(**kwargs):\n\"\"\"Enable conversational debugging mode. This is called automatically on\n    module import. However, users may wish to make changes, e.g. set auto=True\n    or pass in a custom debugger cls, and this function makes that possible.\n\n    Parameters\n    ----------\n    kwargs : any\n        These are passed to our custom debugger class. Some common args are\n        below:\n\n        auto (bool) - if True, automatically have an LLM explain every error\n            that occurs without asking the user for confirmation. This is used\n            in our logging module, for instance. You almost\n            certainly want to keep this as False (the default) for any\n            interactive development.\n        interactive (bool) - if True, error explanations will leave the user in\n            an interactive session where they can ask followup questions. If\n            False (the default), we will simply explain the error and exit the\n            session (more similar to standard python error handling).\n        cls (type) - the debugger class to use.\n        prompt_name (str) - determines what prompt/prompt_name the custom\n            debugger uses, e.g. \"debug_stack_trace\". Users can also define\n            their own custom prompt\n            (https://hdmamin.github.io/roboduck/custom_prompts/)\n            and pass in the file path here.\n        colordiff (bool) - if True, new code snippet will print new parts\n            in green.\n    \"\"\"\n    hook = partial(excepthook, **kwargs)\n\n    def ipy_excepthook(self, etype, evalue, tb, tb_offset):\n\"\"\"IPython doesn't use sys.excepthook. We have to handle this case\n        separately and make sure it expects the right argument names.\n        \"\"\"\n        return hook(etype, evalue, tb)\n\n    # Overwrite default error handling.\n    sys.excepthook = hook\n\n    # Only necessary/possible when in ipython.\n    try:\n        ipy.set_custom_exc((Exception,), ipy_excepthook)\n    except AttributeError:\n        pass\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.disable","title":"<code>disable()</code>","text":"<p>Revert to default behavior when exceptions are thrown.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def disable():\n\"\"\"Revert to default behavior when exceptions are thrown.\"\"\"\n    sys.excepthook = default_excepthook\n    try:\n        # Tried doing `ipy.set_custom_exc((Exception,), None)` as suggested by\n        # stackoverflow and chatgpt but it didn't quite restore the default\n        # behavior. Manually remove this instead. I'm assuming only one custom\n        # exception handler can be assigned for any one exception type and that\n        # if we call disable(), we wish to remove the handler for Exception.\n        ipy.custom_exceptions = tuple(x for x in ipy.custom_exceptions\n                                      if x != Exception)\n    except AttributeError:\n        pass\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.stack_trace","title":"<code>stack_trace()</code>","text":"<p>Lets us recover stack trace as string outside of the functions defined above, which generally only execute automatically when exceptions are thrown. Don't just define this as a partial because that requires sys.last_value etc. to be available at import time, which it often isn't. In the end I think I only used this for development purposes.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def stack_trace():\n\"\"\"Lets us recover stack trace as string outside of the functions defined\n    above, which generally only execute automatically when exceptions are\n    thrown. Don't just define this as a partial because that requires\n    sys.last_value etc. to be available at import time, which it often isn't.\n    In the end I think I only used this for development purposes.\n    \"\"\"\n    try:\n        return print_exception(sys.last_type, sys.last_value,\n                               sys.last_traceback)\n    except AttributeError as e:\n        raise RuntimeError('No stack trace available because an error has '\n                           'not been thrown.') from e\n</code></pre>"},{"location":"ipy_utils/","title":"IPython utils","text":"<p>Functions related to loading, saving, or otherwise working with ipython sessions or jupyter notebooks.</p>"},{"location":"ipy_utils/#lib.roboduck.ipy_utils-functions","title":"Functions","text":""},{"location":"ipy_utils/#lib.roboduck.ipy_utils.load_ipynb","title":"<code>load_ipynb(path, save_if_self=True)</code>","text":"<p>Loads ipynb and formats cells into 1 big string.</p> <p>Adapted from htools.cli.ReadmeUpdater method.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to notebook to load.</p> required <code>save_if_self</code> <code>bool</code> <p>If True, check if this is being called from the current notebook. If so, save it. (If not, we never save - auto saving is only intended to address the scenario where we're in an active notebook and call this function before recent changes have been saved. The load_ipynb call itself means that at least 1 change has inevitably occurred since saving.)</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Code contents of notebook. Each cell is enclosed in triple backticks and separated by newlines.</p> Source code in <code>lib/roboduck/ipy_utils.py</code> <pre><code>def load_ipynb(path, save_if_self=True):\n\"\"\"Loads ipynb and formats cells into 1 big string.\n\n    Adapted from htools.cli.ReadmeUpdater method.\n\n    Parameters\n    ----------\n    path : Path\n        Path to notebook to load.\n    save_if_self : bool\n        If True, check if this is being called from the current notebook. If\n        so, save it. (If not, we never save - auto saving is only intended to\n        address the scenario where we're in an active notebook and call this\n        function before recent changes have been saved. The load_ipynb call\n        itself means that at least 1 change has inevitably occurred since\n        saving.)\n\n    Returns\n    -------\n    str\n        Code contents of notebook. Each cell is enclosed in triple backticks\n        and separated by newlines.\n    \"\"\"\n    if save_if_self:\n        try:\n            self_path = ipynbname.path()\n        except FileNotFoundError:\n            pass\n        else:\n            if self_path == path:\n                save_notebook(path)\n\n    with open(path, 'r') as f:\n        cells = json.load(f)['cells']\n\n    cell_str = ''\n    for cell in cells:\n        if not cell['source']:\n            continue\n        source = '\\n' + ''.join(cell['source']) + '\\n'\n        if cell['cell_type'] == 'code':\n            source = '\\n```' + source + '```\\n'\n        cell_str += source\n    return cell_str\n</code></pre>"},{"location":"ipy_utils/#lib.roboduck.ipy_utils.load_current_ipython_session","title":"<code>load_current_ipython_session(formatted=True)</code>","text":"<p>Load current ipython session as a list and optionally convert it to a nicely formatted str with each cell enclosed in triple backticks.</p> <p>Parameters:</p> Name Type Description Default <code>formatted</code> <code>bool</code> <p>If True, format list of cells into a single str like this (note: if you don't see any backticks below, know that each print statement is enclosed in a separate pair of triple backticks. Rendering this nicely with mkdocs is very tricky and even if it worked, it would badly mess up readability for people viewing the docstring in their IDE):</p> <p>''' <pre><code>print('This is cell 1 code.')\n</code></pre></p> <p><pre><code>print('This is cell 2 code.')\n</code></pre> '''</p> <p>If False, leave it as a list of strings where each string contains content from one cell.</p> <code>True</code> <p>Returns:</p> Type Description <code>list or str</code> Source code in <code>lib/roboduck/ipy_utils.py</code> <pre><code>def load_current_ipython_session(formatted=True):\n\"\"\"Load current ipython session as a list and optionally convert it to a\n    nicely formatted str with each cell enclosed in triple backticks.\n\n    Parameters\n    ----------\n    formatted : bool\n        If True, format list of cells into a single str like this (note: if\n        you don't see any backticks below, know that each print statement is\n        enclosed in a separate pair of triple backticks. Rendering this nicely\n        with mkdocs is very tricky and even if it worked, it would badly mess\n        up readability for people viewing the docstring in their IDE):\n\n        '''\n        ```\n        print('This is cell 1 code.')\n        ```\n\n        ```\n        print('This is cell 2 code.')\n        ```\n        '''\n\n        If False, leave it as a list of strings where each string contains\n        content from one cell.\n\n    Returns\n    -------\n    list or str\n    \"\"\"\n    shell = get_ipython()\n    path = Path('/tmp')/f'{secrets.token_hex(24)}.txt'\n    shell.magic(f'%history -n -f {path}')\n    with open(path, 'r') as f:\n        res = f.read()\n    path.unlink()\n    cells = []\n    for row in res.splitlines():\n        content = row.partition(':')[-1].strip()\n        if content:\n            cells.append(content)\n    if formatted:\n        return '\\n\\n'.join(f'```\\n{cell}\\n```' for cell in cells)\n    return cells\n</code></pre>"},{"location":"ipy_utils/#lib.roboduck.ipy_utils.is_ipy_name","title":"<code>is_ipy_name(name, count_as_true=('In', 'Out', '_dh', '_ih', '_ii', '_iii', '_oh'))</code>","text":"<pre><code>Check if a variable name looks like an ipython output cell name, e.g.\n\"_49\", \"_\", or \"__\".\n\nPorted from [htools](https://github.com/hdmamin/htools) to avoid extra\ndependency.\n\nMore examples:\nReturns True for names like this (technically not sure if something like\n\"__i3\" is actually used in ipython, but it looks like something we\nprobably want to remove it in these contexts anyway.\n['_', '__', '_i3', '__i3', '_4', '_9913', '__7', '__23874']\n\nReturns False for names like\n['_a', 'i22', '__0i', '_03z', '__99t']\nand most \"normal\" variable names.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The variable name to check.</p> required <code>count_as_true</code> <code>Iterable[str]</code> <p>Additional variable names that don't necessarily fit the standard pattern but should nonetheless return True if we encounter them.</p> <code>('In', 'Out', '_dh', '_ih', '_ii', '_iii', '_oh')</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if it looks like an ipython output cell name, False otherwise.</p> Source code in <code>lib/roboduck/ipy_utils.py</code> <pre><code>def is_ipy_name(\n        name,\n        count_as_true=('In', 'Out', '_dh', '_ih', '_ii', '_iii', '_oh')\n):\n\"\"\"```plaintext\n    Check if a variable name looks like an ipython output cell name, e.g.\n    \"_49\", \"_\", or \"__\".\n\n    Ported from [htools](https://github.com/hdmamin/htools) to avoid extra\n    dependency.\n\n    More examples:\n    Returns True for names like this (technically not sure if something like\n    \"__i3\" is actually used in ipython, but it looks like something we\n    probably want to remove it in these contexts anyway.\n    ['_', '__', '_i3', '__i3', '_4', '_9913', '__7', '__23874']\n\n    Returns False for names like\n    ['_a', 'i22', '__0i', '_03z', '__99t']\n    and most \"normal\" variable names.\n    ```\n\n    Parameters\n    ----------\n    name : str\n        The variable name to check.\n    count_as_true : Iterable[str]\n        Additional variable names that don't necessarily fit the standard\n        pattern but should nonetheless return True if we encounter them.\n\n    Returns\n    -------\n    bool\n        True if it looks like an ipython output cell name, False otherwise.\n    \"\"\"\n    # First check if it fits the standard leading underscore format.\n    # Easier to handle the \"only underscores\" case separately because we want\n    # to limit the number of underscores for names like \"_i3\".\n    pat = '^_{1,2}i?\\\\d*$'\n    is_under = bool(re.match(pat, name)) or not name.strip('_')\n    return is_under or name in count_as_true\n</code></pre>"},{"location":"ipy_utils/#lib.roboduck.ipy_utils.save_notebook","title":"<code>save_notebook(file_path)</code>","text":"<p>Save a jupyter notebook. We use this in load_ipynb (optionally) to ensure that when we load a notebook's source code, we get the most up to date version. Adapted from https://stackoverflow.com/questions/32237275/save-an-ipython-notebook-programmatically-from-within-itself/57814673#57814673</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to notebook that you want to save.</p> required Source code in <code>lib/roboduck/ipy_utils.py</code> <pre><code>def save_notebook(file_path):\n\"\"\"Save a jupyter notebook. We use this in load_ipynb (optionally) to\n    ensure that when we load a notebook's source\n    code, we get the most up to date version. Adapted from\n    https://stackoverflow.com/questions/32237275/save-an-ipython-notebook-programmatically-from-within-itself/57814673#57814673\n\n    Parameters\n    ----------\n    file_path : str\n        Path to notebook that you want to save.\n    \"\"\"\n    def file_md5(path):\n        with open(path, 'rb') as f:\n            text = f.read()\n        return hashlib.md5(text).hexdigest()\n\n    start_md5 = file_md5(file_path)\n    display(Javascript('IPython.notebook.save_checkpoint();'))\n    current_md5 = start_md5\n\n    while start_md5 == current_md5:\n        time.sleep(1)\n        current_md5 = file_md5(file_path)\n</code></pre>"},{"location":"logging/","title":"Logging","text":"<p>Logger that attempts to diagnose and propose a solution for any errors it is asked to log. Unlike our debugger and errors modules, explanations are not streamed because the intended use case is not focused on live development.</p>"},{"location":"logging/#lib.roboduck.logging--quickstart","title":"Quickstart","text":"<pre><code>from roboduck import logging\n\nlogger = logging.getLogger(path='/tmp/log.txt')\ndata = {'x': 0}\ntry:\n    x = data.x\nexcept Exception as e:\n    logger.error(e)\n</code></pre>"},{"location":"logging/#lib.roboduck.logging-classes","title":"Classes","text":""},{"location":"logging/#lib.roboduck.logging.DuckLogger","title":"<code>DuckLogger</code>","text":"<p>         Bases: <code>Logger</code></p> <p>Replacement for logging.Logger class that uses our errors module to log natural language explanations and fixes along with the original error. (More specifically, we just wait for the errors module to update the message in the original exception before logging.)</p> Source code in <code>lib/roboduck/logging.py</code> <pre><code>class DuckLogger(Logger):\n\"\"\"Replacement for logging.Logger class that uses our errors module to\n    log natural language explanations and fixes along with the original error.\n    (More specifically, we just wait for the errors module to update the\n    message in the original exception before logging.)\n    \"\"\"\n\n    def __init__(self, name, colordiff=False,\n                 fmt='%(asctime)s [%(levelname)s]: %(message)s', stdout=True,\n                 path='', fmode='a', **kwargs):\n\"\"\"\n        Parameters\n        ----------\n        name : str\n            Same as base logger name arg.\n        colordiff : bool\n            Another kwarg to pass to our excepthook function. This is separate\n            from the others because we want to use a different default than the\n            function has since we often log to a file, in which case\n            colordiff=True may be undesirable.\n        fmt : str\n            Defines logging format. The default format produces output like\n            this when an error is logged:\n            2023-03-08 19:20:52,514 [ERROR]: list indices must be integers or\n            slices, not tuple\n        stdout : bool\n            If True, logged items will appear in stdout. You are free to log\n            to both stdout and a file OR just one (selecting neither will raise\n            an error because logger would be useless in that case).\n        path : str or Path\n            If provided, we log to this file (the dir structure does not need\n            to exist already). If None, we do not log to a file.\n        fmode : str\n            Write mode used when path is not None. Usually 'a' but 'w' might\n            be a reasonable choice in some circumstances.\n        kwargs : any\n            Kwargs that can be passed to our excepthook function. Most of these\n            should generally be kwargs for your debugger class,\n            e.g. RoboDuckDb. These will be updated with the specified\n            `colordiff` as well - we want to set the default to False here\n             because we often want to log to a file, where this will probably\n             not render correctly.\n        \"\"\"\n        if not stdout and not path:\n            raise RuntimeError(\n                f'{type(self).__name__} requires that you set stdout=True '\n                f'and/or provide a non-empty path. Currently, your logger '\n                f'would do neither and be useless.'\n            )\n\n        super().__init__(name)\n        self.excepthook_kwargs = kwargs or {}\n        # Always want silent=True because we don't care about live typing here.\n        # If stdout=True, our super()._log() call still ensures that we log to\n        # stdout after the gpt call completes.\n        defaults = dict(auto=True, sleep=0, silent=True, interactive=False)\n        for k, v in defaults.items():\n            if self.excepthook_kwargs.get(k, v) != v:\n                warnings.warn(\n                    f'You tried to set {k}={self.excepthook_kwargs[k]} '\n                    f'but it must equal {v} in logger. Your arg will be'\n                    f'overridden.'\n                )\n        self.excepthook_kwargs.update(defaults)\n        self.excepthook_kwargs['colordiff'] = self.excepthook_kwargs.get(\n            'colordiff', colordiff\n        )\n        self._add_handlers(fmt, stdout, path, fmode)\n\n    def _add_handlers(self, fmt, stdout, path, fmode):\n\"\"\"Set up handlers to log to stdout and/or a file.\"\"\"\n        formatter = Formatter(fmt)\n        handlers = []\n        if stdout:\n            handlers.append(StreamHandler(sys.stdout))\n        else:\n            # If we don't set this when stdout is False, the root logger ends\n            # up logging to stdout anyway.\n            self.propagate = False\n\n        if path:\n            path = Path(path).resolve()\n            os.makedirs(path.parent, exist_ok=True)\n            handlers.append(FileHandler(path, fmode))\n        for handler in handlers:\n            handler.setFormatter(formatter)\n            self.addHandler(handler)\n\n    def _log(self, level, msg, args, exc_info=None, extra=None,\n             stack_info=False):\n\"\"\"This is where we insert our custom logic to get error explanations.\n        We keep the import inside the method to avoid overwriting\n        sys.excepthook whenever the logging module is imported.\n\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        if isinstance(msg, Exception) and sys.exc_info()[2]:\n            from roboduck import errors\n            errors.excepthook(type(msg), msg, msg.__traceback__,\n                              **self.excepthook_kwargs)\n            msg = sys.last_value\n            errors.disable()\n\n        return super()._log(level, msg, args, exc_info=exc_info,\n                            extra=extra, stack_info=stack_info)\n</code></pre>"},{"location":"logging/#lib.roboduck.logging.DuckLogger-attributes","title":"Attributes","text":""},{"location":"logging/#lib.roboduck.logging.DuckLogger.excepthook_kwargs","title":"<code>excepthook_kwargs = kwargs or {}</code>  <code>instance-attribute</code>","text":""},{"location":"logging/#lib.roboduck.logging-functions","title":"Functions","text":""},{"location":"logging/#lib.roboduck.logging.getLogger","title":"<code>getLogger(name=None, **kwargs)</code>","text":"<p>Mimics interface of builtin logging module. I.e. we can do:</p> <pre><code>from roboduck import logging\n\nlogger = logging.getLogger()\n</code></pre> Source code in <code>lib/roboduck/logging.py</code> <pre><code>def getLogger(name=None, **kwargs):\n\"\"\"Mimics interface of builtin logging module. I.e. we can do:\n\n    ```\n    from roboduck import logging\n\n    logger = logging.getLogger()\n    ```\n    \"\"\"\n    return DuckLogger(name=name, **kwargs)\n</code></pre>"},{"location":"magic/","title":"Magic","text":"<p>GPT-powered rough equivalent of the <code>%debug</code> Jupyter magic. After an error occurs, just run %duck in the next cell to get an explanation. This is very similar to using the errors module, but is less intrusive - you only call it when you want an explanation, rather than having to type y/n after each error. We also provide <code>paste</code> mode, which attempts to paste a solution into a new code cell below, and <code>interactive</code> mode, which throws you into a conversational debugging session (technically closer to the original <code>%debug</code> magic functionality.</p>"},{"location":"magic/#lib.roboduck.magic--quickstart","title":"Quickstart","text":"<pre><code># cell 1\nfrom roboduck import magic\n\nnums = [1, 2, 3]\nnums.add(4)\n</code></pre> <pre><code># cell 2\n%duck\n</code></pre>"},{"location":"magic/#lib.roboduck.magic-classes","title":"Classes","text":""},{"location":"magic/#lib.roboduck.magic.DebugMagic","title":"<code>DebugMagic</code>","text":"<p>         Bases: <code>Magics</code></p> <p>Enter a conversational debugging session after an error is thrown by a jupyter notebook code cell.</p> <p>Examples:</p> <p>After a cell execution throws an error, enter this in the next cell to get an explanation of what caused the error and how to fix it:</p> <p>%duck</p> <p>To instead enter an Interactive conversational debugging session:</p> <p>%duck -i</p> <p>If the -p flag is present, we will try to Paste a solution into a new code cell upon exiting the debugger containing the fixed code snippet.</p> <p>%duck -p</p> <p>Flags can be combined:</p> <p>%duck -ip</p> Source code in <code>lib/roboduck/magic.py</code> <pre><code>@magics_class\nclass DebugMagic(Magics):\n\"\"\"Enter a conversational debugging session after an error is thrown by a\n    jupyter notebook code cell.\n\n    Examples\n    --------\n    After a cell execution throws an error, enter this in the next cell to\n    get an explanation of what caused the error and how to fix it:\n\n    %duck\n\n    To instead enter an Interactive conversational debugging session:\n\n    %duck -i\n\n    If the -p flag is present, we will try to Paste a solution into a new\n    code cell upon exiting the debugger containing the fixed code snippet.\n\n    %duck -p\n\n    Flags can be combined:\n\n    %duck -ip\n    \"\"\"\n\n    @magic_arguments()\n    @argument('-p', action='store_true',\n              help='Boolean flag: if provided, try to PASTE a solution into a '\n                   'new code cell below.')\n    @argument('-i', action='store_true',\n              help='Boolean flag: if provided, use INTERACTIVE mode. Start a '\n                   'conversational debugger session and allow the user to ask '\n                   'custom questions, just as they would if using '\n                   'roboduck.debug.duck(). The default mode, meanwhile, '\n                   'simply asks gpt what caused the error that just '\n                   'occurred and then exits, rather than lingering in a '\n                   'debugger session.')\n    @argument('--prompt', type=str, default=None)\n    @line_magic\n    def duck(self, line=''):\n\"\"\"Silence warnings for a cell. The -p flag can be used to make the\n        change persist, at least until the user changes it again.\n        \"\"\"\n        args = parse_argstring(self.duck, line)\n        if args.prompt:\n            warnings.warn('Support for custom prompts is somewhat limited - '\n                          'your prompt must use the default parse_func '\n                          '(roboduck.utils.parse_completion).')\n        if args.i:\n            old_cls = self.shell.debugger_cls\n            if args.prompt:\n                new_cls = partial(DuckDB, prompt=args.prompt)\n            else:\n                new_cls = DuckDB\n            try:\n                self.shell.debugger_cls = new_cls\n            except AttributeError:\n                print(\n                    'Roboduck is unavailable in your current ipython session. '\n                    'To use it, start a new session with the command:\\n\\n'\n                    'ipython --TerminalIPythonApp.interactive_shell_class='\n                    'roboduck.shell.RoboDuckTerminalInteractiveShell\\n\\n'\n                    '(You will also need to run `from roboduck import '\n                    'magic` in the session to make the magic available.) To '\n                    'make it available automatically for all '\n                    'ipython sessions by default, add the following lines to '\n                    'your ipython config (usually found at '\n                    '~/.ipython/profile_default/ipython_config.py):\\n\\n'\n                    'cfg = get_config()\\ncfg.TerminalIPythonApp.interactive_'\n                    'shell_class = roboduck.shell.'\n                    'RoboDuckTerminalInteractiveShell'\n                    '\\ncfg.InteractiveShellApp.exec_lines = '\n                    '[\"from roboduck import magic\"]'\n                )\n                return\n            self.shell.InteractiveTB.debugger_cls = new_cls\n            self.shell.debugger(force=True)\n            self.shell.debugger_cls = self.shell.InteractiveTB.debugger_cls = old_cls\n        else:\n            # Confine this import to this if clause rather than keeping a top\n            # level import - importing this module overwrites sys.excepthook\n            # which we don't necessarily want in most cases.\n            # Note that this uses the `debug_stack_trace` prompt by default\n            # whereas interactive mode uses `debug` by default.\n            # UPDATE: we could probably use excepthook for both cases\n            # (args.i = True or False) now that\n            # I added interactive support in the errors module. However,\n            # everything is working nicely now and I don't see a compelling\n            # reason to change things at the moment.\n            from roboduck import errors\n            kwargs = {'auto': True, 'color': 'green'}\n            if args.prompt:\n                kwargs['prompt'] = args.prompt\n            errors.excepthook(sys.last_type, sys.last_value,\n                              sys.last_traceback, **kwargs)\n            errors.disable()\n\n        # Insert suggested code into next cell.\n        if args.p and CodeCompletionCache.last_completion:\n            self.shell.set_next_input(CodeCompletionCache.last_new_code,\n                                      replace=False)\n        CodeCompletionCache.reset_class_vars()\n</code></pre>"},{"location":"magic/#lib.roboduck.magic.DebugMagic-functions","title":"Functions","text":""},{"location":"magic/#lib.roboduck.magic.DebugMagic.duck","title":"<code>duck(line='')</code>","text":"<p>Silence warnings for a cell. The -p flag can be used to make the change persist, at least until the user changes it again.</p> Source code in <code>lib/roboduck/magic.py</code> <pre><code>@magic_arguments()\n@argument('-p', action='store_true',\n          help='Boolean flag: if provided, try to PASTE a solution into a '\n               'new code cell below.')\n@argument('-i', action='store_true',\n          help='Boolean flag: if provided, use INTERACTIVE mode. Start a '\n               'conversational debugger session and allow the user to ask '\n               'custom questions, just as they would if using '\n               'roboduck.debug.duck(). The default mode, meanwhile, '\n               'simply asks gpt what caused the error that just '\n               'occurred and then exits, rather than lingering in a '\n               'debugger session.')\n@argument('--prompt', type=str, default=None)\n@line_magic\ndef duck(self, line=''):\n\"\"\"Silence warnings for a cell. The -p flag can be used to make the\n    change persist, at least until the user changes it again.\n    \"\"\"\n    args = parse_argstring(self.duck, line)\n    if args.prompt:\n        warnings.warn('Support for custom prompts is somewhat limited - '\n                      'your prompt must use the default parse_func '\n                      '(roboduck.utils.parse_completion).')\n    if args.i:\n        old_cls = self.shell.debugger_cls\n        if args.prompt:\n            new_cls = partial(DuckDB, prompt=args.prompt)\n        else:\n            new_cls = DuckDB\n        try:\n            self.shell.debugger_cls = new_cls\n        except AttributeError:\n            print(\n                'Roboduck is unavailable in your current ipython session. '\n                'To use it, start a new session with the command:\\n\\n'\n                'ipython --TerminalIPythonApp.interactive_shell_class='\n                'roboduck.shell.RoboDuckTerminalInteractiveShell\\n\\n'\n                '(You will also need to run `from roboduck import '\n                'magic` in the session to make the magic available.) To '\n                'make it available automatically for all '\n                'ipython sessions by default, add the following lines to '\n                'your ipython config (usually found at '\n                '~/.ipython/profile_default/ipython_config.py):\\n\\n'\n                'cfg = get_config()\\ncfg.TerminalIPythonApp.interactive_'\n                'shell_class = roboduck.shell.'\n                'RoboDuckTerminalInteractiveShell'\n                '\\ncfg.InteractiveShellApp.exec_lines = '\n                '[\"from roboduck import magic\"]'\n            )\n            return\n        self.shell.InteractiveTB.debugger_cls = new_cls\n        self.shell.debugger(force=True)\n        self.shell.debugger_cls = self.shell.InteractiveTB.debugger_cls = old_cls\n    else:\n        # Confine this import to this if clause rather than keeping a top\n        # level import - importing this module overwrites sys.excepthook\n        # which we don't necessarily want in most cases.\n        # Note that this uses the `debug_stack_trace` prompt by default\n        # whereas interactive mode uses `debug` by default.\n        # UPDATE: we could probably use excepthook for both cases\n        # (args.i = True or False) now that\n        # I added interactive support in the errors module. However,\n        # everything is working nicely now and I don't see a compelling\n        # reason to change things at the moment.\n        from roboduck import errors\n        kwargs = {'auto': True, 'color': 'green'}\n        if args.prompt:\n            kwargs['prompt'] = args.prompt\n        errors.excepthook(sys.last_type, sys.last_value,\n                          sys.last_traceback, **kwargs)\n        errors.disable()\n\n    # Insert suggested code into next cell.\n    if args.p and CodeCompletionCache.last_completion:\n        self.shell.set_next_input(CodeCompletionCache.last_new_code,\n                                  replace=False)\n    CodeCompletionCache.reset_class_vars()\n</code></pre>"},{"location":"shell/","title":"Shell","text":"<p>This module allows our roboduck <code>%duck</code> magic to work in ipython. Ipython uses a TerminalInteractiveShell class which makes its debugger_cls attribute read only. We provide a drop-in replacement that allows our magic class to set that attribute when necessary. Note that you'd need to start an ipython session with the command:</p> <pre><code>ipython --TerminalIPythonApp.interactive_shell_class=roboduck.shell.RoboDuckTerminalInteractiveShell\n</code></pre> <p>for this to work. You'll still need to run <code>from roboduck import magic</code> inside your session to make it avaialble.</p> <p>Alternatively, you can make it available automatically for all ipython sessions by adding the following lines to your ipython config (usually found at ~/.ipython/profile_default/ipython_config.py):</p> <pre><code>cfg = get_config()\ncfg.TerminalIPythonApp.interactive_shell_class = roboduck.shell.RoboDuckTerminalInteractiveShell\ncfg.InteractiveShellApp.exec_lines = [\"from roboduck import magic\"]\n</code></pre>"},{"location":"shell/#lib.roboduck.shell-classes","title":"Classes","text":""},{"location":"shell/#lib.roboduck.shell.RoboDuckTerminalInteractiveShell","title":"<code>RoboDuckTerminalInteractiveShell</code>","text":"<p>         Bases: <code>TerminalInteractiveShell</code></p> <p>TerminalInteractiveShell replacement class whose debugger_cls attribute is NOT read-only, thereby allowing our <code>duck</code> magic to overwrite it when necessary.</p> Source code in <code>lib/roboduck/shell.py</code> <pre><code>class RoboDuckTerminalInteractiveShell(TerminalInteractiveShell):\n\"\"\"TerminalInteractiveShell replacement class whose debugger_cls attribute\n    is NOT read-only, thereby allowing our `duck` magic to overwrite it when\n    necessary.\n    \"\"\"\n\n    debugger_cls = Pdb\n</code></pre>"},{"location":"shell/#lib.roboduck.shell.RoboDuckTerminalInteractiveShell-attributes","title":"Attributes","text":""},{"location":"shell/#lib.roboduck.shell.RoboDuckTerminalInteractiveShell.debugger_cls","title":"<code>debugger_cls = Pdb</code>  <code>class-attribute</code>","text":""},{"location":"utils/","title":"Utils","text":"<p>Utility functions used by other roboduck modules.</p>"},{"location":"utils/#lib.roboduck.utils-functions","title":"Functions","text":""},{"location":"utils/#lib.roboduck.utils.colored","title":"<code>colored(text, color)</code>","text":"<p>Add tags to color text and then reset color afterwards. Note that this does NOT actually print anything.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text that should be colored.</p> required <code>color</code> <code>str</code> <p>Color name, e.g. \"red\". Must be available in the colorama lib. If None or empty str, just return the text unchanged.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Note that you need to print() the result for it to show up in the desired color. Otherwise it will just have some unintelligible characters appended and prepended.</p> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def colored(text, color):\n\"\"\"Add tags to color text and then reset color afterwards. Note that this\n    does NOT actually print anything.\n\n    Parameters\n    ----------\n    text : str\n        Text that should be colored.\n    color : str\n        Color name, e.g. \"red\". Must be available in the colorama lib. If None\n        or empty str, just return the text unchanged.\n\n    Returns\n    -------\n    str\n        Note that you need to print() the result for it to show up in the\n        desired color. Otherwise it will just have some unintelligible\n        characters appended and prepended.\n    \"\"\"\n    if not color:\n        return text\n    color = getattr(Fore, color.upper())\n    return f'{color}{text}{Style.RESET_ALL}'\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.colordiff_new_str","title":"<code>colordiff_new_str(old, new, color='green')</code>","text":"<p>Given two strings, return the new one with new parts in green. Note that deletions are ignored because we want to retain only characters in the new string. Remember colors are only displayed correctly when printing the resulting string - otherwise it just looks like we added extra junk characters.</p> <p>Idea is that when displaying a revised code snippet from gpt, we want to draw attention to the new bits.</p> <p>Adapted from this gist + variations in comments: https://gist.github.com/ines/04b47597eb9d011ade5e77a068389521</p> <p>Parameters:</p> Name Type Description Default <code>old</code> <code>str</code> <p>This is what <code>new</code> is compared to when identifying differences.</p> required <code>new</code> <code>str</code> <p>Determines content of output str.</p> required <code>color</code> <code>str</code> <p>Text color for new characters.</p> <code>'green'</code> <p>Returns:</p> Type Description <code>str</code> <p>Same content as <code>new</code> but color new parts in a different color.</p> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def colordiff_new_str(old, new, color='green'):\n\"\"\"Given two strings, return the new one with new parts in green. Note that\n    deletions are ignored because we want to retain only characters in the new\n    string. Remember colors are only displayed correctly when printing the\n    resulting string - otherwise it just looks like we added extra junk\n    characters.\n\n    Idea is that when displaying a revised code snippet from gpt, we want to\n    draw attention to the new bits.\n\n    Adapted from this gist + variations in comments:\n    https://gist.github.com/ines/04b47597eb9d011ade5e77a068389521\n\n    Parameters\n    ----------\n    old : str\n        This is what `new` is compared to when identifying differences.\n    new : str\n        Determines content of output str.\n    color : str\n        Text color for new characters.\n\n    Returns\n    -------\n    str\n        Same content as `new` but color new parts in a different color.\n    \"\"\"\n    res = []\n    matcher = difflib.SequenceMatcher(None, old, new)\n    for opcode, s1, e1, s2, e2 in matcher.get_opcodes():\n        if opcode == 'delete':\n            continue\n        chunk = new[s2:e2]\n        if opcode in ('insert', 'replace'):\n            chunk = colored(chunk, color)\n        res.append(chunk)\n    return ''.join(res)\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.type_annotated_dict_str","title":"<code>type_annotated_dict_str(dict_, func=repr)</code>","text":"<p>String representation (or repr) of a dict, where each line includes an inline comment showing the type of the value.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>dict</code> <p>The dict to represent.</p> required <code>func</code> <code>function</code> <p>The function to apply to each key and value in the dict to get some kind of str representation. Note that it is applied to each key/value as a whole, not to each item within that key/value. See examples.</p> <code>repr</code> <p>Returns:</p> Type Description <code>str</code> <p>Examples:</p> <p>Notice below how foo and cat are not in quotes but ('bar',) and ['x'] do contain quotes.</p> <pre><code>&gt;&gt;&gt; d = {'foo': 'cat', ('bar',): ['x']}\n&gt;&gt;&gt; type_annotated_dict_str(d, str)\n{\n    foo: cat,   # type: str\n    ('bar',): ['x'],   # type: list\n}\n</code></pre> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def type_annotated_dict_str(dict_, func=repr):\n\"\"\"String representation (or repr) of a dict, where each line includes\n    an inline comment showing the type of the value.\n\n    Parameters\n    ----------\n    dict_ : dict\n        The dict to represent.\n    func : function\n        The function to apply to each key and value in the dict to get some\n        kind of str representation. Note that it is applied to each key/value\n        as a whole, not to each item within that key/value. See examples.\n\n    Returns\n    -------\n    str\n\n    Examples\n    --------\n    Notice below how foo and cat are not in quotes but ('bar',) and ['x'] do\n    contain quotes.\n    &gt;&gt;&gt; d = {'foo': 'cat', ('bar',): ['x']}\n    &gt;&gt;&gt; type_annotated_dict_str(d, str)\n    {\n        foo: cat,   # type: str\n        ('bar',): ['x'],   # type: list\n    }\n    \"\"\"\n    type_strs = [f'\\n    {func(k)}: {func(v)},   # type: {type(v).__name__}'\n                 for k, v in dict_.items()]\n    return '{' + ''.join(type_strs) + '\\n}'\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.truncated_repr","title":"<code>truncated_repr(obj, max_len=79)</code>","text":"<p>Return an object's repr, truncated to ensure that it doesn't take up more characters than we want. This is used to reduce our chances of using up all our available tokens in a gpt prompt simply communicating that a giant data structure exists, e.g. list(range(1_000_000)). Our use case doesn't call for anything super precise so the max_len should be thought of as more of guide than an exact max. I think it's enforced but I didn't put a whole lot of thought or effort into confirming that.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>any</code> required <code>max_len</code> <code>int</code> <p>Max number of characters for resulting repr. Think of this more as an estimate than a hard guarantee - precision isn't important in our use case. The result will likely be shorter than this because we want to truncate in a readable place, e.g. taking the repr of the first k items of a list instead of taking the repr of all items and then slicing off the end of the repr.</p> <code>79</code> <p>Returns:</p> Type Description <code>str</code> <p>Repr for obj, truncated to approximately max_len characters or fewer. When possible, we insert ellipses into the repr to show that truncation occurred. Technically there are some edge cases we don't handle (e.g. if obj is a class with an insanely long name) but that's not a big deal, at least at the moment. I can always revisit that later if necessary.</p> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def truncated_repr(obj, max_len=79):\n\"\"\"Return an object's repr, truncated to ensure that it doesn't take up\n    more characters than we want. This is used to reduce our chances of using\n    up all our available tokens in a gpt prompt simply communicating that a\n    giant data structure exists, e.g. list(range(1_000_000)). Our use\n    case doesn't call for anything super precise so the max_len should be\n    thought of as more of guide than an exact max. I think it's enforced but I\n    didn't put a whole lot of thought or effort into confirming that.\n\n    Parameters\n    ----------\n    obj : any\n    max_len : int\n        Max number of characters for resulting repr. Think of this more as an\n        estimate than a hard guarantee - precision isn't important in our use\n        case. The result will likely be shorter than this because we want to\n        truncate in a readable place, e.g. taking the repr of the first k items\n        of a list instead of taking the repr of all items and then slicing off\n        the end of the repr.\n\n    Returns\n    -------\n    str\n        Repr for obj, truncated to approximately max_len characters or fewer.\n        When possible, we insert ellipses into the repr to show that truncation\n        occurred. Technically there are some edge cases we don't handle (e.g.\n        if obj is a class with an insanely long name) but that's not a big\n        deal, at least at the moment. I can always revisit that later if\n        necessary.\n    \"\"\"\n    def qualname(obj):\n\"\"\"Similar to type(obj).__qualname__() but that method doesn't always\n        include the module(s). e.g. pandas Index has __qualname__ \"Index\" but\n        this funnction returns \"&lt;pandas.core.indexes.base.Index&gt;\".\n        \"\"\"\n        text = str(type(obj))\n        names = re.search(\"&lt;class '([a-zA-Z_.]*)'&gt;\", text).groups()\n        assert len(names) == 1, f'Should have found only 1 qualname but ' \\\n                                f'found: {names}'\n        return f'&lt;{names[0]}&gt;'\n\n    open2close = {\n        '[': ']',\n        '(': ')',\n        '{': '}'\n    }\n    repr_ = repr(obj)\n    if len(repr_) &lt; max_len:\n        return repr_\n    if isinstance(obj, pd.DataFrame):\n        cols = truncated_repr(obj.columns.tolist(), max_len - 26)\n        return f'pd.DataFrame(columns=' \\\n               f'{truncated_repr(cols, max_len - 22)})'\n    if isinstance(obj, pd.Series):\n        return f'pd.Series({truncated_repr(obj.tolist(), max_len - 11)})'\n    if isinstance(obj, dict):\n        length = 5\n        res = ''\n        for k, v in obj.items():\n            if length &gt;= max_len - 2:\n                break\n            new_str = f'{k!r}: {v!r}, '\n            length += len(new_str)\n            res += new_str\n        return \"{\" + res.rstrip() + \"...}\"\n    if isinstance(obj, str):\n        return repr_[:max_len - 4] + \"...'\"\n    if isinstance(obj, Iterable):\n        # A bit risky but sort of elegant. Just recursively take smaller\n        # slices until we get an acceptable length. We may end up going\n        # slightly over the max length after adding our ellipses but it's\n        # not that big a deal, this isn't meant to be super precise. We\n        # can also end up with fewer items than we could have fit - if we\n        # exhaustively check every possible length one by one until we\n        # find the max length that fits, we can get a very slow function\n        # when inputs are long.\n        # Can't easily pass smaller max_len value into recursive call\n        # because we always want to compare to the user-specified value.\n        n = int(max_len / len(repr_) * len(obj))\n        if n == len(obj):\n            # Even slicing to just first item is too long, so just revert\n            # to treating this like a non-iterable object.\n            return qualname(obj)\n        # Need to slice set while keeping the original dtype.\n        if isinstance(obj, set):\n            slice_ = set(list(obj)[:n])\n        else:\n            try:\n                slice_ = obj[:n]\n            except Exception as e:\n                warnings.warn(f'Failed to slice obj {obj}. Result may not be '\n                              f'truncated as much as desired. Error:\\n{e}')\n                slice_ = obj\n        repr_ = truncated_repr(slice_, max_len)\n        non_brace_idx = len(repr_) - 1\n        while repr_[non_brace_idx] in open2close.values():\n            non_brace_idx -= 1\n        if non_brace_idx &lt;= 0 or (non_brace_idx == 3\n                                  and repr_.startswith('set')):\n            return repr_[:-1] + '...' + repr_[-1]\n        return repr_[:non_brace_idx + 1] + ',...' + repr_[non_brace_idx + 1:]\n\n    # We know it's non-iterable at this point.\n    if isinstance(obj, type):\n        return f'&lt;class {obj.__name__}&gt;'\n    if isinstance(obj, (int, float)):\n        return truncated_repr(format(obj, '.3e'), max_len)\n    return qualname(obj)\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.load_yaml","title":"<code>load_yaml(path, section=None)</code>","text":"<p>Load a yaml file. Useful for loading prompts.</p> <p>Borrowed from jabberwocky.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> required <code>section</code> <code>str or None</code> <p>I vaguely recall yaml files can define different subsections. This lets you return a specific one if you want. Usually leave as None which returns the whole contents.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def load_yaml(path, section=None):\n\"\"\"Load a yaml file. Useful for loading prompts.\n\n    Borrowed from jabberwocky.\n\n    Parameters\n    ----------\n    path : str or Path\n    section : str or None\n        I vaguely recall yaml files can define different subsections. This lets\n        you return a specific one if you want. Usually leave as None which\n        returns the whole contents.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    with open(path, 'r') as f:\n        data = yaml.load(f, Loader=yaml.FullLoader) or {}\n    return data.get(section, data)\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.update_yaml","title":"<code>update_yaml(path, delete_if_none=True, **kwargs)</code>","text":"<p>Update a yaml file with new values.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to yaml file to update. If it doesn't exist, it will be created. Any necessary intermediate directories will be created too.</p> required <code>delete_if_none</code> <code>bool</code> <p>If True, any k-v pairs in kwargs where v is None will be treated as an instruction to delete key k from the yaml file. If False, we will actually set <code>{field}: None</code> in the yaml file.</p> <code>True</code> <code>kwargs</code> <code>any</code> <p>Key-value pairs to update the yaml file with.</p> <code>{}</code> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def update_yaml(path, delete_if_none=True, **kwargs):\n\"\"\"Update a yaml file with new values.\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to yaml file to update. If it doesn't exist, it will be created.\n        Any necessary intermediate directories will be created too.\n    delete_if_none : bool\n        If True, any k-v pairs in kwargs where v is None will be treated as an\n        instruction to delete key k from the yaml file. If False, we will\n        actually set `{field}: None` in the yaml file.\n    kwargs : any\n        Key-value pairs to update the yaml file with.\n    \"\"\"\n    path = Path(path).expanduser()\n    os.makedirs(path.parent, exist_ok=True)\n    try:\n        data = load_yaml(path)\n    except FileNotFoundError:\n        data = {}\n    for k, v in kwargs.items():\n        if v is None and delete_if_none:\n            data.pop(k, None)\n        else:\n            data[k] = v\n    with open(path, 'w') as f:\n        yaml.dump(data, f)\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.extract_code","title":"<code>extract_code(text, join_multi=True, multi_prefix_template='\\n\\n# {i}\\n')</code>","text":"<p>Extract code snippet from a GPT response (e.g. from our <code>debug</code> chat prompt. See <code>Examples</code> for expected format.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <code>join_multi</code> <code>bool</code> <p>If multiple code snippets are found, we can either choose to join them into one string or return a list of strings. If the former, we prefix each snippet with <code>multi_prefix_template</code> to make it clearer where each new snippet starts.</p> <code>True</code> <code>multi_prefix_template</code> <code>str</code> <p>If join_multi=True and multiple code snippets are found, we prepend this to each code snippet before joining into a single string. It should accept a single parameter {i} which numbers each code snippet in the order they were found in <code>text</code> (1-indexed).</p> <code>'\\n\\n# {i}\\n'</code> <p>Returns:</p> Type Description <code>str or list</code> <p>Code snippet from <code>text</code>. If we find multiple snippets, we either join them into one big string (if join_multi is True) or return a list of strings otherwise.</p> <p>Examples:</p> <pre><code>text = '''Appending to a tuple is not allowed because tuples are immutable.\nHowever, in this code snippet, the tuple b contains two lists, and lists\nare mutable. Therefore, appending to b[1] (which is a list) does not raise\nan error. To fix this, you can either change b[1] to a tuple or create a\nnew tuple that contains the original elements of b and the new list.\n\n```python\n# Corrected code snippet\na = 3\nb = ([0, 1], [2, 3])\nb = (b[0], b[1] + [a])\n```'''\n\nprint(extract_code(text))\n\n# Extracted code snippet\n'''\na = 3\nb = ([0, 1], [2, 3])\nb = (b[0], b[1] + [a])\n'''\n</code></pre> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def extract_code(text, join_multi=True, multi_prefix_template='\\n\\n# {i}\\n'):\n\"\"\"Extract code snippet from a GPT response (e.g. from our `debug` chat\n    prompt. See `Examples` for expected format.\n\n    Parameters\n    ----------\n    text : str\n    join_multi : bool\n        If multiple code snippets are found, we can either choose to join them\n        into one string or return a list of strings. If the former, we prefix\n        each snippet with `multi_prefix_template` to make it clearer where\n        each new snippet starts.\n    multi_prefix_template : str\n        If join_multi=True and multiple code snippets are found, we prepend\n        this to each code snippet before joining into a single string. It\n        should accept a single parameter {i} which numbers each code snippet\n        in the order they were found in `text` (1-indexed).\n\n    Returns\n    -------\n    str or list\n        Code snippet from `text`. If we find multiple snippets, we either join\n        them into one big string (if join_multi is True) or return a\n        list of strings otherwise.\n\n    Examples\n    --------\n    ```plaintext\n    text = '''Appending to a tuple is not allowed because tuples are immutable.\n    However, in this code snippet, the tuple b contains two lists, and lists\n    are mutable. Therefore, appending to b[1] (which is a list) does not raise\n    an error. To fix this, you can either change b[1] to a tuple or create a\n    new tuple that contains the original elements of b and the new list.\n\n    ```python\n    # Corrected code snippet\n    a = 3\n    b = ([0, 1], [2, 3])\n    b = (b[0], b[1] + [a])\n    ```'''\n\n    print(extract_code(text))\n\n    # Extracted code snippet\n    '''\n    a = 3\n    b = ([0, 1], [2, 3])\n    b = (b[0], b[1] + [a])\n    '''\n    ```\n    \"\"\"\n    chunks = re.findall(\"(?s)```(?:python)?\\n(.*?)\\n```\", text)\n    if not join_multi:\n        return chunks\n    if len(chunks) &gt; 1:\n        chunks = [multi_prefix_template.format(i=i) + chunk\n                  for i, chunk in enumerate(chunks, 1)]\n        chunks[0] = chunks[0].lstrip()\n    return ''.join(chunks)\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.parse_completion","title":"<code>parse_completion(text)</code>","text":"<p>This function is called on the gpt completion text in roboduck.debug.DuckDB.ask_language_model (i.e. when the user asks a question during a debugging session, or when an error occurs when in auto-explain errors mode).</p> <p>Users can define their own custom function as a replacement (mostly useful when defining custom prompts too). The only requirements are that the function must take 1 string input and return a dict containing the keys \"explanation\" and \"code\", with an optional key \"extra\" that can be used to store any additional information (probably in a dict). For example, if you wrote a prompt that asked gpt to return valid json, you could potentially use json.loads() as your drop-in replacement (ignoring validation/error handling, which you might prefer to handle via a langchain chain anyway).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>GPT completion. This should contain both a natural language explanation and code.</p> required <p>Returns:</p> Type Description <code>dict[str]</code> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def parse_completion(text):\n\"\"\"This function is called on the gpt completion text in\n    roboduck.debug.DuckDB.ask_language_model (i.e. when the user asks a\n    question during a debugging session, or when an error occurs when in\n    auto-explain errors mode).\n\n    Users can define their own custom function as a replacement (mostly\n    useful when defining custom prompts too). The only requirements are that\n    the function must take 1 string input and return a dict containing the\n    keys \"explanation\" and \"code\", with an optional key \"extra\" that can be\n    used to store any additional information (probably in a dict). For example,\n    if you wrote a prompt that asked gpt to return valid json, you could\n    potentially use json.loads() as your drop-in replacement (ignoring\n    validation/error handling, which you might prefer to handle via a langchain\n    chain anyway).\n\n    Parameters\n    ----------\n    text : str\n        GPT completion. This should contain both a natural language explanation\n        and code.\n\n    Returns\n    -------\n    dict[str]\n    \"\"\"\n    # Keep an eye out for how this performs - considered going with a more\n    # complex regex or other approach here but since part 2 is supposed to be\n    # code only, maybe that's okay. Extract_code could get weird if gpt\n    # uses triple backticks in a function docstring but that should be very\n    # rare, and the instructions sort of discourage it.\n    explanation = text.partition('\\n```')[0]\n    code = extract_code(text)\n    return {'explanation': explanation,\n            'code': code}\n</code></pre>"},{"location":"utils/#lib.roboduck.utils.available_models","title":"<code>available_models()</code>","text":"<p>Show user available values for model_name parameter in debug.DuckDB class/ debug.duck function/errors.enable function etc.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Maps provider name (e.g. 'openai') to list of valid model_name values. Provider name should correspond to a langchain or roboduck class named like ChatOpenai (i.e. Chat{provider.title()}). Eventually would like to support other providers like anthropic but never got off API waitlist.</p> Source code in <code>lib/roboduck/utils.py</code> <pre><code>def available_models():\n\"\"\"Show user available values for model_name parameter in debug.DuckDB\n    class/ debug.duck function/errors.enable function etc.\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Maps provider name (e.g. 'openai') to list of valid\n        model_name values. Provider name should correspond to a langchain or\n        roboduck class named like ChatOpenai (i.e. Chat{provider.title()}).\n        Eventually would like to support other providers like anthropic but\n        never got off API waitlist.\n    \"\"\"\n    # Weirdly, env var is set and available but openai can't seem to find it\n    # unless we explicitly set it here.\n    openai.api_key = os.environ.get('OPENAI_API_KEY')\n    res = {}\n\n    # This logic may not always hold but as of April 2023, this returns\n    # openai's available chat models.\n    openai_res = openai.Model.list()\n    res['openai'] = [row['id'] for row in openai_res['data']\n                     if row['id'].startswith('gpt')]\n    return res\n</code></pre>"},{"location":"cli/cli/","title":"CLI","text":"<p>Command line tool that allows us to run files with explainable error mode enabled without changing the file itself (perhaps not that useful given that errors mode can be enabled with a single import, but just another option). This file needs to be stashed in roboduck/cli subdir to avoid circular import error caused by logging.py name collision with standard library.</p>"},{"location":"cli/cli/#lib.roboduck.cli.cli-functions","title":"Functions","text":""},{"location":"cli/cli/#lib.roboduck.cli.cli.make_import_statement","title":"<code>make_import_statement(cls_name)</code>","text":"<p>Given a class name like 'roboduck.debug.DuckDB', construct the import statement (str) that should likely be used to import that class (in this case 'from roboduck.debug import DuckDB'.</p> <p>Parameters:</p> Name Type Description Default <code>cls_name</code> <code>str</code> <p>Class name including module (essentially qualname?), e.g. roboduck.DuckDB. (Note that this would need to be roboduck.debug.DuckDB if we didn't include DuckDB in roboduck's init.py.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>E.g. \"from roboduck import DuckDB\"</p> Source code in <code>lib/roboduck/cli/cli.py</code> <pre><code>def make_import_statement(cls_name):\n\"\"\"Given a class name like 'roboduck.debug.DuckDB', construct the import\n    statement (str) that should likely be used to import that class (in this\n    case 'from roboduck.debug import DuckDB'.\n\n    Parameters\n    ----------\n    cls_name : str\n        Class name including module (essentially __qualname__?), e.g.\n        roboduck.DuckDB. (Note that this would need to be roboduck.debug.DuckDB\n        if we didn't include DuckDB in roboduck's __init__.py.)\n\n    Returns\n    -------\n    str\n        E.g. \"from roboduck import DuckDB\"\n    \"\"\"\n    parts = cls_name.split('.')\n    if len(parts) == 1:\n        return f'import {parts[0]}'\n    else:\n        lib = '.'.join(parts[:-1])\n        return f'from {lib} import {parts[-1]}'\n</code></pre>"},{"location":"cli/cli/#lib.roboduck.cli.cli.run","title":"<code>run()</code>","text":"<p>Execute a python script with auto error mode enabled.</p> <p>Run a python script with roboduck's errors mode automatically enabled.</p> <p>Examples:</p> <p>Make sure to include the equals sign between option name and value. If using a custom chat_class, the full name must be provided.</p> <pre><code>duck my_script.py\nduck my_script.py --chat_class=roboduck.DummyChatClass\nduck my_script.py --auto=True --prompt_name=~/my_custom_prompt.yaml\n</code></pre> Source code in <code>lib/roboduck/cli/cli.py</code> <pre><code>def run():\n\"\"\"Execute a python script with auto error mode enabled.\n\n    Run a python script with roboduck's errors mode\n    automatically enabled.\n\n    Examples\n    --------\n    Make sure to include the equals sign between option name and value.\n    If using a custom chat_class, the full name must be provided.\n\n    ```\n    duck my_script.py\n    duck my_script.py --chat_class=roboduck.DummyChatClass\n    duck my_script.py --auto=True --prompt_name=~/my_custom_prompt.yaml\n    ```\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=run.__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    parser.add_argument('file', help='The python script to execute.')\n    args, kwargs_ = parser.parse_known_args()\n    kwargs = {}\n    imports = []\n    for x in kwargs_:\n        if not (x.startswith('--') and '=' in x):\n            raise ValueError(f'Malformed command. Encountered {x!r} when '\n                             'parsing command but expected format like '\n                             '--key=val.')\n        k, v = x.split('=')\n        k = k.strip('--')\n        if k in ('cls', 'chat_class'):\n            imports.append(make_import_statement(v))\n            kwargs[k] = v.rpartition('.')[-1]\n        else:\n            kwargs[k] = ast.literal_eval(v)\n\n    # Grab source code and insert our imports and enable error mode.\n    path = Path(args.file).resolve()\n    tmp_path = Path('/tmp')/path.name\n    with open(path, 'r') as f:\n        src_text = f.read()\n    new_text = 'from roboduck import errors\\n' + '\\n'.join(imports) + '\\n'\n    if kwargs:\n        kwargs_str = ''\n        for k, v in kwargs.items():\n            kwargs_str += f'{k}='\n            if k in ('cls', 'chat_class'):\n                kwargs_str += v\n            else:\n                kwargs_str += repr(v)\n            kwargs_str += ', '\n        new_text += f'errors.enable(**dict({kwargs_str}))\\n'\n    modified_text = new_text + src_text\n\n    # Create copy file with imports and errors enabled, try to execute it, then\n    # restore original file. Keep all file renaming and writing inside try\n    # block to avoid confusion caused by premature sigints.\n    try:\n        path.rename(tmp_path)\n        with open(path, 'w') as f:\n            f.write(modified_text)\n        subprocess.call(['python', str(path)])\n    except Exception as e:\n        raise e\n    # At one point I observed an error where a sigint during error explanation\n    # resulted in tmp_path not being found and the file at the user-specified\n    # path was left with the roboduck errors import. I believe moving the\n    # `path.rename` step inside the try block fixed that but to be safe,\n    # we add some additional error handling.\n    finally:\n        if tmp_path.is_file():\n            tmp_path.rename(path)\n        else:\n            with open(path, 'w') as f:\n                f.write(src_text)\n</code></pre>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"debug/","title":"Debug","text":"<p>A conversational debugger and drop-in replacement for pdb. Python's default interactive debugging session is already a crude conversation with your program or interpreter, in a sense - this just lets your program communicate to you more effectively.</p>"},{"location":"debug/#lib.roboduck.debug--quickstart","title":"Quickstart","text":"<pre><code># Our replacement for python's `breakpoint`.\nfrom roboduck.debug import duck\n\n# Broken version of bubble sort. Notice the duck() call on the second to last\n# line.\ndef bubble_sort(nums):\n    for i in range(len(nums)):\n        for j in range(len(nums)):\n            if nums[j] &gt; nums[j + 1]:\n                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n                duck()\n    return nums\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.CodeCompletionCache","title":"<code>CodeCompletionCache</code>","text":"<p>Just stores the last completion from DuckDB in a way that our <code>duck</code> jupyter magic can access (without relying on global variable, though not sure if this is meaningfully different). The magic only needs to access it in insert mode (-i flag) to insert the fixed code snippet into a new code cell.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>@store_class_defaults(attr_filter=lambda x: x.startswith('last_'))\nclass CodeCompletionCache:\n\"\"\"Just stores the last completion from DuckDB in a way that our\n    `duck` jupyter magic can access (without relying on global variable, though\n    not sure if this is meaningfully different). The magic only needs to access\n    it in insert mode (-i flag) to insert the fixed code snippet into a new\n    code cell.\n    \"\"\"\nlast_completion = ''\nlast_explanation = ''\nlast_code = ''\nlast_new_code = ''\nlast_code_diff = ''\nlast_extra = {}\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB","title":"<code>DuckDB</code>","text":"<p>         Bases: <code>Pdb</code></p> <p>Conversational debugger powered by gpt models (currently codex, possibly eventually chatGPT). Once you're in a debugging session, any user command containing a question mark will be interpreted as a question for gpt. Prefixing your question with \"[dev]\" will print out the full prompt before making the query.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>class DuckDB(Pdb):\n\"\"\"Conversational debugger powered by gpt models (currently codex, possibly\n    eventually chatGPT). Once you're in a debugging session, any user command\n    containing a question mark will be interpreted as a question for gpt.\n    Prefixing your question with \"[dev]\" will print out the full prompt before\n    making the query.\n    \"\"\"\ndef __init__(self, prompt_name='debug', max_len_per_var=79, silent=False,\npdb_kwargs=None, parse_func=parse_completion, color='green',\n**chat_kwargs):\n\"\"\"\n        Parameters\n        ----------\n        prompt_name: str\n            Name of prompt template to use when querying chatGPT. Roboduck\n            currently provides several builtin options\n            (see roboduck.prompts.chat):\n                debug - for interactive debugging sessions on the relevant\n                    snippet of code.\n                debug_full - for interactive debugging sessions on the whole\n                    notebook (no difference from \"debug\" for scripts). Risks\n                    creating a context that is too long.\n                debug_stack_trace - for automatic error explanations or\n                    logging.\n            Alternatively, can also define your own template in a yaml file\n            mimicking the format of the builtin templates and pass in the\n            path to that file as a string.\n        max_len_per_var: int\n            Limits number of characters per variable when communicating\n            current state (local or global depending on `full_context`) to\n            gpt. If unbounded, that section of the prompt alone could grow\n            very big . I somewhat arbitrarily set 79 as the default, i.e.\n            1 line of python per variable. I figure that's usually enough to\n            communicate the gist of what's happening.\n        silent: bool\n            If True, print gpt completions to stdout. One example of when False\n            is appropriate is our logging module - we want to get the\n            explanation and update the exception message which then gets\n            logged, but we don't care about typing results in real time.\n        pdb_kwargs: dict or None\n            Additional kwargs for base Pdb class.\n        parse_func: function\n            This will be called on the generated text each time gpt provides a\n            completion. It returns a dictionary whose values will be stored\n            in CodeCompletionCache in this module. See the default function's\n            docstring for guidance on writing a custom function.\n        color: str\n            Color to print gpt completions in. Sometimes we want to change this\n            to red, such as in the errors module, to make it clearer that an\n            error occurred.\n        chat_kwargs: any\n            Additional kwargs to configure our Chat class (passed to\n            its `from_config` factory). Common example would be setting\n            `chat_class=roboduck.langchain.chat.DummyChatModel`\n            which mocks api calls (good for development, saves money).\n        \"\"\"\nsuper().__init__(**pdb_kwargs or {})\nself.prompt = '&gt;&gt;&gt; '\nself.duck_prompt = '[Duck] '\nself.query_kwargs = {}\nchat_kwargs['name'] = prompt_name\nif silent:\nchat_kwargs['streaming'] = False\nelse:\nchat_kwargs['streaming'] = True\nchat_kwargs['callback_manager'] = CallbackManager(\n[LiveTypingCallbackHandler(color=color)]\n)\n# Dev color is what we print the prompt in when user asks a question\n# in dev mode.\nself.color = color\nself.dev_color = 'blue' if self.color == 'red' else 'red'\n# Must create self.chat before setting _chat_prompt_keys,\n# and full_context after both of those.\nself.chat = Chat.from_config(**chat_kwargs)\nself.default_user_key, self.backup_user_key = self._chat_prompt_keys()\nself.full_context = 'full_code' in self.field_names()\nself.prompt_name = prompt_name\nself.repr_func = partial(truncated_repr, max_len=max_len_per_var)\nself.silent = silent\nself.parse_func = parse_func\n# This gets updated every time the user asks a question.\nself.prev_kwargs_hash = None\ndef _chat_prompt_keys(self):\n\"\"\"Retrieve default and backup user reply prompt keys (names) from\n        self.chat object. If the prompt template has only one reply type,\n        the backup key will equal the default key.\n        \"\"\"\nkeys = list(self.chat.user_templates)\ndefault = keys[0]\nbackup = default\nif len(keys) &gt; 1:\nbackup = keys[1]\nif len(keys) &gt; 2:\nwarnings.warn(\n'You\\'re using a chat prompt template with &gt;2 types or '\n'user replies. This is not recommended because it\\'s '\n'not clear how to determine which reply type to use. We '\n'arbitrarily choose the first non-default key as the '\nf'backup reply type (\"{backup}\").'\n)\nreturn default, backup\ndef field_names(self, key=''):\n\"\"\"Get names of variables that are expected to be passed into default\n        user prompt template.\n        Returns\n        -------\n        set[str]\n        \"\"\"\nreturn self.chat.input_variables(key)\ndef _get_next_line(self, code_snippet):\n\"\"\"Retrieve next line of code that will be executed. Must call this\n        before we remove the duck() call.\n        Parameters\n        ----------\n        code_snippet: str\n        \"\"\"\nlines = code_snippet.splitlines()\nmax_idx = len(lines) - 1\n# Adjust f_lineno because it's 1 - indexed by default.\n# Set default next_line in case we don't find any valid line.\nline_no = self.curframe.f_lineno - 1\nnext_line = ''\nwhile line_no &lt;= max_idx:\nif lines[line_no].strip().startswith('duck('):\nline_no += 1\nelse:\nnext_line = lines[line_no]\nbreak\nreturn next_line\ndef _get_prompt_kwargs(self):\n\"\"\"Construct a dictionary describing the current state of our code\n        (variable names and values, source code, file type). This will be\n        passed to our langchain chat.reply() method to fill in the debug prompt\n        template.\n        Returns\n        -------\n        dict: contains keys 'code', 'local_vars', 'global_vars', 'file_type'.\n        If we specified full_context=True on init, we also include the key\n        'full_code'.\n        \"\"\"\nres = {}\n# Get current code snippet.\n# Fails when running code from cmd line like:\n# 'python -c \"print(x)\"'.\n# Haven't been able to find a way around this yet.\ntry:\n# Find next line before removing duck call to avoid messing up our\n# index.\ncode_snippet = inspect.getsource(self.curframe)\nres['next_line'] = self._get_next_line(code_snippet)\nres['code'] = self._remove_debugger_call(code_snippet)\nexcept OSError as err:\nself.error(err)\nres['local_vars'] = type_annotated_dict_str(\n{k: v for k, v in self.curframe_locals.items()\nif not is_ipy_name(k)},\nself.repr_func\n)\n# Get full source code if necessary.\nif self.full_context:\n# File is a string, either a file name or something like\n# &lt;ipython-input-50-e97ed612f523&gt;.\nfile = inspect.getsourcefile(self.curframe.f_code)\nif file.startswith('&lt;ipython'):\n# If we're in ipython, ipynbname.path() throws a\n# FileNotFoundError.\ntry:\nfull_code = load_ipynb(ipynbname.path())\nres['file_type'] = 'jupyter notebook'\nexcept FileNotFoundError:\n# TODO: maybe ipython session needs to use a modified\n# version of this func regardless of self.full_context,\n# and should return full code as list initially and\n# override res['code'] with last executed cell. Otherwise\n# I think getsource(curframe) may load a lot more code than\n# we usually want in ipython session.\nfull_code = load_current_ipython_session()\nres['file_type'] = 'ipython session'\nelse:\nwith open(file, 'r') as f:\nfull_code = f.read()\nres['file_type'] = 'python script'\nres['full_code'] = self._remove_debugger_call(full_code)\nused_tokens = set(res['full_code'].split())\nelse:\n# This is intentionally different from the used_tokens line in the\n# if clause - we only want to consider local code here.\nused_tokens = set(res['code'].split())\n# Namespace is often polluted with lots of unused globals (htools is\n# very much guilty of this \ud83d\ude2c) and we don't want to clutter up the\n# prompt with these.\nres['global_vars'] = type_annotated_dict_str(\n{k: v for k, v in self.curframe.f_globals.items()\nif k in used_tokens and not is_ipy_name(k)},\nself.repr_func\n)\nreturn res\n@staticmethod\ndef _remove_debugger_call(code_str):\n\"\"\"Remove `duck` function call (our equivalent of `breakpoint` from\n        source code string. Including it introduces a slight risk that gpt\n        will fixate on this mistery function as a potential bug cause.\n        \"\"\"\nreturn '\\n'.join(line for line in code_str.splitlines()\nif not line.strip().startswith('duck('))\ndef onecmd(self, line):\n\"\"\"Base class describes this as follows:\n        Interpret the argument as though it had been typed in response to the\n        prompt. Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n        We add an extra check in the if block to check if the user asked a\n        question. If so, we ask gpt. If not, we treat it as a regular pdb\n        command.\n        Parameters\n        ----------\n        line: str or tuple\n            If str, this is a regular line like in the standard debugger.\n            If tuple, this contains (line str, stack trace str - see\n            roboduck.errors.post_mortem for the actual insertion into the\n            cmdqueue). This is for use with the debug_stack_trace mode.\n        \"\"\"\nif isinstance(line, tuple):\nline, stack_trace = line\nelse:\nstack_trace = ''\nif not self.commands_defining:\nif '?' in line:\nreturn self.ask_language_model(\nline,\nstack_trace=stack_trace,\nverbose=line.startswith('[dev]')\n)\nreturn cmd.Cmd.onecmd(self, line)\nelse:\nreturn self.handle_command_def(line)\ndef ask_language_model(self, question, stack_trace='', verbose=False):\n\"\"\"When the user asks a question during a debugging session, query\n        gpt for the answer and type it back to them live.\n        Parameters\n        ----------\n        question : str\n            User question, e.g. \"Why are the first three values in nums equal\n            to 5 when the input list only had a single 5?\". (Example is from\n            a faulty bubble sort implementation.)\n        stack_trace : str\n            When using the \"debug_stack_trace\" prompt, we need to pass a\n            stack trace string into the prompt.\n        verbose : bool\n            If True, print the full gpt prompt in red before making the api\n            call. User activates this mode by prefixing their question with\n            '[dev]'. This overrides self.silent.\n        \"\"\"\n# Don't provide long context-laden prompt if nothing has changed since\n# the user's last question. This is often a followup/clarifying\n# question.\nprompt_kwargs = self._get_prompt_kwargs()\nkwargs_hash = hash(str(prompt_kwargs))\nif kwargs_hash == self.prev_kwargs_hash:\nprompt_kwargs.clear()\nprompt_key = self.backup_user_key\nelse:\nprompt_key = self.default_user_key\n# Perform surgery on kwargs depending on what fields are expected.\nfield_names = self.field_names(prompt_key)\nif 'question' in field_names:\nprompt_kwargs['question'] = question\nif stack_trace:\nprompt_kwargs['stack_trace'] = stack_trace\n# Validate that expected fields are present and provide interpretable\n# error message if not.\nkwargs_names = set(prompt_kwargs)\nonly_in_kwargs = kwargs_names - field_names\nonly_in_expected = field_names - kwargs_names\nerror_msg = 'If you are using a custom prompt, you may need to ' \\\n                    'subclass roboduck.debug.DuckDB and override the ' \\\n                    '_get_prompt_kwargs method.'\nif only_in_kwargs:\nraise RuntimeError(\nf'Received unexpected kwarg(s): {only_in_kwargs}. {error_msg} '\n)\nif only_in_expected:\nraise RuntimeError(\nf'Missing required kwarg(s): {only_in_expected}. {error_msg}'\n)\nprompt = self.chat.user_message(key_=prompt_key,\n**prompt_kwargs).content\nif len(prompt.split()) &gt; 1_000:\nwarnings.warn(\n'Prompt is very long (&gt;1k words). You\\'re approaching a risky'\n' zone where your prompt + completion might exceed the max '\n'sequence length.'\n)\nif verbose:\nprint(colored(prompt, 'red'))\nif not self.silent:\nprint(colored(self.duck_prompt, self.color), end='')\n# The actual LLM call.\nres = self.chat.reply(**prompt_kwargs, key_=prompt_key)\nanswer = res.content.strip()\nif not answer:\nanswer = 'Sorry, I don\\'t know. Can you try ' \\\n                     'rephrasing your question?'\n# This is intentionally nested in if statement because if answer is\n# truthy, we will have already printed it via our callback if not\n# in silent mode.\nif not self.silent:\nprint(colored(answer, self.color))\nparsed_kwargs = self.parse_func(answer)\n# When using the `duck` jupyter magic in \"insert\" mode, we reference\n# the CodeCompletionCache to populate the new code cell.\nCodeCompletionCache.last_completion = answer\nCodeCompletionCache.last_explanation = parsed_kwargs['explanation']\n# TODO: maybe check if code or full_code is more appropriate to store\n# as last_code, either depending on self.full_context or by doing a\n# quick str similarity to each.\n# Contextless prompt has no `code` key.\nold_code = prompt_kwargs.get('code', '')\nnew_code = parsed_kwargs['code']\nCodeCompletionCache.last_code_diff = colordiff_new_str(old_code,\nnew_code)\nCodeCompletionCache.last_code = old_code\nCodeCompletionCache.last_new_code = new_code\nCodeCompletionCache.last_extra = parsed_kwargs.get('extra', {})\nself.prev_kwargs_hash = kwargs_hash\ndef precmd(self, line):\n\"\"\"We need to define this to make our errors module work. Our\n        post_mortem function sometimes places a tuple in our debugger's\n        cmdqueue and precmd is called as part of the default cmdloop method.\n        Technically it calls postcmd too but we don't need to override that\n        because it does nothing with its line argument.\n        \"\"\"\nif isinstance(line, tuple):\nline, trace = line\nreturn super().precmd(line), trace\nreturn super().precmd(line)\ndef print_stack_entry(self, frame_lineno, prompt_prefix='\\n-&gt; '):\n\"\"\"This is called automatically when entering a debugger session\n        and it prints a message to stdout like\n        ```\n        &gt; &lt;ipython-input-20-9c67d40d0f93&gt;(2)&lt;module&gt;()\n        -&gt; print + 6\n        ```\n        In silent mode (like when using the roboduck logger with stdout=False),\n        we want to disable that message.\n        \"\"\"\nif self.silent:\nreturn\nframe, lineno = frame_lineno\nif frame is self.curframe:\nprefix = '&gt; '\nelse:\nprefix = '  '\nself.message(prefix +\nself.format_stack_entry(frame_lineno, prompt_prefix))\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.__init__","title":"<code>__init__(prompt_name='debug', max_len_per_var=79, silent=False, pdb_kwargs=None, parse_func=parse_completion, color='green', **chat_kwargs)</code>","text":""},{"location":"debug/#lib.roboduck.debug.DuckDB.__init__--parameters","title":"Parameters","text":"str <p>Name of prompt template to use when querying chatGPT. Roboduck currently provides several builtin options (see roboduck.prompts.chat):     debug - for interactive debugging sessions on the relevant         snippet of code.     debug_full - for interactive debugging sessions on the whole         notebook (no difference from \"debug\" for scripts). Risks         creating a context that is too long.     debug_stack_trace - for automatic error explanations or         logging. Alternatively, can also define your own template in a yaml file mimicking the format of the builtin templates and pass in the path to that file as a string.</p> int <p>Limits number of characters per variable when communicating current state (local or global depending on <code>full_context</code>) to gpt. If unbounded, that section of the prompt alone could grow very big . I somewhat arbitrarily set 79 as the default, i.e. 1 line of python per variable. I figure that's usually enough to communicate the gist of what's happening.</p> bool <p>If True, print gpt completions to stdout. One example of when False is appropriate is our logging module - we want to get the explanation and update the exception message which then gets logged, but we don't care about typing results in real time.</p> dict or None <p>Additional kwargs for base Pdb class.</p> function <p>This will be called on the generated text each time gpt provides a completion. It returns a dictionary whose values will be stored in CodeCompletionCache in this module. See the default function's docstring for guidance on writing a custom function.</p> str <p>Color to print gpt completions in. Sometimes we want to change this to red, such as in the errors module, to make it clearer that an error occurred.</p> any <p>Additional kwargs to configure our Chat class (passed to its <code>from_config</code> factory). Common example would be setting <code>chat_class=roboduck.langchain.chat.DummyChatModel</code> which mocks api calls (good for development, saves money).</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def __init__(self, prompt_name='debug', max_len_per_var=79, silent=False,\npdb_kwargs=None, parse_func=parse_completion, color='green',\n**chat_kwargs):\n\"\"\"\n    Parameters\n    ----------\n    prompt_name: str\n        Name of prompt template to use when querying chatGPT. Roboduck\n        currently provides several builtin options\n        (see roboduck.prompts.chat):\n            debug - for interactive debugging sessions on the relevant\n                snippet of code.\n            debug_full - for interactive debugging sessions on the whole\n                notebook (no difference from \"debug\" for scripts). Risks\n                creating a context that is too long.\n            debug_stack_trace - for automatic error explanations or\n                logging.\n        Alternatively, can also define your own template in a yaml file\n        mimicking the format of the builtin templates and pass in the\n        path to that file as a string.\n    max_len_per_var: int\n        Limits number of characters per variable when communicating\n        current state (local or global depending on `full_context`) to\n        gpt. If unbounded, that section of the prompt alone could grow\n        very big . I somewhat arbitrarily set 79 as the default, i.e.\n        1 line of python per variable. I figure that's usually enough to\n        communicate the gist of what's happening.\n    silent: bool\n        If True, print gpt completions to stdout. One example of when False\n        is appropriate is our logging module - we want to get the\n        explanation and update the exception message which then gets\n        logged, but we don't care about typing results in real time.\n    pdb_kwargs: dict or None\n        Additional kwargs for base Pdb class.\n    parse_func: function\n        This will be called on the generated text each time gpt provides a\n        completion. It returns a dictionary whose values will be stored\n        in CodeCompletionCache in this module. See the default function's\n        docstring for guidance on writing a custom function.\n    color: str\n        Color to print gpt completions in. Sometimes we want to change this\n        to red, such as in the errors module, to make it clearer that an\n        error occurred.\n    chat_kwargs: any\n        Additional kwargs to configure our Chat class (passed to\n        its `from_config` factory). Common example would be setting\n        `chat_class=roboduck.langchain.chat.DummyChatModel`\n        which mocks api calls (good for development, saves money).\n    \"\"\"\nsuper().__init__(**pdb_kwargs or {})\nself.prompt = '&gt;&gt;&gt; '\nself.duck_prompt = '[Duck] '\nself.query_kwargs = {}\nchat_kwargs['name'] = prompt_name\nif silent:\nchat_kwargs['streaming'] = False\nelse:\nchat_kwargs['streaming'] = True\nchat_kwargs['callback_manager'] = CallbackManager(\n[LiveTypingCallbackHandler(color=color)]\n)\n# Dev color is what we print the prompt in when user asks a question\n# in dev mode.\nself.color = color\nself.dev_color = 'blue' if self.color == 'red' else 'red'\n# Must create self.chat before setting _chat_prompt_keys,\n# and full_context after both of those.\nself.chat = Chat.from_config(**chat_kwargs)\nself.default_user_key, self.backup_user_key = self._chat_prompt_keys()\nself.full_context = 'full_code' in self.field_names()\nself.prompt_name = prompt_name\nself.repr_func = partial(truncated_repr, max_len=max_len_per_var)\nself.silent = silent\nself.parse_func = parse_func\n# This gets updated every time the user asks a question.\nself.prev_kwargs_hash = None\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.ask_language_model","title":"<code>ask_language_model(question, stack_trace='', verbose=False)</code>","text":"<p>When the user asks a question during a debugging session, query gpt for the answer and type it back to them live.</p>"},{"location":"debug/#lib.roboduck.debug.DuckDB.ask_language_model--parameters","title":"Parameters","text":"str <p>User question, e.g. \"Why are the first three values in nums equal to 5 when the input list only had a single 5?\". (Example is from a faulty bubble sort implementation.)</p> str <p>When using the \"debug_stack_trace\" prompt, we need to pass a stack trace string into the prompt.</p> bool <p>If True, print the full gpt prompt in red before making the api call. User activates this mode by prefixing their question with '[dev]'. This overrides self.silent.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def ask_language_model(self, question, stack_trace='', verbose=False):\n\"\"\"When the user asks a question during a debugging session, query\n    gpt for the answer and type it back to them live.\n    Parameters\n    ----------\n    question : str\n        User question, e.g. \"Why are the first three values in nums equal\n        to 5 when the input list only had a single 5?\". (Example is from\n        a faulty bubble sort implementation.)\n    stack_trace : str\n        When using the \"debug_stack_trace\" prompt, we need to pass a\n        stack trace string into the prompt.\n    verbose : bool\n        If True, print the full gpt prompt in red before making the api\n        call. User activates this mode by prefixing their question with\n        '[dev]'. This overrides self.silent.\n    \"\"\"\n# Don't provide long context-laden prompt if nothing has changed since\n# the user's last question. This is often a followup/clarifying\n# question.\nprompt_kwargs = self._get_prompt_kwargs()\nkwargs_hash = hash(str(prompt_kwargs))\nif kwargs_hash == self.prev_kwargs_hash:\nprompt_kwargs.clear()\nprompt_key = self.backup_user_key\nelse:\nprompt_key = self.default_user_key\n# Perform surgery on kwargs depending on what fields are expected.\nfield_names = self.field_names(prompt_key)\nif 'question' in field_names:\nprompt_kwargs['question'] = question\nif stack_trace:\nprompt_kwargs['stack_trace'] = stack_trace\n# Validate that expected fields are present and provide interpretable\n# error message if not.\nkwargs_names = set(prompt_kwargs)\nonly_in_kwargs = kwargs_names - field_names\nonly_in_expected = field_names - kwargs_names\nerror_msg = 'If you are using a custom prompt, you may need to ' \\\n                'subclass roboduck.debug.DuckDB and override the ' \\\n                '_get_prompt_kwargs method.'\nif only_in_kwargs:\nraise RuntimeError(\nf'Received unexpected kwarg(s): {only_in_kwargs}. {error_msg} '\n)\nif only_in_expected:\nraise RuntimeError(\nf'Missing required kwarg(s): {only_in_expected}. {error_msg}'\n)\nprompt = self.chat.user_message(key_=prompt_key,\n**prompt_kwargs).content\nif len(prompt.split()) &gt; 1_000:\nwarnings.warn(\n'Prompt is very long (&gt;1k words). You\\'re approaching a risky'\n' zone where your prompt + completion might exceed the max '\n'sequence length.'\n)\nif verbose:\nprint(colored(prompt, 'red'))\nif not self.silent:\nprint(colored(self.duck_prompt, self.color), end='')\n# The actual LLM call.\nres = self.chat.reply(**prompt_kwargs, key_=prompt_key)\nanswer = res.content.strip()\nif not answer:\nanswer = 'Sorry, I don\\'t know. Can you try ' \\\n                 'rephrasing your question?'\n# This is intentionally nested in if statement because if answer is\n# truthy, we will have already printed it via our callback if not\n# in silent mode.\nif not self.silent:\nprint(colored(answer, self.color))\nparsed_kwargs = self.parse_func(answer)\n# When using the `duck` jupyter magic in \"insert\" mode, we reference\n# the CodeCompletionCache to populate the new code cell.\nCodeCompletionCache.last_completion = answer\nCodeCompletionCache.last_explanation = parsed_kwargs['explanation']\n# TODO: maybe check if code or full_code is more appropriate to store\n# as last_code, either depending on self.full_context or by doing a\n# quick str similarity to each.\n# Contextless prompt has no `code` key.\nold_code = prompt_kwargs.get('code', '')\nnew_code = parsed_kwargs['code']\nCodeCompletionCache.last_code_diff = colordiff_new_str(old_code,\nnew_code)\nCodeCompletionCache.last_code = old_code\nCodeCompletionCache.last_new_code = new_code\nCodeCompletionCache.last_extra = parsed_kwargs.get('extra', {})\nself.prev_kwargs_hash = kwargs_hash\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.field_names","title":"<code>field_names(key='')</code>","text":"<p>Get names of variables that are expected to be passed into default user prompt template.</p>"},{"location":"debug/#lib.roboduck.debug.DuckDB.field_names--returns","title":"Returns","text":"<p>set[str]</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def field_names(self, key=''):\n\"\"\"Get names of variables that are expected to be passed into default\n    user prompt template.\n    Returns\n    -------\n    set[str]\n    \"\"\"\nreturn self.chat.input_variables(key)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.onecmd","title":"<code>onecmd(line)</code>","text":"<p>Base class describes this as follows:</p> <p>Interpret the argument as though it had been typed in response to the prompt. Checks whether this line is typed at the normal prompt or in a breakpoint command list definition.</p> <p>We add an extra check in the if block to check if the user asked a question. If so, we ask gpt. If not, we treat it as a regular pdb command.</p>"},{"location":"debug/#lib.roboduck.debug.DuckDB.onecmd--parameters","title":"Parameters","text":"str or tuple <p>If str, this is a regular line like in the standard debugger. If tuple, this contains (line str, stack trace str - see roboduck.errors.post_mortem for the actual insertion into the cmdqueue). This is for use with the debug_stack_trace mode.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def onecmd(self, line):\n\"\"\"Base class describes this as follows:\n    Interpret the argument as though it had been typed in response to the\n    prompt. Checks whether this line is typed at the normal prompt or in\n    a breakpoint command list definition.\n    We add an extra check in the if block to check if the user asked a\n    question. If so, we ask gpt. If not, we treat it as a regular pdb\n    command.\n    Parameters\n    ----------\n    line: str or tuple\n        If str, this is a regular line like in the standard debugger.\n        If tuple, this contains (line str, stack trace str - see\n        roboduck.errors.post_mortem for the actual insertion into the\n        cmdqueue). This is for use with the debug_stack_trace mode.\n    \"\"\"\nif isinstance(line, tuple):\nline, stack_trace = line\nelse:\nstack_trace = ''\nif not self.commands_defining:\nif '?' in line:\nreturn self.ask_language_model(\nline,\nstack_trace=stack_trace,\nverbose=line.startswith('[dev]')\n)\nreturn cmd.Cmd.onecmd(self, line)\nelse:\nreturn self.handle_command_def(line)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.precmd","title":"<code>precmd(line)</code>","text":"<p>We need to define this to make our errors module work. Our post_mortem function sometimes places a tuple in our debugger's cmdqueue and precmd is called as part of the default cmdloop method. Technically it calls postcmd too but we don't need to override that because it does nothing with its line argument.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def precmd(self, line):\n\"\"\"We need to define this to make our errors module work. Our\n    post_mortem function sometimes places a tuple in our debugger's\n    cmdqueue and precmd is called as part of the default cmdloop method.\n    Technically it calls postcmd too but we don't need to override that\n    because it does nothing with its line argument.\n    \"\"\"\nif isinstance(line, tuple):\nline, trace = line\nreturn super().precmd(line), trace\nreturn super().precmd(line)\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.DuckDB.print_stack_entry","title":"<code>print_stack_entry(frame_lineno, prompt_prefix='\\n-&gt; ')</code>","text":"<p>This is called automatically when entering a debugger session and it prints a message to stdout like</p> <pre><code>&gt; &lt;ipython-input-20-9c67d40d0f93&gt;(2)&lt;module&gt;()\n-&gt; print + 6\n</code></pre> <p>In silent mode (like when using the roboduck logger with stdout=False), we want to disable that message.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>def print_stack_entry(self, frame_lineno, prompt_prefix='\\n-&gt; '):\n\"\"\"This is called automatically when entering a debugger session\n    and it prints a message to stdout like\n    ```\n    &gt; &lt;ipython-input-20-9c67d40d0f93&gt;(2)&lt;module&gt;()\n    -&gt; print + 6\n    ```\n    In silent mode (like when using the roboduck logger with stdout=False),\n    we want to disable that message.\n    \"\"\"\nif self.silent:\nreturn\nframe, lineno = frame_lineno\nif frame is self.curframe:\nprefix = '&gt; '\nelse:\nprefix = '  '\nself.message(prefix +\nself.format_stack_entry(frame_lineno, prompt_prefix))\n</code></pre>"},{"location":"debug/#lib.roboduck.debug.duck","title":"<code>duck(**kwargs)</code>","text":"<p>Roboduck equivalent of native python breakpoint(). The DuckDB docstring is below. Any kwargs passed in to this function will be passed to its constructor.</p> Source code in <code>lib/roboduck/debug.py</code> <pre><code>@add_docstring(DuckDB.__init__)\ndef duck(**kwargs):\n\"\"\"Roboduck equivalent of native python breakpoint().\n    The DuckDB docstring is below. Any kwargs passed in to this function\n    will be passed to its constructor.\n    \"\"\"\nDuckDB(**kwargs).set_trace(sys._getframe().f_back)\n</code></pre>"},{"location":"errors/","title":"Errors","text":"<p>Errors that explain themselves! Or more precisely, that are explained to you by a gpt-esque model. Simply importing this module will change python's default behavior when it encounters an error.</p>"},{"location":"errors/#lib.roboduck.errors--quickstart","title":"Quickstart","text":""},{"location":"errors/#lib.roboduck.errors--after-this-import-error-explanations-are-automatically-enabled","title":"After this import, error explanations are automatically enabled.","text":"<p>from roboduck import errors</p>"},{"location":"errors/#lib.roboduck.errors--go-back-to-pythons-regular-behavior-on-errors","title":"Go back to python's regular behavior on errors.","text":"<p>errors.disable()</p>"},{"location":"errors/#lib.roboduck.errors--you-can-use-enable-to-change-settings-or-manually-re-enable-gpt","title":"You can use <code>enable</code> to change settings or manually re-enable gpt","text":""},{"location":"errors/#lib.roboduck.errors--explanations-by-default-we-ask-the-user-if-they-want-an-explanation-after","title":"explanations. By default, we ask the user if they want an explanation after","text":""},{"location":"errors/#lib.roboduck.errors--each-error-yn-setting-autotrue-skips-this-step-and-always-explains","title":"each error (y/n). Setting auto=True skips this step and always explains","text":""},{"location":"errors/#lib.roboduck.errors--errors-not-recommended-in-most-cases-but-its-an-option","title":"errors (not recommended in most cases, but it's an option).","text":"<p>errors.enable(auto=True)</p>"},{"location":"errors/#lib.roboduck.errors.disable","title":"<code>disable()</code>","text":"<p>Revert to default behavior when exceptions are thrown.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def disable():\n\"\"\"Revert to default behavior when exceptions are thrown.\n    \"\"\"\nsys.excepthook = default_excepthook\ntry:\n# Tried doing `ipy.set_custom_exc((Exception,), None)` as suggested by\n# stackoverflow and chatgpt but it didn't quite restore the default\n# behavior. Manually remove this instead. I'm assuming only one custom\n# exception handler can be assigned for any one exception type and that\n# if we call disable(), we wish to remove the handler for Exception.\nipy.custom_exceptions = tuple(x for x in ipy.custom_exceptions\nif x != Exception)\nexcept AttributeError:\npass\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.enable","title":"<code>enable(**kwargs)</code>","text":"<p>Enable conversational debugging mode. This is called automatically on module import. However, users may wish to make changes, e.g. set auto=True or pass in a custom debugger cls, and this function makes that possible.</p>"},{"location":"errors/#lib.roboduck.errors.enable--parameters","title":"Parameters","text":"any <p>auto (bool) - if True, automatically have gpt explain every error that     occurs. Mostly useful for logging in production. You almost     certainly want to keep this as the default of False for any     interactive development. cls (type) - the debugger class to use. prompt_name (str) - determines what prompt/prompt_name the custom     debugger uses, e.g. \"debug_stack_trace\" colordiff (bool) - if True, new code snippet will print new parts     in green. Or any other args that can be passed to our debugger cls.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def enable(**kwargs):\n\"\"\"Enable conversational debugging mode. This is called automatically on\n    module import. However, users may wish to make changes, e.g. set auto=True\n    or pass in a custom debugger cls, and this function makes that possible.\n    Parameters\n    ----------\n    kwargs: any\n        auto (bool) - if True, automatically have gpt explain every error that\n            occurs. Mostly useful for logging in production. You almost\n            certainly want to keep this as the default of False for any\n            interactive development.\n        cls (type) - the debugger class to use.\n        prompt_name (str) - determines what prompt/prompt_name the custom\n            debugger uses, e.g. \"debug_stack_trace\"\n        colordiff (bool) - if True, new code snippet will print new parts\n            in green.\n        Or any other args that can be passed to our debugger cls.\n    \"\"\"\nhook = partial(excepthook, **kwargs)\ndef ipy_excepthook(self, etype, evalue, tb, tb_offset):\n\"\"\"IPython doesn't use sys.excepthook. We have to handle this case\n        separately and make sure it expects the right argument names.\n        \"\"\"\nreturn hook(etype, evalue, tb)\n# Overwrite default error handling.\nsys.excepthook = hook\n# Only necessary/possible when in ipython.\ntry:\nipy.set_custom_exc((Exception,), ipy_excepthook)\nexcept AttributeError:\npass\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.excepthook","title":"<code>excepthook(etype, val, tb, prompt_name='debug_stack_trace', auto=False, cls=DuckDB, **kwargs)</code>","text":"<p>Replaces sys.excepthook when module is imported. When an error is thrown, the user is asked whether they want an explanation of what went wrong. If they enter 'y' or 'yes', it will query gpt for help. Unlike roboduck.debug.duck(), the user does not need to manually type a question, and we don't linger in the debugger - we just write gpt's explanation and exit.</p> <p>Disable by calling roboduck.errors.disable().</p> <p>Parameters are the same as the default sys.excepthook function. Kwargs are forwarded to our custom postmortem function.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def excepthook(etype, val, tb, prompt_name='debug_stack_trace',\nauto=False, cls=DuckDB, **kwargs):\n\"\"\"Replaces sys.excepthook when module is imported. When an error is\n    thrown, the user is asked whether they want an explanation of what went\n    wrong. If they enter 'y' or 'yes', it will query gpt for help. Unlike\n    roboduck.debug.duck(), the user does not need to manually type a\n    question, and we don't linger in the debugger - we just write gpt's\n    explanation and exit.\n    Disable by calling roboduck.errors.disable().\n    Parameters are the same as the default sys.excepthook function. Kwargs\n    are forwarded to our custom postmortem function.\n    \"\"\"\nsys.last_type, sys.last_value, sys.last_traceback = etype, val, tb\ntrace = print_exception(etype, val, tb)\nif not kwargs.get('silent', False):\nprint(trace)\nkwargs.update(prompt_name=prompt_name, trace=trace, t=tb, Pdb=cls)\nif auto:\nreturn post_mortem(**kwargs)\nwhile True:\ncmd = input('Explain error message? [y/n]\\n').lower().strip()\nif cmd in ('y', 'yes'):\nreturn post_mortem(**kwargs)\nif cmd in ('n', 'no'):\nreturn\nprint('Unrecognized command. Valid choices are \"y\" or \"n\".\\n')\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.post_mortem","title":"<code>post_mortem(t=None, Pdb=DuckDB, trace='', prompt_name='debug_stack_trace', colordiff=True, **kwargs)</code>","text":"<p>Drop-in replacement (hence the slightly odd arg order, where trace is required but third positionally) for pdb.post_mortem that allows us to get both the stack trace AND global/local vars from the program state right before an exception occurred.</p>"},{"location":"errors/#lib.roboduck.errors.post_mortem--parameters","title":"Parameters","text":"some kind of traceback type? <p>A holdover from the default post_mortem class, not actually sure what type this is but it doesn't really matter for our use.</p> type <p>Debugger class. Name is capitalized to provide consistent interface with default post_mortem function.</p> str <p>Stack trace formatted as a single string. Required - default value just helps us maintain a consistent interface with pdb.post_mortem.</p> str <p>The prompt name that will be passed to our debugger class. Usually should leave this as the default. We expect the name to contain 'debug' and will warn if it doesn't.</p> bool <p>If True, the new code snippet in the exception will print new parts in green.</p> any <p>Additional kwargs to pass to debugger class constructor. The docstring of the default class is included below for reference.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>@add_docstring(DuckDB.__init__)\ndef post_mortem(t=None, Pdb=DuckDB, trace='', prompt_name='debug_stack_trace',\ncolordiff=True, **kwargs):\n\"\"\"Drop-in replacement (hence the slightly odd arg order, where trace is\n    required but third positionally) for pdb.post_mortem that allows us to get\n    both the stack trace AND global/local vars from the program state right\n    before an exception occurred.\n    Parameters\n    ----------\n    t: some kind of traceback type?\n        A holdover from the default post_mortem class, not actually sure what\n        type this is but it doesn't really matter for our use.\n    Pdb: type\n        Debugger class. Name is capitalized to provide consistent interface\n        with default post_mortem function.\n    trace: str\n        Stack trace formatted as a single string. Required - default value\n        just helps us maintain a consistent interface with pdb.post_mortem.\n    prompt_name: str\n        The prompt name that will be passed to our debugger class. Usually\n        should leave this as the default. We expect the name to contain\n        'debug' and will warn if it doesn't.\n    colordiff: bool\n        If True, the new code snippet in the exception will print new\n        parts in green.\n    kwargs: any\n        Additional kwargs to pass to debugger class constructor. The docstring\n        of the default class is included below for reference.\n    \"\"\"\nif t is None:\nt = sys.exc_info()[2]\nassert t is not None, \"post_mortem outside of exception context\"\nif 'debug' not in prompt_name:\nwarnings.warn(\nf'You passed an unexpected prompt_name ({prompt_name}) to '\nf'post_mortem. Are you sure you didn\\'t mean to use '\nf'debug_stack_trace?'\n)\nassert trace, 'Trace passed to post_mortem should be truthy.'\n# This serves almost like a soft assert statement - if user defines some\n# custom debugger class and the question leaks through, gpt should\n# hopefully warn us.\ndummy_question = (\n'This is a fake question to ensure that our ask_language_model '\n'method gets called. Our debugger class should remove this from the '\n'prompt kwargs before calling gpt. If you can read this, can you '\n'indicate that in your response?'\n)\nkwargs['color'] = kwargs.get('color', 'red')\np = Pdb(prompt_name=prompt_name, **kwargs)\np.reset()\np.cmdqueue.insert(0, (dummy_question, trace))\np.cmdqueue.insert(1, 'q')\np.interaction(None, t)\n# Make gpt explanation available as part of last error message,\n# accessible via sys.last_value.\nlast_value = getattr(sys, 'last_value', None)\nif CodeCompletionCache.last_completion and last_value:\ncode_name = 'last_code_diff' if colordiff else 'last_new_code'\nlast_value.args = tuple(\narg if i else f'{arg}\\n\\n{CodeCompletionCache.last_explanation}'\nf'\\n\\n{getattr(CodeCompletionCache, code_name)}'\nfor i, arg in enumerate(last_value.args)\n)\n</code></pre>"},{"location":"errors/#lib.roboduck.errors.print_exception","title":"<code>print_exception(etype, value, tb, limit=None, file=None, chain=True)</code>","text":"<p>Replacement for traceback.print_exception() that returns the whole stack trace as a single string. Used in roboduck's custom excepthook to allow us to show the stack trace to gpt. The original function's docstring is below:</p> <p>Print exception up to 'limit' stack trace entries from 'tb' to 'file'.</p> <p>This differs from print_tb() in the following ways: (1) if traceback is not None, it prints a header \"Traceback (most recent call last):\"; (2) it prints the exception type and value after the stack trace; (3) if type is SyntaxError and value has the appropriate format, it prints the line where the syntax error occurred with a caret on the next line indicating the approximate position of the error.</p> Source code in <code>lib/roboduck/errors.py</code> <pre><code>def print_exception(etype, value, tb, limit=None, file=None, chain=True):\n\"\"\"Replacement for traceback.print_exception() that returns the\n    whole stack trace as a single string. Used in roboduck's custom excepthook\n    to allow us to show the stack trace to gpt. The original function's\n    docstring is below:\n    Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.\n    \"\"\"\n# format_exception has ignored etype for some time, and code such as cgitb\n# passes in bogus values as a result. For compatibility with such code we\n# ignore it here (rather than in the new TracebackException API).\nif file is None:\nfile = sys.stderr\ntrace = ''.join(\nTracebackException(type(value), value, tb, limit=limit)\n.format(chain=chain)\n)\nif file != sys.stderr:\nwith open(file, 'w') as f:\nf.write(trace)\nreturn trace\n</code></pre>"}]}
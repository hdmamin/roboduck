{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Determine how much work it would take to update jabberwocky to:\n",
    "\n",
    "- [x] support gpt 3.5 models\n",
    "- [x] support gpt 4 models\n",
    "- [x] support chatGPT\n",
    "- use ConversationManager for PDB subclass\n",
    "\n",
    "\"Support\" could mean gpt.query works, prompt_manager.query works, conversation_manager.query works, engine_map.get works, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:05.113549Z",
     "start_time": "2023-03-18T05:20:05.097317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:08.097102Z",
     "start_time": "2023-03-18T05:20:05.525459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from htools import *\n",
    "from jabberwocky.openai_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:08.160205Z",
     "start_time": "2023-03-18T05:20:08.100226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/cairina/roboduck\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support gpt 3.5 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text-davinci-003 is already considered a 3.5 model\n",
    "- context length is about double gpt3's, half gpt4's (ignoring the unreleased super long context)\n",
    "- really just \"gpt-3.5-turbo\", there are no non-davinci level models here (at least for text completion)\n",
    "- gpt-3.5-turbo is 1/10 the cost of the equivalent davinci model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:18.633928Z",
     "start_time": "2023-03-18T05:20:18.473426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptManager('debug_full', 'creative_writing_ideas', 'time_complexity', 'word2number', 'eli', 'explain_code', 'tldr', 'translate', 'short_dates', 'test_professional', 'summarize_conversation', 'analyze_writing', 'social_hypotheses', 'conversation_transcript', 'wiki_bio_cleanup', 'debug', 'conversation', 'simplify_ml', 'ml_abstract', 'debug_stack_trace', 'shortest', 'mma', 'punctuate_alexa', 'journal_entry_ideas', 'punctuate_transcription', 'extract_backend_slot', 'leading_scholars', 'extract_code', 'debate', 'punctuate', 'conversation_generalized', 'default', 'how_to', 'debug_duckling', 'emotion_markup_language', 'nytimes_article')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm = PromptManager(verbose=False)\n",
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:19.161027Z",
     "start_time": "2023-03-18T05:20:18.832057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:1903: UserWarning: You've chosen to load >=1 custom personas but you are using me=\"me\". Some custom personas expect you to set `me` to your name. Stop phrases may not work as intended if you do not override conv.me.\n",
      "  'You\\'ve chosen to load >=1 custom personas but you are using '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<jabberwocky.openai_utils.ConversationManager at 0x7fde19c20828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = ConversationManager()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:19.285022Z",
     "start_time": "2023-03-18T05:20:19.215435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBackend <current_name: openai>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:34:18.659494Z",
     "start_time": "2023-03-20T00:34:18.052952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "turbo = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:21.391975Z",
     "start_time": "2023-03-18T05:20:21.152003Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model \"gpt-3.5-turbo\" does not contain any of the recognized openai bases ['ada', 'babbage', 'curie', 'davinci'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9cd7b6e4aba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEngineMap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mturbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(cls, model, backend, infer, default, openai_passthrough, basify)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mengine_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_base_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mengine_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mopenai_base_engine\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m    705\u001b[0m                    if chunk in cls.bases]\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             raise ValueError(f'Model \"{model}\" does not contain any of the '\n\u001b[0m\u001b[1;32m    708\u001b[0m                              f'recognized openai bases {cls.bases}.')\n\u001b[1;32m    709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Model \"gpt-3.5-turbo\" does not contain any of the recognized openai bases ['ada', 'babbage', 'curie', 'davinci']."
     ]
    }
   ],
   "source": [
    "EngineMap.get(turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:34:20.110518Z",
     "start_time": "2023-03-20T00:34:20.073711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:660: UserWarning: Allowing model \"gpt-3.5-turbo\" to pass through because openai_passthrough=True. We trust you to make sure this is a valid model.\n",
      "  f'Allowing model \"{model}\" to pass through because '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying again w/ jabberwocky 3.0.0.\n",
    "EngineMap.get(turbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with passthrough=True, we still get an error currently bc turbo doesn't include any of the typical base names (e.g. curie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:20:23.004689Z",
     "start_time": "2023-03-18T05:20:22.953340Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model \"gpt-3.5-turbo\" does not contain any of the recognized openai bases ['ada', 'babbage', 'curie', 'davinci'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-388fcaa60271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEngineMap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mturbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenai_passthrough\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(cls, model, backend, infer, default, openai_passthrough, basify)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mengine_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_base_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mengine_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mopenai_base_engine\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m    705\u001b[0m                    if chunk in cls.bases]\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             raise ValueError(f'Model \"{model}\" does not contain any of the '\n\u001b[0m\u001b[1;32m    708\u001b[0m                              f'recognized openai bases {cls.bases}.')\n\u001b[1;32m    709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Model \"gpt-3.5-turbo\" does not contain any of the recognized openai bases ['ada', 'babbage', 'curie', 'davinci']."
     ]
    }
   ],
   "source": [
    "EngineMap.get(turbo, openai_passthrough=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:40:16.454484Z",
     "start_time": "2023-03-20T00:40:16.416762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got ValueError(Model \"gpt-3.5-turbo\" does not contain any of the recognized openai bases ['ada', 'babbage', 'curie', 'davinci'].).\n"
     ]
    }
   ],
   "source": [
    "with assert_raises(ValueError):\n",
    "    EngineMap.get(turbo, openai_passthrough=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- 3.5 not currently supported. Updates should be confined to EngineMap class (I think?) so not horrible but not super simple either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No ada/babbage/curie/davinci, just \"gpt-4\", \"gpt-4-32k\".\n",
    "- Context length is 2x gpt3.5, 4x gpt3. 32k version is 8x and 16x, respectively.\n",
    "- Won't be natively supported either (like the 3.5 models), but once I figure out how to support 1, the other change should be basically identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:34:47.980680Z",
     "start_time": "2023-03-20T00:34:47.877403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:660: UserWarning: Allowing model \"gpt-4\" to pass through because openai_passthrough=True. We trust you to make sure this is a valid model.\n",
      "  f'Allowing model \"{model}\" to pass through because '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpt-4'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying again w/ jabberwocky 3.0.0.\n",
    "EngineMap.get('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:39:53.272705Z",
     "start_time": "2023-03-20T00:39:53.232651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got ValueError(Model \"gpt-4\" does not contain any of the recognized openai bases ['ada', 'babbage', 'curie', 'davinci'].).\n"
     ]
    }
   ],
   "source": [
    "# Trying again w/ jabberwocky 3.0.0.\n",
    "with assert_raises(ValueError):\n",
    "    EngineMap.get('gpt-4', openai_passthrough=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uses different obj openai.ChatCompletion instead of openai.Completion\n",
    "- instead of `prompt=str`, it takes in `messages=list[dict]`.\n",
    "- messages are pretty similar to what I store in conv manager. System message are top level instructions (e.g. bio), user messages are from human, assistant messages are from gpt. (Roles are \"system\", \"user\", \"assistant\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:21:59.565416Z",
     "start_time": "2023-03-18T05:21:59.529182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': 'alan_turing',\n",
       " 'gender': 'M',\n",
       " 'img_path': '/Users/hmamin/jabberwocky/data/conversation_personas/alan_turing/profile.jpg',\n",
       " 'img_url': 'https://upload.wikimedia.org/wikipedia/commons/f/f5/Turing-statue-Bletchley_14.jpg',\n",
       " 'nationality': 'English'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.start_conversation('Alan Turing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:00.277238Z",
     "start_time": "2023-03-18T05:22:00.241341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.user_turn_window, cm.gpt3_turn_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:01.695919Z",
     "start_time": "2023-03-18T05:22:00.989602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 0, 'temperature': 0.7, 'max_tokens': 100, 'frequency_penalty': 0.5, 'stop': ['\\n\\nMe:', 'This is a conversation with'], 'prompt': 'The following is a transcript of a conversation with Alan Turing. Alan Mathison Turing (23 June 1912 - 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.\\n\\nMe: Explain the distinction between agentic AI and tool AI.\\n\\nAlan Turing:', 'meta': {'backend_name': 'openai', 'query_func': 'query_gpt3', 'datetime': 'Fri Mar 17 22:22:01 2023', 'version': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A tool AI is an idea which has been brought about by way of science and technology, and is now being used by our own day to enable people to do important and necessary things for society. A agentic AI is something which is brought about by ourselves and our own actions, and is now being used by us and others in the future.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, full = cm.query('Explain the distinction between agentic AI and tool AI.',\n",
    "         model=0)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:04.757381Z",
     "start_time": "2023-03-18T05:22:04.265770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 0, 'temperature': 0.7, 'max_tokens': 100, 'frequency_penalty': 0.5, 'stop': ['\\n\\nMe:', 'This is a conversation with'], 'prompt': 'The following is a transcript of a conversation with Alan Turing. Alan Mathison Turing (23 June 1912 - 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.\\n\\nMe: Explain the distinction between agentic AI and tool AI.\\n\\nAlan Turing: A tool AI is an idea which has been brought about by way of science and technology, and is now being used by our own day to enable people to do important and necessary things for society. A agentic AI is something which is brought about by ourselves and our own actions, and is now being used by us and others in the future.\\n\\nMe: Could an AI be both agentic and tool?\\n\\nAlan Turing:', 'meta': {'backend_name': 'openai', 'query_func': 'query_gpt3', 'datetime': 'Fri Mar 17 22:22:04 2023', 'version': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"No, an AI cannot be both agentic and tool. An AI is only toolful when I have set out to do a good work, and am now using my life's labour to make it a good one.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, full = cm.query('Could an AI be both agentic and tool?', model=0)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:06.028870Z",
     "start_time": "2023-03-18T05:22:05.438632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 0, 'temperature': 0.7, 'max_tokens': 100, 'frequency_penalty': 0.5, 'stop': ['\\n\\nMe:', 'This is a conversation with'], 'prompt': \"The following is a transcript of a conversation with Alan Turing. Alan Mathison Turing (23 June 1912 - 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.\\n\\nMe: Could an AI be both agentic and tool?\\n\\nAlan Turing: No, an AI cannot be both agentic and tool. An AI is only toolful when I have set out to do a good work, and am now using my life's labour to make it a good one.\\n\\nMe: What other types might we define to classify AIs?\\n\\nAlan Turing:\", 'meta': {'backend_name': 'openai', 'query_func': 'query_gpt3', 'datetime': 'Fri Mar 17 22:22:05 2023', 'version': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know, but they will be of three types: those who rely on intuition and understanding, those who rely on thought, and those who rely on technology.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, full = cm.query('What other types might we define to classify AIs?',\n",
    "                     model=0)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:08.838367Z",
     "start_time": "2023-03-18T05:22:08.801834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Explain the distinction between agentic AI and tool AI.',\n",
       " 'Could an AI be both agentic and tool?',\n",
       " 'What other types might we define to classify AIs?']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.user_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:09.892730Z",
     "start_time": "2023-03-18T05:22:09.857519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A tool AI is an idea which has been brought about by way of science and technology, and is now being used by our own day to enable people to do important and necessary things for society. A agentic AI is something which is brought about by ourselves and our own actions, and is now being used by us and others in the future.',\n",
       " \"No, an AI cannot be both agentic and tool. An AI is only toolful when I have set out to do a good work, and am now using my life's labour to make it a good one.\",\n",
       " \"I don't know, but they will be of three types: those who rely on intuition and understanding, those who rely on thought, and those who rely on technology.\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.gpt3_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:13.453757Z",
     "start_time": "2023-03-18T05:22:13.414819Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_messages(self, user_text='', do_full=True):\n",
    "    current_persona = self.current['persona']\n",
    "    if not current_persona:\n",
    "        raise RuntimeError('No persona loaded. Have you started a '\n",
    "                           'conversation?')\n",
    "\n",
    "    pretty_name = self.process_name(current_persona, inverse=True)\n",
    "    messages = [\n",
    "        {'role': 'system', \n",
    "         'content': f'{self.name2base[current_persona]}\\n\\n'}\n",
    "    ]\n",
    "    user_turns = list(self.user_turns)\n",
    "    if user_text:\n",
    "        user_turns.append(user_text)\n",
    "    gpt3_turns = self.gpt3_turns\n",
    "    if not do_full:\n",
    "        user_turns = self.user_turns[-self.user_turn_window:]\n",
    "        gpt3_turns = self.gpt3_turns[-self.gpt3_turn_window:]\n",
    "\n",
    "    if len(user_turns) - len(gpt3_turns) not in (0, 1):\n",
    "        raise RuntimeError(\n",
    "            f'Mismatched turn counts: user has {len(user_turns)} and gpt3'\n",
    "            f' has {len(gpt3_turns)} turns.'\n",
    "        )\n",
    "    user_turns = [{'role': 'user', 'content': f'{self.me}: {turn}'}\n",
    "                  for turn in user_turns]\n",
    "    \n",
    "    # Strip gpt3 turns to be safe since streaming mode only strips them\n",
    "    # once the full query completes, and GUI uses full_conversation\n",
    "    # property while query is still in progress.\n",
    "    gpt3_turns = [\n",
    "        {'role': 'assistant', 'content': f'{pretty_name}: {turn.strip()}'}\n",
    "        for turn in gpt3_turns\n",
    "    ]\n",
    "    ordered = [user_turns, gpt3_turns]\n",
    "    if len(gpt3_turns) == len(user_turns) and not do_full:\n",
    "        ordered = reversed(ordered)\n",
    "    interleaved = [turn for row in zip_longest(user_turns, gpt3_turns) \n",
    "                   for turn in row if turn]\n",
    "    messages.extend(interleaved)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:24:57.461327Z",
     "start_time": "2023-03-18T05:24:57.392095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'The following is a transcript of a conversation with Alan Turing. Alan Mathison Turing (23 June 1912 - 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.\\n\\n'},\n",
       " {'role': 'user',\n",
       "  'content': 'Me: Explain the distinction between agentic AI and tool AI.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Alan Turing: A tool AI is an idea which has been brought about by way of science and technology, and is now being used by our own day to enable people to do important and necessary things for society. A agentic AI is something which is brought about by ourselves and our own actions, and is now being used by us and others in the future.'},\n",
       " {'role': 'user', 'content': 'Me: Could an AI be both agentic and tool?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Alan Turing: No, an AI cannot be both agentic and tool. An AI is only toolful when I have set out to do a good work, and am now using my life's labour to make it a good one.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Me: What other types might we define to classify AIs?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Alan Turing: I don't know, but they will be of three types: those who rely on intuition and understanding, those who rely on thought, and those who rely on technology.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Me: In a similar vein, what 3 categories would you place human thinkers into?'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = get_messages(cm, \n",
    "                        user_text='In a similar vein, what 3 categories would you place human thinkers into?')\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:22:25.422279Z",
     "start_time": "2023-03-18T05:22:18.353566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\r\n",
      "Version: 0.27.2\r\n",
      "Summary: Python client library for the OpenAI API\r\n",
      "Home-page: https://github.com/openai/openai-python\r\n",
      "Author: OpenAI\r\n",
      "Author-email: support@openai.com\r\n",
      "License: \r\n",
      "Location: /Users/hmamin/anaconda3/lib/python3.7/site-packages\r\n",
      "Requires: aiohttp, requests, tqdm, typing-extensions\r\n",
      "Required-by: chronological, jabberwocky\r\n"
     ]
    }
   ],
   "source": [
    "# Was using 0.18.1. Had to upgrade to get chat functionality, now\n",
    "# 0.27.2. Had to restart notebook despite autoreload being enabled 🤔.\n",
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:25:44.778441Z",
     "start_time": "2023-03-18T05:25:41.382876Z"
    }
   },
   "outputs": [],
   "source": [
    "res = openai.ChatCompletion().create(model='gpt-3.5-turbo', messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:25:53.957466Z",
     "start_time": "2023-03-18T05:25:53.895494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.openai_object.OpenAIObject"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:25:46.003733Z",
     "start_time": "2023-03-18T05:25:45.956681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-6vJMT6o6mNSHmduWpaC6oeSiPx6UG at 0x7fde19f1aaf0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Alan Turing: I would say that human thinkers can be categorized into three groups: those who rely on intuition and creativity, those who rely on logical thinking and analysis, and those who rely on experimental and empirical methods. Of course, many individuals embody a combination of these categories, and it is the unique balance of these skills that make each person's thinking and problem-solving abilities distinct.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1679117141,\n",
       "  \"id\": \"chatcmpl-6vJMT6o6mNSHmduWpaC6oeSiPx6UG\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 77,\n",
       "    \"prompt_tokens\": 346,\n",
       "    \"total_tokens\": 423\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T05:26:18.293449Z",
     "start_time": "2023-03-18T05:26:18.250044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alan Turing: I would say that human thinkers can be categorized into three groups: those who rely on intuition and creativity, those who rely on logical thinking and analysis, and those who rely on experimental and empirical methods. Of course, many individuals embody a combination of these categories, and it is the unique balance of these skills that make each person's thinking and problem-solving abilities distinct.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- Not too hard to construct inputs needed for chatmodel.\n",
    "- Code might get a little ugly delegating to chatgpt or not depending on model name though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom roboduck persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:01:24.984727Z",
     "start_time": "2023-03-20T00:01:24.817152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload after creating new roboduck persona.\n",
    "cm = ConversationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:01:25.056533Z",
     "start_time": "2023-03-20T00:01:25.014101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a transcript of a conversation with Roboduck. RoboDuck is an incredibly effective AI programming assistant. He is friendly, helpful, and detail-oriented. RoboDuck has in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and has decades of experience helping Python programmers resolve their most challenging bugs.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.name2base['roboduck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:46:15.053922Z",
     "start_time": "2023-03-20T00:46:15.001921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use as user_prompt_template.format(**user_kwargs, question=question).\n",
    "user_prompt_template = load_prompt('debug', verbose=False)['prompt'].lstrip('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:46:19.404020Z",
     "start_time": "2023-03-20T00:46:19.369354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER KEY\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "{question}\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "{code}\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{local_vars}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{global_vars}\n",
      "\n",
      "SOLUTION PART 1:\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:48:34.572996Z",
     "start_time": "2023-03-20T00:48:34.517987Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "user_kwargs = {\n",
    "    'code': \"\"\"\n",
    "def bubble_sort(nums):\n",
    "    for i in range(len(nums)):\n",
    "        for j in range(len(nums)):\n",
    "            if nums[j] > nums[j + 1]:\n",
    "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
    "    return nums\n",
    "\"\"\".strip(),\n",
    "    'local_vars': \"\"\"\n",
    "{\n",
    "    'nums': [3, 4, 2, 1, 5, 9],   # type: list\n",
    "    'i': 0,   # type: int\n",
    "    'j': 4,   # type: int\n",
    "}\n",
    "\"\"\".strip(),\n",
    "'global_vars': \"\"\"\n",
    "{\n",
    "}\"\"\".strip()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:49:05.832810Z",
     "start_time": "2023-03-20T00:49:05.782273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER KEY\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "Why will this throw an index error soon?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "def bubble_sort(nums):\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(len(nums)):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
      "    return nums\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{\n",
      "    'nums': [3, 4, 2, 1, 5, 9],   # type: list\n",
      "    'i': 0,   # type: int\n",
      "    'j': 4,   # type: int\n",
      "}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{\n",
      "}\n",
      "\n",
      "SOLUTION PART 1:\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_template.format(**user_kwargs, question='Why will this throw an index error soon?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:07:02.109220Z",
     "start_time": "2023-03-20T00:07:02.050894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': 'roboduck',\n",
       " 'img_path': '/Users/hmamin/jabberwocky/data/conversation_personas_custom/roboduck/profile.jpg',\n",
       " 'gender': 'M',\n",
       " 'nationality': 'American'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.start_conversation('roboduck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:07:21.983292Z",
     "start_time": "2023-03-20T00:07:15.783062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'code-davinci-002', 'temperature': 0.7, 'max_tokens': 512, 'frequency_penalty': 0.5, 'stop': ['\\n\\nMe:', 'This is a conversation with'], 'prompt': \"The following is a transcript of a conversation with Roboduck. RoboDuck is an incredibly effective AI programming assistant. He is friendly, helpful, and detail-oriented. RoboDuck has in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and has decades of experience helping Python programmers resolve their most challenging bugs.\\n\\nMe: This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\\n\\nQUESTION:\\nWhy will this throw an index error soon?\\n\\nCURRENT CODE SNIPPET:\\ndef bubble_sort(nums):\\n    for i in range(len(nums)):\\n        for j in range(len(nums)):\\n            if nums[j] > nums[j + 1]:\\n                nums[j + 1], nums[j] = nums[j], nums[j + 1]\\n    return nums\\n\\nLOCAL VARIABLES:\\n{\\n    'nums': [3, 4, 2, 1, 5, 9],   # type: list\\n    'i': 0,   # type: int\\n    'j': 4,   # type: int\\n}\\n\\nGLOBAL VARIABLES:\\n{\\n}\\n\\nSOLUTION PART 1:\\n\\nRoboduck:\", 'meta': {'backend_name': 'openai', 'query_func': 'query_gpt3', 'datetime': 'Sun Mar 19 17:07:15 2023', 'version': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Haven't implemented support for turbo or chat yet.\n",
    "res, full = cm.query(\n",
    "    user_prompt_template.format(**user_kwargs, question='Why will this throw an index error soon?'), \n",
    "    model='code-davinci-002', max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:07:26.967476Z",
     "start_time": "2023-03-20T00:07:26.898121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like there could be a few issues with your code snippet.\n",
      "The first issue is that the code is not correct for bubble sort.\n",
      "\n",
      "SOLUTION PART 2:\n",
      "def bubble_sort(nums):\n",
      "    for i in range(len(nums)):   # type: int\n",
      "        for j in range(len(nums) - 1 - i):   # type: int\n",
      "            if nums[j] > nums[j + 1]:   # type: bool\n",
      "                nums[j + 1], nums[j] = nums[j], nums[j + 1]   # type: tuple, tuple, NoneType, NoneType, NoneType\n",
      "    return nums   # type: list\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:10:19.545010Z",
     "start_time": "2023-03-20T00:10:17.766085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'code-davinci-002', 'temperature': 0.7, 'max_tokens': 512, 'frequency_penalty': 0.5, 'stop': ['\\n\\nMe:', 'This is a conversation with'], 'prompt': \"The following is a transcript of a conversation with Roboduck. RoboDuck is an incredibly effective AI programming assistant. He is friendly, helpful, and detail-oriented. RoboDuck has in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and has decades of experience helping Python programmers resolve their most challenging bugs.\\n\\nMe: This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\\n\\nQUESTION:\\nWhy will this throw an index error soon?\\n\\nCURRENT CODE SNIPPET:\\ndef bubble_sort(nums):\\n    for i in range(len(nums)):\\n        for j in range(len(nums)):\\n            if nums[j] > nums[j + 1]:\\n                nums[j + 1], nums[j] = nums[j], nums[j + 1]\\n    return nums\\n\\nLOCAL VARIABLES:\\n{\\n    'nums': [3, 4, 2, 1, 5, 9],   # type: list\\n    'i': 0,   # type: int\\n    'j': 4,   # type: int\\n}\\n\\nGLOBAL VARIABLES:\\n{\\n}\\n\\nSOLUTION PART 1:\\n\\nRoboduck: It looks like there could be a few issues with your code snippet.\\nThe first issue is that the code is not correct for bubble sort.\\n\\nSOLUTION PART 2:\\ndef bubble_sort(nums):\\n    for i in range(len(nums)):   # type: int\\n        for j in range(len(nums) - 1 - i):   # type: int\\n            if nums[j] > nums[j + 1]:   # type: bool\\n                nums[j + 1], nums[j] = nums[j], nums[j + 1]   # type: tuple, tuple, NoneType, NoneType, NoneType\\n    return nums   # type: list\\n\\nMe: Can you explain why j iterates over len(nums) - 1 - i?\\n\\nRoboduck:\", 'meta': {'backend_name': 'openai', 'query_func': 'query_gpt3', 'datetime': 'Sun Mar 19 17:10:17 2023', 'version': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Haven't implemented support for turbo or chat yet.\n",
    "res, full = cm.query(\n",
    "    'Can you explain why j iterates over len(nums) - 1 - i?',\n",
    "    model='code-davinci-002', max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:10:25.637852Z",
     "start_time": "2023-03-20T00:10:25.604485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because the last i elements are already in place, you can safely ignore them when running subsequent iterations.\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with gpt-3.5-turbo after tentatively enabling that capability in jabberwocky 3.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:42:39.696576Z",
     "start_time": "2023-03-20T00:42:39.655942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': 'roboduck',\n",
       " 'img_path': '/Users/hmamin/jabberwocky/data/conversation_personas_custom/roboduck/profile.jpg',\n",
       " 'gender': 'M',\n",
       " 'nationality': 'American'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.start_conversation('roboduck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:43:13.638520Z",
     "start_time": "2023-03-20T00:43:13.596248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "{question}\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "def bubble_sort(nums):\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(len(nums)):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
      "    return nums\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{\n",
      "    'nums': [3, 4, 2, 1, 5, 9],   # type: list\n",
      "    'i': 0,   # type: int\n",
      "    'j': 4,   # type: int\n",
      "}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{\n",
      "}\n",
      "\n",
      "SOLUTION PART 1:\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, looks like turbo isn't available with the completion endpoint. Need to use ChatCompletion().create() instead of Completion().Create(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:49:39.438665Z",
     "start_time": "2023-03-20T00:49:38.767535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-3.5-turbo', 'temperature': 0.7, 'max_tokens': 512, 'frequency_penalty': 0.5, 'stop': ['\\n\\nMe:', 'This is a conversation with'], 'prompt': \"The following is a transcript of a conversation with Roboduck. RoboDuck is an incredibly effective AI programming assistant. He is friendly, helpful, and detail-oriented. RoboDuck has in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and has decades of experience helping Python programmers resolve their most challenging bugs.\\n\\nMe: ANSWER KEY\\n\\nThis code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\\n\\nQUESTION:\\nWhy will this throw an index error soon?\\n\\nCURRENT CODE SNIPPET:\\ndef bubble_sort(nums):\\n    for i in range(len(nums)):\\n        for j in range(len(nums)):\\n            if nums[j] > nums[j + 1]:\\n                nums[j + 1], nums[j] = nums[j], nums[j + 1]\\n    return nums\\n\\nLOCAL VARIABLES:\\n{\\n    'nums': [3, 4, 2, 1, 5, 9],   # type: list\\n    'i': 0,   # type: int\\n    'j': 4,   # type: int\\n}\\n\\nGLOBAL VARIABLES:\\n{\\n}\\n\\nSOLUTION PART 1:\\n\\nRoboduck:\", 'meta': {'backend_name': 'openai', 'query_func': 'query_gpt3', 'datetime': 'Sun Mar 19 17:49:38 2023', 'version': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:665: UserWarning: Allowing model \"gpt-3.5-turbo\" to pass through because openai_passthrough=True. We trust you to make sure this is a valid model.\n",
      "  f'Allowing model \"{model}\" to pass through because '\n"
     ]
    },
    {
     "ename": "MockFunctionException",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMockFunctionException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-30ce150319dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     user_prompt_template.format(**user_kwargs, \n\u001b[1;32m      4\u001b[0m                                 question='Why will this throw an index error soon?'), \n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, text, debug, extra_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[0;31m# Update turns after query in case something goes wrong and it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m         \u001b[0;31m# doesn't actually execute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2536\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_turns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonhm/htools/htools/meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2000\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2001\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2002\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, prompt, strip_output, log, optimize_cost, subwords, drop_fragment, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                               \u001b[0mstrip_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                               \u001b[0msubwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_fragment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_fragment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                               **kwargs)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     def _query(self, prompt, query_func, strip_output=True, log=True,\n",
      "\u001b[0;32m~/jabberwocky/lib/jabberwocky/openai_utils.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, prompt, query_func, strip_output, log, subwords, drop_fragment, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMockFunctionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMockFunctionException\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "# Try again now that turbo is available.\n",
    "res, full = cm.query(\n",
    "    user_prompt_template.format(**user_kwargs, \n",
    "                                question='Why will this throw an index error soon?'), \n",
    "    model='gpt-3.5-turbo', max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

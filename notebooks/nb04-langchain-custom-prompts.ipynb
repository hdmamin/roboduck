{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:44:59.400405Z",
     "start_time": "2023-04-16T04:44:59.310021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:45:33.036695Z",
     "start_time": "2023-04-16T04:45:32.998248Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, \\\n",
    "    HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import ChatResult, ChatGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from htools import *\n",
    "from jabberwocky.openai_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:08.803028Z",
     "start_time": "2023-03-27T05:47:08.696893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/roboduck\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:08.865773Z",
     "start_time": "2023-03-27T05:47:08.811102Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = api_key = load_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T06:53:26.226263Z",
     "start_time": "2023-03-25T06:53:26.174407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: Could try davinci text as well but codex is free for now. You may want to strip triple double-quotes from the end in case codex generates them (we don't include that as a stop phrase because codex might generate a docstring as part of a correct code snippet).\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "\"\"\"ANSWER KEY\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "{question}\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "{code}\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{local_vars}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{global_vars}\n",
      "\n",
      "SOLUTION PART 1:\n"
     ]
    }
   ],
   "source": [
    "print(load_prompt('debug')['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T06:53:29.030576Z",
     "start_time": "2023-03-25T06:53:29.000618Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"You are an incredibly effective AI programming assistant. You have in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and your goal is to help Python programmers resolve their most challenging bugs.\n",
    "\"\"\"\n",
    "system_prompt = SystemMessage(content=system_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T06:53:29.208500Z",
     "start_time": "2023-03-25T06:53:29.172102Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "user_prompt_text = \"\"\"This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two parts. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it (if you don't know what the problem is, SOLUTION PART 1 should instead list a few possible causes or things I could try in order to identify the issue). In the section titled SOLUTION PART 2, write a corrected version of the input code snippet (if you don't know, SOLUTION PART 2 should say None). SOLUTION PART 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings. Be concise and use simple language because I am a beginning programmer.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CURRENT CODE SNIPPET:\n",
    "{code}\n",
    "\n",
    "LOCAL VARIABLES:\n",
    "{local_vars}\n",
    "\n",
    "GLOBAL VARIABLES:\n",
    "{global_vars}\"\"\"\n",
    "user_prompt_template = HumanMessagePromptTemplate.from_template(\n",
    "    user_prompt_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T06:53:29.812462Z",
     "start_time": "2023-03-25T06:53:29.778952Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'question': 'Why will this throw an index error soon?',\n",
    "    'code': \"\"\"def bubble_sort(nums):\n",
    "    for i in range(len(nums)):\n",
    "        for j in range(len(nums)):\n",
    "            if nums[j] > nums[j + 1]:\n",
    "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
    "    return nums\"\"\",\n",
    "    'local_vars': \"\"\"{\n",
    "    'nums': [3, 4, 2, 1, 5, 9],   # type: list\n",
    "    'i': 0,   # type: int\n",
    "    'j': 4,   # type: int\n",
    "}\"\"\",\n",
    "    'global_vars': \"\"\"{\n",
    "}\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T06:53:30.186584Z",
     "start_time": "2023-03-25T06:53:30.155314Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    system_prompt,\n",
    "    user_prompt_template.format(**kwargs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T06:53:30.591338Z",
     "start_time": "2023-03-25T06:53:30.553451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an incredibly effective AI programming assistant. You have in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and your goal is to help Python programmers resolve their most challenging bugs.\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two parts. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it (if you don't know what the problem is, SOLUTION PART 1 should instead list a few possible causes or things I could try in order to identify the issue). In the section titled SOLUTION PART 2, write a corrected version of the input code snippet (if you don't know, SOLUTION PART 2 should say None). SOLUTION PART 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "Why will this throw an index error soon?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "def bubble_sort(nums):\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(len(nums)):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
      "    return nums\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{\n",
      "    'nums': [3, 4, 2, 1, 5, 9],   # type: list\n",
      "    'i': 0,   # type: int\n",
      "    'j': 4,   # type: int\n",
      "}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(m.content for m in messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T04:39:53.830156Z",
     "start_time": "2023-03-27T04:39:53.791727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fed643756d0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', model_kwargs={'temperature': 0.66, 'a': 'b'}, openai_api_key=None, request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=33)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=.66, a='b', max_tokens=33, \n",
    "                  model_name='gpt-3.5-turbo')\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T04:39:54.027579Z",
     "start_time": "2023-03-27T04:39:53.987254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 0.66, 'a': 'b'}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T04:40:00.939864Z",
     "start_time": "2023-03-27T04:40:00.879291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-22T04:06:34.986994Z",
     "start_time": "2023-03-22T04:06:34.949026Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLUTION PART 1:\n",
      "The code will throw an index error soon because the inner loop is iterating up to the length of the list, which means that on the last iteration, `nums[j + 1]` will be out of range. To fix this, we need to change the range of the inner loop to `range(len(nums) - i - 1)`.\n",
      "\n",
      "SOLUTION PART 2:\n",
      "\n",
      "```\n",
      "def bubble_sort(nums):\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(len(nums) - i - 1):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
      "    return nums\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-22T04:11:21.695628Z",
     "start_time": "2023-03-22T04:11:21.649258Z"
    }
   },
   "outputs": [],
   "source": [
    "messages.append(res)\n",
    "messages.append(\n",
    "    user_prompt_template.format(\n",
    "        **{**kwargs, \n",
    "           'question': 'Can you revise your solution so you only find the length of nums once?'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-22T04:11:55.671084Z",
     "start_time": "2023-03-22T04:11:49.538517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLUTION PART 1:\n",
      "The problem with the current code is that it is calling `len(nums)` twice in the inner loop, which is inefficient. To fix this, we can store the length of `nums` in a variable before the loop and use that variable instead.\n",
      "\n",
      "SOLUTION PART 2:\n",
      "\n",
      "```\n",
      "def bubble_sort(nums):\n",
      "    n = len(nums)\n",
      "    for i in range(n):\n",
      "        for j in range(n - i - 1):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j + 1], nums[j] = nums[j], nums[j + 1]\n",
      "    return nums\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took first stab at storing this info in a file (py for now). Try loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T04:18:23.370444Z",
     "start_time": "2023-03-31T04:18:23.334545Z"
    }
   },
   "outputs": [],
   "source": [
    "from inspect import Parameter\n",
    "import importlib\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.callbacks.base import CallbackManager, BaseCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from roboduck.prompts import load_template\n",
    "from roboduck.utils import colored\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T04:24:56.397606Z",
     "start_time": "2023-03-31T04:24:56.310835Z"
    }
   },
   "outputs": [],
   "source": [
    "chat.chat.callback_manager.on_llm_new_token??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:08:57.889685Z",
     "start_time": "2023-04-02T05:08:57.761807Z"
    }
   },
   "outputs": [],
   "source": [
    "class LiveTypingCallbackHandler(StreamingStdOutCallbackHandler):\n",
    "    # This parent class mostly just gives prevents us from having to write a\n",
    "    # bunch of boilerplate methods that do nothing, but it also makes sense\n",
    "    # since this implements a specific subcase of streaming to stdout.\n",
    "    always_verbose = True\n",
    "    \n",
    "    def __init__(self, color='green', sleep=.01):\n",
    "        self.color = color\n",
    "        self.sleep = sleep\n",
    "    \n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        for char in token:\n",
    "            sys.stdout.write(colored(char, self.color))\n",
    "            time.sleep(self.sleep)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T07:04:58.998750Z",
     "start_time": "2023-04-01T07:04:58.961955Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class DummyChatModel:\n",
    "    # We'd have to be a bit more rigid about expects init args if we want to\n",
    "    # subclass from BaseChatModel. For now this is fine.\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.verbose = getattr(self, 'verbose', True)\n",
    "        \n",
    "    def __call__(self, messages, stop=None):\n",
    "        return self._generate(messages, stop=stop).generations[0].message\n",
    "    \n",
    "    def _generate(self, messages, stop=None):\n",
    "        res = messages[-1].content.upper()\n",
    "        if self.streaming:\n",
    "            tokens = [tok + ' ' for tok in res.split(' ')]\n",
    "            tokens[-1] = tokens[-1].rstrip(' ')\n",
    "            for token in tokens:\n",
    "                self.callback_manager.on_llm_new_token(\n",
    "                    token,\n",
    "                    verbose=self.verbose,\n",
    "                )\n",
    "        message = AIMessage(content=res)\n",
    "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
    "    \n",
    "    async def _agenerate(self, messages, stop=None):\n",
    "        warnings.warn(\n",
    "            f'{type(self).__name__} doesn\\'t provide a real _agenerate '\n",
    "            'method. Calling synchronous generate() instead.'\n",
    "        )\n",
    "        return self._generate(messages, stop=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T06:51:53.473756Z",
     "start_time": "2023-04-01T06:51:53.443509Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Old approach. Decorator is not as useful for our use case because we want to\n",
    "# define the base function once but create several variants of it using\n",
    "# different fields.\n",
    "# def add_kwargs(*fields):\n",
    "#     def decorator(func):\n",
    "#         # In practice langchain checks for this anyway if we ask for a\n",
    "#         # completion, but outside of that context we need typecheck\n",
    "#         # because otherwise we could provide no kwargs and _func wouldn't\n",
    "#         # complain. Just use generic type because we only care that a value is\n",
    "#         # provided.\n",
    "#         @typecheck(**{f: object for f in fields})\n",
    "#         @wraps(func)\n",
    "#         def wrapper(**kwargs):\n",
    "#             return func(**kwargs)\n",
    "\n",
    "#         sig = signature(wrapper)\n",
    "#         params_ = {field: Parameter(field, Parameter.KEYWORD_ONLY)\n",
    "#                    for field in fields}\n",
    "#         wrapper.__signature__ = sig.replace(parameters=params_.values())\n",
    "#         return wrapper\n",
    "#     return decorator\n",
    "\n",
    "\n",
    "# @add_kwargs('question', 'abc')\n",
    "# def _reply(**kwargs):\n",
    "#     return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T07:00:28.620929Z",
     "start_time": "2023-04-01T07:00:28.545630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got TypeError(Field \"key\" is not a valid hide_field because it has no default value in the original function.).\n"
     ]
    }
   ],
   "source": [
    "def add_kwargs(func, fields, hide_fields=(), strict=False):\n",
    "    # Hide_fields must have default values in existing function. They will not\n",
    "    # show up in the new docstring and the user will not be able to pass in a\n",
    "    # value when calling the new function - it will always use the default.\n",
    "    # To set different defaults, you can pass in a partial rather than a \n",
    "    # function as the first arg here.\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    if hide_fields and not strict:\n",
    "        raise ValueError(\n",
    "            'You must set strict=True when providing one or more '\n",
    "            'hide_fields. Otherwise the user can still pass in those args.'\n",
    "        )\n",
    "    sig = signature(wrapper)\n",
    "    params_ = {k: v for k, v in sig.parameters.items()}\n",
    "    \n",
    "    # Remove any fields we want to hide.\n",
    "    for field in hide_fields:\n",
    "        if field not in params_:\n",
    "            warnings.warn(f'No need to hide field {field} because it\\'s not '\n",
    "                          'in the existing function signature.')\n",
    "        elif params_.pop(field).default == Parameter.empty:\n",
    "            raise TypeError(\n",
    "                f'Field \"{field}\" is not a valid hide_field because it has '\n",
    "                'no default value in the original function.'\n",
    "            )\n",
    "            \n",
    "    if getattr(params_.pop('kwargs', None), 'kind') != Parameter.VAR_KEYWORD:\n",
    "        raise TypeError(f'Function {func} must accept **kwargs.')\n",
    "    new_params = {\n",
    "        field: Parameter(field, Parameter.KEYWORD_ONLY)\n",
    "        for field in fields\n",
    "    }\n",
    "    overlap = set(new_params) & set(params_)\n",
    "    if overlap:\n",
    "        raise RuntimeError(\n",
    "            f'Some of the kwargs you tried to inject into {func} already '\n",
    "            'exist in its signature. This is not allowed because it\\'s '\n",
    "            'unclear how to resolve default values and parameter type.'\n",
    "        )\n",
    "\n",
    "    params_.update(new_params)\n",
    "    wrapper.__signature__ = sig.replace(parameters=params_.values())\n",
    "    if strict:\n",
    "        # In practice langchain checks for this anyway if we ask for a\n",
    "        # completion, but outside of that context we need typecheck\n",
    "        # because otherwise we could provide no kwargs and _func wouldn't\n",
    "        # complain. Just use generic type because we only care that a value is\n",
    "        # provided.\n",
    "        wrapper = typecheck(wrapper, **{f: object for f in fields})\n",
    "    return wrapper\n",
    "\n",
    "def _reply(key, bar=-1, **kwargs):\n",
    "    \"\"\"Test docstring.\"\"\"\n",
    "    return key, bar, kwargs\n",
    "\n",
    "reply = add_kwargs(_reply, ['statement', 'response'])\n",
    "strict_reply = add_kwargs(_reply, ['statement', 'response'], strict=True)\n",
    "with assert_raises(TypeError):\n",
    "    no_key_reply = add_kwargs(_reply, ['statement', 'response'],\n",
    "                              hide_fields=['key'], strict=True)\n",
    "    \n",
    "    \n",
    "def reply_(key='aaa', bar=-1, **kwargs):\n",
    "    \"\"\"Test docstring.\"\"\"\n",
    "    return key, bar, kwargs\n",
    "\n",
    "\n",
    "no_key_reply = add_kwargs(reply_, ['statement', 'response'],\n",
    "                          hide_fields=['key'], strict=True)\n",
    "diff_key_reply = add_kwargs(\n",
    "    partial(reply_, key='zzz'), ['statement', 'response'],\n",
    "    hide_fields=['key'], strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:16.353196Z",
     "start_time": "2023-03-27T05:47:16.314181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 99, {'statement': 44, 'response': -1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply('a', bar=99, statement=44, response=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:16.504396Z",
     "start_time": "2023-03-27T05:47:16.470254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 99, {'statement': 44})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply('a', bar=99, statement=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:16.648014Z",
     "start_time": "2023-03-27T05:47:16.611858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 99, {'statement': 44, 'response': -1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strict_reply('a', bar=99, statement=44, response=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:17.003765Z",
     "start_time": "2023-03-27T05:47:16.956934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got TypeError(missing a required argument: 'response').\n"
     ]
    }
   ],
   "source": [
    "with assert_raises(TypeError):\n",
    "    strict_reply('a', bar=99, statement=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:17.329670Z",
     "start_time": "2023-03-27T05:47:17.286439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aaa', 99, {'statement': 44, 'response': -1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_key_reply(bar=99, statement=44, response=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:17.812991Z",
     "start_time": "2023-03-27T05:47:17.775467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got TypeError(got an unexpected keyword argument 'key').\n"
     ]
    }
   ],
   "source": [
    "with assert_raises(TypeError):\n",
    "    no_key_reply(bar=99, statement=44, response=-1, key=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:18.023613Z",
     "start_time": "2023-03-27T05:47:17.987097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zzz', 99, {'statement': 44, 'response': -1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_key_reply(bar=99, statement=44, response=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T05:47:18.309140Z",
     "start_time": "2023-03-27T05:47:18.270188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got TypeError(got an unexpected keyword argument 'key').\n"
     ]
    }
   ],
   "source": [
    "with assert_raises(TypeError):\n",
    "    diff_key_reply(bar=99, statement=44, response=-1, key='eeee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:25:10.523729Z",
     "start_time": "2023-04-02T05:25:10.468629Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_code(text, join_multi=True, multi_prefix_template='\\n\\n# {i}\\n'):\n",
    "    # multi_prefix_template is only used when join_multi=True and multiple\n",
    "    # chunks are found.\n",
    "    chunks = re.findall(\"(?s)```\\n(.*?)\\n```\", text)\n",
    "    if not join_multi:\n",
    "        return chunks\n",
    "    if len(chunks) > 1:\n",
    "        chunks = [multi_prefix_template.format(i=i) + chunk \n",
    "                  for i, chunk in enumerate(chunks, 1)]\n",
    "        chunks[0] = chunks[0].lstrip()\n",
    "    return ''.join(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:01:26.342039Z",
     "start_time": "2023-04-02T05:01:26.023804Z"
    }
   },
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    \n",
    "    def __init__(self, system, user, chat_class=ChatOpenAI,\n",
    "                 history=(), streaming=True, **kwargs):\n",
    "        # kwargs can include both model_kwargs to pass to chat_class \n",
    "        # (things like temperature or top_p that affect completion directly)\n",
    "        # and other miscellaneous kwargs `callback_manager` or `verbose`.\n",
    "        self.kwargs = dict(kwargs)\n",
    "        self.kwargs.update(streaming=streaming)\n",
    "        if 'callback_manager' not in self.kwargs:\n",
    "            self.kwargs['callback_manager'] = CallbackManager(\n",
    "                [LiveTypingCallbackHandler()]\n",
    "            )\n",
    "        self.chat = chat_class(**self.kwargs)\n",
    "        self.system_message = SystemMessage(content=system)\n",
    "        if isinstance(user, str):\n",
    "            user = {'reply': user}\n",
    "        self.user_templates = {\n",
    "            k: HumanMessagePromptTemplate.from_template(v)\n",
    "            for k, v in user.items()\n",
    "        }\n",
    "        self.default_user_key = next(iter(self.user_templates))\n",
    "        self.default_user_fields = (self.user_templates[self.default_user_key]\n",
    "                                    .input_variables)\n",
    "        self._history = list(history) or [self.system_message]\n",
    "        self._create_reply_methods()\n",
    "        \n",
    "    def _create_reply_methods(self):\n",
    "        \"\"\"Creates two options for user to send replies:\n",
    "        1. call chat.reply(), using the key_ arg to determine which type of\n",
    "        user_message is sent. The docstring shows the default user \n",
    "        message type's fields but if you set the key accordingly you can pass\n",
    "        in fields for another message type. We choose not to infer key_\n",
    "        because some user_message types may accept the same fields.\n",
    "        2. call methods like chat.question() or chat.statement(), where 'chat'\n",
    "        and 'statement' are the names of all available user message types\n",
    "        (i.e. the keys of the `user` dict in the prompt config file). You can\n",
    "        not pass in key_.\n",
    "        \"\"\"\n",
    "        for k, v in self.user_templates.items():\n",
    "            if hasattr(self, k):\n",
    "                warnings.warn(\n",
    "                    f'Name collision: prompt defines user message type {k} '\n",
    "                    f'but Chat class already has a method with that name. '\n",
    "                    f'Method will be named {k}_ instead.'\n",
    "                )\n",
    "                k = k + '_'\n",
    "            meth = add_kwargs(partial(self._reply, key_=k), \n",
    "                              fields=v.input_variables,\n",
    "                              hide_fields=['key_'],\n",
    "                              strict=True)\n",
    "            setattr(self, k, meth)\n",
    "        setattr(\n",
    "            self, \n",
    "            'reply', \n",
    "            add_kwargs(self._reply, self.default_user_fields, strict=False)\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, name, **kwargs):\n",
    "        template = load_template(name)\n",
    "        if kwargs:\n",
    "            template['kwargs'].update(kwargs)\n",
    "        kwargs = template.pop('kwargs', {})\n",
    "        return cls(**template, **kwargs)\n",
    "        \n",
    "    def _user_message(self, *, key_='', **kwargs):\n",
    "        key = key_ or self.default_user_key\n",
    "        template = self.user_templates[key]\n",
    "        return template.format(**kwargs)\n",
    "    \n",
    "    def _reply(self, *, key_='', **kwargs):\n",
    "        user_message = self._user_message(key_=key_, **kwargs)\n",
    "        self._history.append(user_message)\n",
    "        try:\n",
    "            response = self.chat(self._history)\n",
    "        except Exception as e:\n",
    "            self._history.pop(-1)\n",
    "            raise e\n",
    "        self._history.append(response)\n",
    "        return response\n",
    "    \n",
    "    def history(self, sep='\\n\\n', speaker_prefix=True):\n",
    "        \"\"\"Return chat history as a single string.\"\"\"\n",
    "        res = []\n",
    "        for row in self._history:\n",
    "            reply = row.content\n",
    "            if speaker_prefix:\n",
    "                speaker = type(row).__name__.split('Message')[0]\n",
    "                reply = f'{speaker}: {reply}'\n",
    "            res.append(reply)\n",
    "        return sep.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T05:14:51.399944Z",
     "start_time": "2023-04-04T05:14:51.363884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code', 'global_vars', 'local_vars', 'next_line', 'question']"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.user_templates['contextful'].input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T05:14:57.159441Z",
     "start_time": "2023-04-04T05:14:57.125340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question']"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.user_templates['contextless'].input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:01:27.236759Z",
     "start_time": "2023-04-02T05:01:27.166049Z"
    }
   },
   "outputs": [],
   "source": [
    "chat = Chat.from_config('debug', chat_class=DummyChatModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:01:43.222491Z",
     "start_time": "2023-04-02T05:01:43.184479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two sections, separated by an empty line. Do not include section titles of any kind. In section 1, use plain English to explain what the problem is and how to fix it (if you couldn't identify the problem, section 1 should instead list a few possible causes or things I could try in order to identify the issue). In section 2, write a corrected version of the input code snippet (if you don't know, section 2 should be empty). Section 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings. Section 2 must be entirely enclosed in one pair of triple backticks (\"```\"). \n",
      "\n",
      "QUESTION:\n",
      "Why?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "a = 3\n",
      "b = 4\n",
      "\n",
      "NEXT LINE:\n",
      "b = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{True: False}\n"
     ]
    }
   ],
   "source": [
    "tmp = chat._user_message(\n",
    "    code='a = 3\\nb = 4', question='Why?', local_vars='{3: 4}',\n",
    "    global_vars='{True: False}', next_line='b = 4'\n",
    ")\n",
    "\n",
    "print(tmp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:01:44.294327Z",
     "start_time": "2023-04-02T05:01:44.257691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Why?\n"
     ]
    }
   ],
   "source": [
    "tmp = chat._user_message(\n",
    "    key_='contextless',\n",
    "    question='Why?'\n",
    ")\n",
    "\n",
    "print(tmp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:01:45.712217Z",
     "start_time": "2023-04-02T05:01:45.421225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mQ\u001b[39m\u001b[32mU\u001b[39m\u001b[32mE\u001b[39m\u001b[32mS\u001b[39m\u001b[32mT\u001b[39m\u001b[32mI\u001b[39m\u001b[32mO\u001b[39m\u001b[32mN\u001b[39m\u001b[32m:\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32mX\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='QUESTION:\\nX', additional_kwargs={})"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.contextless(question='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:01:49.433559Z",
     "start_time": "2023-04-02T05:01:49.399344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got TypeError(got an unexpected keyword argument 'key_').\n"
     ]
    }
   ],
   "source": [
    "with assert_raises(TypeError):\n",
    "    chat.contextless(question='x', key_='contextful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:02:03.480852Z",
     "start_time": "2023-04-02T05:02:03.432772Z"
    }
   },
   "outputs": [],
   "source": [
    "chat.contextful(\n",
    "    code='a = 3\\nb = 4', question='Why?', local_vars='{3: 4}',\n",
    "    global_vars='{True: False}', next_line='b = 4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T03:28:15.009296Z",
     "start_time": "2023-03-31T03:28:14.973681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"THIS CODE SNIPPET IS NOT WORKING AS EXPECTED. HELP ME DEBUG IT. FIRST READ MY QUESTION, THEN EXAMINE THE SNIPPET OF CODE THAT IS CAUSING THE ISSUE AND LOOK AT THE VALUES OF THE LOCAL AND GLOBAL VARIABLES. YOUR RESPONSE MUST HAVE EXACTLY TWO SECTIONS, SEPARATED BY AN EMPTY LINE. DO NOT INCLUDE SECTION TITLES OF ANY KIND. IN SECTION 1, USE PLAIN ENGLISH TO EXPLAIN WHAT THE PROBLEM IS AND HOW TO FIX IT (IF YOU COULDN'T IDENTIFY THE PROBLEM, SECTION 1 SHOULD INSTEAD LIST A FEW POSSIBLE CAUSES OR THINGS I COULD TRY IN ORDER TO IDENTIFY THE ISSUE). IN SECTION 2, WRITE A CORRECTED VERSION OF THE INPUT CODE SNIPPET (IF YOU DON'T KNOW, SECTION 2 SHOULD BE EMPTY). SECTION 2 MUST CONTAIN ONLY PYTHON CODE - THERE MUST NOT BE ANY ENGLISH EXPLANATION OUTSIDE OF CODE COMMENTS OR DOCSTRINGS.\\n\\nQUESTION:\\nWHY?\\n\\nCURRENT CODE SNIPPET:\\nA = 3\\nB = 4\\n\\nNEXT LINE:\\nB = 4\\n\\nLOCAL VARIABLES:\\n{3: 4}\\n\\nGLOBAL VARIABLES:\\n{TRUE: FALSE}\", additional_kwargs={})"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.reply(\n",
    "    code='a = 3\\nb = 4', question='Why?', local_vars='{3: 4}',\n",
    "    global_vars='{True: False}', next_line='b = 4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:02:14.133402Z",
     "start_time": "2023-04-02T05:02:13.574702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mQ\u001b[39m\u001b[32mU\u001b[39m\u001b[32mE\u001b[39m\u001b[32mS\u001b[39m\u001b[32mT\u001b[39m\u001b[32mI\u001b[39m\u001b[32mO\u001b[39m\u001b[32mN\u001b[39m\u001b[32m:\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32mH\u001b[39m\u001b[32mO\u001b[39m\u001b[32mW\u001b[39m\u001b[32m \u001b[39m\u001b[32mA\u001b[39m\u001b[32mR\u001b[39m\u001b[32mE\u001b[39m\u001b[32m \u001b[39m\u001b[32mY\u001b[39m\u001b[32mO\u001b[39m\u001b[32mU\u001b[39m\u001b[32m?\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='QUESTION:\\nHOW ARE YOU?', additional_kwargs={})"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.reply(question='How are you?', key_='contextless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T03:28:17.532691Z",
     "start_time": "2023-03-31T03:28:17.496818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an incredibly effective AI programming assistant. You have in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and your goal is to help Python programmers resolve their most challenging bugs. Be concise and use simple language.\n",
      "\n",
      "Human: QUESTION:\n",
      "x\n",
      "\n",
      "AI: QUESTION:\n",
      "X\n",
      "\n",
      "Human: This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two sections, separated by an empty line. Do not include section titles of any kind. In section 1, use plain English to explain what the problem is and how to fix it (if you couldn't identify the problem, section 1 should instead list a few possible causes or things I could try in order to identify the issue). In section 2, write a corrected version of the input code snippet (if you don't know, section 2 should be empty). Section 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings.\n",
      "\n",
      "QUESTION:\n",
      "Why?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "a = 3\n",
      "b = 4\n",
      "\n",
      "NEXT LINE:\n",
      "b = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{True: False}\n",
      "\n",
      "AI: THIS CODE SNIPPET IS NOT WORKING AS EXPECTED. HELP ME DEBUG IT. FIRST READ MY QUESTION, THEN EXAMINE THE SNIPPET OF CODE THAT IS CAUSING THE ISSUE AND LOOK AT THE VALUES OF THE LOCAL AND GLOBAL VARIABLES. YOUR RESPONSE MUST HAVE EXACTLY TWO SECTIONS, SEPARATED BY AN EMPTY LINE. DO NOT INCLUDE SECTION TITLES OF ANY KIND. IN SECTION 1, USE PLAIN ENGLISH TO EXPLAIN WHAT THE PROBLEM IS AND HOW TO FIX IT (IF YOU COULDN'T IDENTIFY THE PROBLEM, SECTION 1 SHOULD INSTEAD LIST A FEW POSSIBLE CAUSES OR THINGS I COULD TRY IN ORDER TO IDENTIFY THE ISSUE). IN SECTION 2, WRITE A CORRECTED VERSION OF THE INPUT CODE SNIPPET (IF YOU DON'T KNOW, SECTION 2 SHOULD BE EMPTY). SECTION 2 MUST CONTAIN ONLY PYTHON CODE - THERE MUST NOT BE ANY ENGLISH EXPLANATION OUTSIDE OF CODE COMMENTS OR DOCSTRINGS.\n",
      "\n",
      "QUESTION:\n",
      "WHY?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "A = 3\n",
      "B = 4\n",
      "\n",
      "NEXT LINE:\n",
      "B = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{TRUE: FALSE}\n",
      "\n",
      "Human: This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two sections, separated by an empty line. Do not include section titles of any kind. In section 1, use plain English to explain what the problem is and how to fix it (if you couldn't identify the problem, section 1 should instead list a few possible causes or things I could try in order to identify the issue). In section 2, write a corrected version of the input code snippet (if you don't know, section 2 should be empty). Section 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings.\n",
      "\n",
      "QUESTION:\n",
      "Why?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "a = 3\n",
      "b = 4\n",
      "\n",
      "NEXT LINE:\n",
      "b = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{True: False}\n",
      "\n",
      "AI: THIS CODE SNIPPET IS NOT WORKING AS EXPECTED. HELP ME DEBUG IT. FIRST READ MY QUESTION, THEN EXAMINE THE SNIPPET OF CODE THAT IS CAUSING THE ISSUE AND LOOK AT THE VALUES OF THE LOCAL AND GLOBAL VARIABLES. YOUR RESPONSE MUST HAVE EXACTLY TWO SECTIONS, SEPARATED BY AN EMPTY LINE. DO NOT INCLUDE SECTION TITLES OF ANY KIND. IN SECTION 1, USE PLAIN ENGLISH TO EXPLAIN WHAT THE PROBLEM IS AND HOW TO FIX IT (IF YOU COULDN'T IDENTIFY THE PROBLEM, SECTION 1 SHOULD INSTEAD LIST A FEW POSSIBLE CAUSES OR THINGS I COULD TRY IN ORDER TO IDENTIFY THE ISSUE). IN SECTION 2, WRITE A CORRECTED VERSION OF THE INPUT CODE SNIPPET (IF YOU DON'T KNOW, SECTION 2 SHOULD BE EMPTY). SECTION 2 MUST CONTAIN ONLY PYTHON CODE - THERE MUST NOT BE ANY ENGLISH EXPLANATION OUTSIDE OF CODE COMMENTS OR DOCSTRINGS.\n",
      "\n",
      "QUESTION:\n",
      "WHY?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "A = 3\n",
      "B = 4\n",
      "\n",
      "NEXT LINE:\n",
      "B = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{TRUE: FALSE}\n",
      "\n",
      "Human: QUESTION:\n",
      "How are you?\n",
      "\n",
      "AI: QUESTION:\n",
      "HOW ARE YOU?\n"
     ]
    }
   ],
   "source": [
    "print(chat.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T03:28:20.633350Z",
     "start_time": "2023-03-31T03:28:20.603194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an incredibly effective AI programming assistant. You have in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and your goal is to help Python programmers resolve their most challenging bugs. Be concise and use simple language.\n",
      "\n",
      "QUESTION:\n",
      "x\n",
      "\n",
      "QUESTION:\n",
      "X\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two sections, separated by an empty line. Do not include section titles of any kind. In section 1, use plain English to explain what the problem is and how to fix it (if you couldn't identify the problem, section 1 should instead list a few possible causes or things I could try in order to identify the issue). In section 2, write a corrected version of the input code snippet (if you don't know, section 2 should be empty). Section 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings.\n",
      "\n",
      "QUESTION:\n",
      "Why?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "a = 3\n",
      "b = 4\n",
      "\n",
      "NEXT LINE:\n",
      "b = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{True: False}\n",
      "\n",
      "THIS CODE SNIPPET IS NOT WORKING AS EXPECTED. HELP ME DEBUG IT. FIRST READ MY QUESTION, THEN EXAMINE THE SNIPPET OF CODE THAT IS CAUSING THE ISSUE AND LOOK AT THE VALUES OF THE LOCAL AND GLOBAL VARIABLES. YOUR RESPONSE MUST HAVE EXACTLY TWO SECTIONS, SEPARATED BY AN EMPTY LINE. DO NOT INCLUDE SECTION TITLES OF ANY KIND. IN SECTION 1, USE PLAIN ENGLISH TO EXPLAIN WHAT THE PROBLEM IS AND HOW TO FIX IT (IF YOU COULDN'T IDENTIFY THE PROBLEM, SECTION 1 SHOULD INSTEAD LIST A FEW POSSIBLE CAUSES OR THINGS I COULD TRY IN ORDER TO IDENTIFY THE ISSUE). IN SECTION 2, WRITE A CORRECTED VERSION OF THE INPUT CODE SNIPPET (IF YOU DON'T KNOW, SECTION 2 SHOULD BE EMPTY). SECTION 2 MUST CONTAIN ONLY PYTHON CODE - THERE MUST NOT BE ANY ENGLISH EXPLANATION OUTSIDE OF CODE COMMENTS OR DOCSTRINGS.\n",
      "\n",
      "QUESTION:\n",
      "WHY?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "A = 3\n",
      "B = 4\n",
      "\n",
      "NEXT LINE:\n",
      "B = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{TRUE: FALSE}\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two sections, separated by an empty line. Do not include section titles of any kind. In section 1, use plain English to explain what the problem is and how to fix it (if you couldn't identify the problem, section 1 should instead list a few possible causes or things I could try in order to identify the issue). In section 2, write a corrected version of the input code snippet (if you don't know, section 2 should be empty). Section 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings.\n",
      "\n",
      "QUESTION:\n",
      "Why?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "a = 3\n",
      "b = 4\n",
      "\n",
      "NEXT LINE:\n",
      "b = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{True: False}\n",
      "\n",
      "THIS CODE SNIPPET IS NOT WORKING AS EXPECTED. HELP ME DEBUG IT. FIRST READ MY QUESTION, THEN EXAMINE THE SNIPPET OF CODE THAT IS CAUSING THE ISSUE AND LOOK AT THE VALUES OF THE LOCAL AND GLOBAL VARIABLES. YOUR RESPONSE MUST HAVE EXACTLY TWO SECTIONS, SEPARATED BY AN EMPTY LINE. DO NOT INCLUDE SECTION TITLES OF ANY KIND. IN SECTION 1, USE PLAIN ENGLISH TO EXPLAIN WHAT THE PROBLEM IS AND HOW TO FIX IT (IF YOU COULDN'T IDENTIFY THE PROBLEM, SECTION 1 SHOULD INSTEAD LIST A FEW POSSIBLE CAUSES OR THINGS I COULD TRY IN ORDER TO IDENTIFY THE ISSUE). IN SECTION 2, WRITE A CORRECTED VERSION OF THE INPUT CODE SNIPPET (IF YOU DON'T KNOW, SECTION 2 SHOULD BE EMPTY). SECTION 2 MUST CONTAIN ONLY PYTHON CODE - THERE MUST NOT BE ANY ENGLISH EXPLANATION OUTSIDE OF CODE COMMENTS OR DOCSTRINGS.\n",
      "\n",
      "QUESTION:\n",
      "WHY?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "A = 3\n",
      "B = 4\n",
      "\n",
      "NEXT LINE:\n",
      "B = 4\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{3: 4}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{TRUE: FALSE}\n",
      "\n",
      "QUESTION:\n",
      "How are you?\n",
      "\n",
      "QUESTION:\n",
      "HOW ARE YOU?\n"
     ]
    }
   ],
   "source": [
    "# This would make more sense if our messages included speakers, i.e. if a\n",
    "# user_message looked like 'Me: {reply}' and gpt was prompted to reply like\n",
    "# 'Robert Sapolsky: {reply}' (for example).\n",
    "print(chat.history(speaker_prefix=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real openai obj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:02:19.569501Z",
     "start_time": "2023-04-02T05:02:19.524872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.base.CallbackManager object at 0x7fdb293cca30>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', model_kwargs={'temperature': 0.0, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'logit_bias': {}, 'stop': ['QUESTION', 'SOLUTION PART 3']}, openai_api_key=None, request_timeout=60, max_retries=6, streaming=True, n=1, max_tokens=512)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat.from_config('debug')\n",
    "chat.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:02:32.194295Z",
     "start_time": "2023-04-02T05:02:32.158022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gpt-3.5-turbo',\n",
       " 'max_tokens': 512,\n",
       " 'temperature': 0.0,\n",
       " 'top_p': 1.0,\n",
       " 'frequency_penalty': 0.0,\n",
       " 'presence_penalty': 0.0,\n",
       " 'logit_bias': {},\n",
       " 'stop': ['QUESTION', 'SOLUTION PART 3'],\n",
       " 'streaming': True,\n",
       " 'callback_manager': <langchain.callbacks.base.CallbackManager at 0x7fdb293cca30>}"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:03:21.892208Z",
     "start_time": "2023-04-02T05:03:10.507110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mA\u001b[39m\u001b[32mp\u001b[39m\u001b[32mp\u001b[39m\u001b[32me\u001b[39m\u001b[32mn\u001b[39m\u001b[32md\u001b[39m\u001b[32mi\u001b[39m\u001b[32mn\u001b[39m\u001b[32mg\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mo\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mu\u001b[39m\u001b[32mp\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32mn\u001b[39m\u001b[32mo\u001b[39m\u001b[32mt\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32ml\u001b[39m\u001b[32ml\u001b[39m\u001b[32mo\u001b[39m\u001b[32mw\u001b[39m\u001b[32me\u001b[39m\u001b[32md\u001b[39m\u001b[32m \u001b[39m\u001b[32mb\u001b[39m\u001b[32me\u001b[39m\u001b[32mc\u001b[39m\u001b[32ma\u001b[39m\u001b[32mu\u001b[39m\u001b[32ms\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mu\u001b[39m\u001b[32mp\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32mr\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mi\u001b[39m\u001b[32mm\u001b[39m\u001b[32mm\u001b[39m\u001b[32mu\u001b[39m\u001b[32mt\u001b[39m\u001b[32ma\u001b[39m\u001b[32mb\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32m.\u001b[39m\u001b[32m \u001b[39m\u001b[32mH\u001b[39m\u001b[32mo\u001b[39m\u001b[32mw\u001b[39m\u001b[32me\u001b[39m\u001b[32mv\u001b[39m\u001b[32me\u001b[39m\u001b[32mr\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32mi\u001b[39m\u001b[32mn\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32mo\u001b[39m\u001b[32md\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32ms\u001b[39m\u001b[32mn\u001b[39m\u001b[32mi\u001b[39m\u001b[32mp\u001b[39m\u001b[32mp\u001b[39m\u001b[32me\u001b[39m\u001b[32mt\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mu\u001b[39m\u001b[32mp\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mb\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32mo\u001b[39m\u001b[32mn\u001b[39m\u001b[32mt\u001b[39m\u001b[32ma\u001b[39m\u001b[32mi\u001b[39m\u001b[32mn\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mw\u001b[39m\u001b[32mo\u001b[39m\u001b[32m \u001b[39m\u001b[32ml\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32mt\u001b[39m\u001b[32ms\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32mn\u001b[39m\u001b[32md\u001b[39m\u001b[32m \u001b[39m\u001b[32ml\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32mt\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32mr\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mm\u001b[39m\u001b[32mu\u001b[39m\u001b[32mt\u001b[39m\u001b[32ma\u001b[39m\u001b[32mb\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32m.\u001b[39m\u001b[32m \u001b[39m\u001b[32mT\u001b[39m\u001b[32mh\u001b[39m\u001b[32me\u001b[39m\u001b[32mr\u001b[39m\u001b[32me\u001b[39m\u001b[32mf\u001b[39m\u001b[32mo\u001b[39m\u001b[32mr\u001b[39m\u001b[32me\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32mp\u001b[39m\u001b[32mp\u001b[39m\u001b[32me\u001b[39m\u001b[32mn\u001b[39m\u001b[32md\u001b[39m\u001b[32mi\u001b[39m\u001b[32mn\u001b[39m\u001b[32mg\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mo\u001b[39m\u001b[32m \u001b[39m\u001b[32mb\u001b[39m\u001b[32m[\u001b[39m\u001b[32m1\u001b[39m\u001b[32m]\u001b[39m\u001b[32m \u001b[39m\u001b[32m(\u001b[39m\u001b[32mw\u001b[39m\u001b[32mh\u001b[39m\u001b[32mi\u001b[39m\u001b[32mc\u001b[39m\u001b[32mh\u001b[39m\u001b[32m \u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32m \u001b[39m\u001b[32ml\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32mt\u001b[39m\u001b[32m)\u001b[39m\u001b[32m \u001b[39m\u001b[32md\u001b[39m\u001b[32mo\u001b[39m\u001b[32me\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32mn\u001b[39m\u001b[32mo\u001b[39m\u001b[32mt\u001b[39m\u001b[32m \u001b[39m\u001b[32mr\u001b[39m\u001b[32ma\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32mn\u001b[39m\u001b[32m \u001b[39m\u001b[32me\u001b[39m\u001b[32mr\u001b[39m\u001b[32mr\u001b[39m\u001b[32mo\u001b[39m\u001b[32mr\u001b[39m\u001b[32m.\u001b[39m\u001b[32m \u001b[39m\u001b[32mT\u001b[39m\u001b[32mo\u001b[39m\u001b[32m \u001b[39m\u001b[32mf\u001b[39m\u001b[32mi\u001b[39m\u001b[32mx\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32my\u001b[39m\u001b[32mo\u001b[39m\u001b[32mu\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32ma\u001b[39m\u001b[32mn\u001b[39m\u001b[32m \u001b[39m\u001b[32me\u001b[39m\u001b[32mi\u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32me\u001b[39m\u001b[32mr\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32mh\u001b[39m\u001b[32ma\u001b[39m\u001b[32mn\u001b[39m\u001b[32mg\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mb\u001b[39m\u001b[32m[\u001b[39m\u001b[32m1\u001b[39m\u001b[32m]\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mo\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mu\u001b[39m\u001b[32mp\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mo\u001b[39m\u001b[32mr\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32mr\u001b[39m\u001b[32me\u001b[39m\u001b[32ma\u001b[39m\u001b[32mt\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32m \u001b[39m\u001b[32mn\u001b[39m\u001b[32me\u001b[39m\u001b[32mw\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mu\u001b[39m\u001b[32mp\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32ma\u001b[39m\u001b[32mt\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32mo\u001b[39m\u001b[32mn\u001b[39m\u001b[32mt\u001b[39m\u001b[32ma\u001b[39m\u001b[32mi\u001b[39m\u001b[32mn\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mo\u001b[39m\u001b[32mr\u001b[39m\u001b[32mi\u001b[39m\u001b[32mg\u001b[39m\u001b[32mi\u001b[39m\u001b[32mn\u001b[39m\u001b[32ma\u001b[39m\u001b[32ml\u001b[39m\u001b[32m \u001b[39m\u001b[32me\u001b[39m\u001b[32ml\u001b[39m\u001b[32me\u001b[39m\u001b[32mm\u001b[39m\u001b[32me\u001b[39m\u001b[32mn\u001b[39m\u001b[32mt\u001b[39m\u001b[32ms\u001b[39m\u001b[32m \u001b[39m\u001b[32mo\u001b[39m\u001b[32mf\u001b[39m\u001b[32m \u001b[39m\u001b[32mb\u001b[39m\u001b[32m \u001b[39m\u001b[32ma\u001b[39m\u001b[32mn\u001b[39m\u001b[32md\u001b[39m\u001b[32m \u001b[39m\u001b[32mt\u001b[39m\u001b[32mh\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32mn\u001b[39m\u001b[32me\u001b[39m\u001b[32mw\u001b[39m\u001b[32m \u001b[39m\u001b[32ml\u001b[39m\u001b[32mi\u001b[39m\u001b[32ms\u001b[39m\u001b[32mt\u001b[39m\u001b[32m.\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32m`\u001b[39m\u001b[32m`\u001b[39m\u001b[32m`\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32m#\u001b[39m\u001b[32m \u001b[39m\u001b[32mC\u001b[39m\u001b[32mo\u001b[39m\u001b[32mr\u001b[39m\u001b[32mr\u001b[39m\u001b[32me\u001b[39m\u001b[32mc\u001b[39m\u001b[32mt\u001b[39m\u001b[32me\u001b[39m\u001b[32md\u001b[39m\u001b[32m \u001b[39m\u001b[32mc\u001b[39m\u001b[32mo\u001b[39m\u001b[32md\u001b[39m\u001b[32me\u001b[39m\u001b[32m \u001b[39m\u001b[32ms\u001b[39m\u001b[32mn\u001b[39m\u001b[32mi\u001b[39m\u001b[32mp\u001b[39m\u001b[32mp\u001b[39m\u001b[32me\u001b[39m\u001b[32mt\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32ma\u001b[39m\u001b[32m \u001b[39m\u001b[32m=\u001b[39m\u001b[32m \u001b[39m\u001b[32m3\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32mb\u001b[39m\u001b[32m \u001b[39m\u001b[32m=\u001b[39m\u001b[32m \u001b[39m\u001b[32m(\u001b[39m\u001b[32m[\u001b[39m\u001b[32m0\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32m1\u001b[39m\u001b[32m]\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32m[\u001b[39m\u001b[32m2\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32m3\u001b[39m\u001b[32m]\u001b[39m\u001b[32m)\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32mb\u001b[39m\u001b[32m \u001b[39m\u001b[32m=\u001b[39m\u001b[32m \u001b[39m\u001b[32m(\u001b[39m\u001b[32mb\u001b[39m\u001b[32m[\u001b[39m\u001b[32m0\u001b[39m\u001b[32m]\u001b[39m\u001b[32m,\u001b[39m\u001b[32m \u001b[39m\u001b[32mb\u001b[39m\u001b[32m[\u001b[39m\u001b[32m1\u001b[39m\u001b[32m]\u001b[39m\u001b[32m \u001b[39m\u001b[32m+\u001b[39m\u001b[32m \u001b[39m\u001b[32m[\u001b[39m\u001b[32ma\u001b[39m\u001b[32m]\u001b[39m\u001b[32m)\u001b[39m\u001b[32m\n",
      "\u001b[39m\u001b[32m`\u001b[39m\u001b[32m`\u001b[39m\u001b[32m`\u001b[39m"
     ]
    }
   ],
   "source": [
    "res = chat.reply(\n",
    "    code='a = 3\\nb = ([0, 1], [2, 3])\\nb[1].append(a)',\n",
    "    question='I thought tuples were immutable. Why doesn\\'t appending a throw an error?',       \n",
    "    local_vars='{\"a\": 3, \"b\": ([0, 1], [2, 3])}',\n",
    "    global_vars='{\"x\": True}', next_line='b[1].append(a)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:03:24.445902Z",
     "start_time": "2023-04-02T05:03:24.396865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Corrected code snippet\n",
      "a = 3\n",
      "b = ([0, 1], [2, 3])\n",
      "b = (b[0], b[1] + [a])\n"
     ]
    }
   ],
   "source": [
    "print(extract_code(res.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T05:22:53.231349Z",
     "start_time": "2023-04-02T05:22:53.187312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Corrected code snippet\\na = 3\\nb = ([0, 1], [2, 3])\\nb = (b[0], b[1] + [a])'"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_code(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing overriding stop kwargs to make sure it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T04:47:55.875377Z",
     "start_time": "2023-03-29T04:47:55.843869Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "manager = CallbackManager([LiveTypingCallbackHandler()])\n",
    "# manager.on_llm_new_token('caterpillar', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T04:49:13.037719Z",
     "start_time": "2023-03-29T04:49:12.977398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=True, callback_manager=<langchain.callbacks.base.CallbackManager object at 0x7fdb28ae8280>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', model_kwargs={'temperature': 0.0, 'top_p': 0.99, 'frequency_penalty': 0.2, 'presence_penalty': 0.0, 'logit_bias': {37811: -100, 27901: -50}, 'stop': ['QUESTION', 'SOLUTION PART 3']}, openai_api_key=None, request_timeout=60, max_retries=6, streaming=True, n=1, max_tokens=512)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat.from_config('debug',\n",
    "                        streaming=True,\n",
    "                        callback_manager=manager)\n",
    "chat.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T04:49:26.968070Z",
     "start_time": "2023-03-29T04:49:17.693148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mS\u001b[0m\u001b[32mO\u001b[0m\u001b[32mL\u001b[0m\u001b[32mU\u001b[0m\u001b[32mT\u001b[0m\u001b[32mI\u001b[0m\u001b[32mO\u001b[0m\u001b[32mN\u001b[0m\u001b[32m \u001b[0m\u001b[32mP\u001b[0m\u001b[32mA\u001b[0m\u001b[32mR\u001b[0m\u001b[32mT\u001b[0m\u001b[32m \u001b[0m\u001b[32m1\u001b[0m\u001b[32m:\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mT\u001b[0m\u001b[32mh\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mc\u001b[0m\u001b[32mo\u001b[0m\u001b[32md\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mi\u001b[0m\u001b[32ms\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mr\u001b[0m\u001b[32my\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32mg\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mo\u001b[0m\u001b[32m \u001b[0m\u001b[32mm\u001b[0m\u001b[32mu\u001b[0m\u001b[32ml\u001b[0m\u001b[32mt\u001b[0m\u001b[32mi\u001b[0m\u001b[32mp\u001b[0m\u001b[32ml\u001b[0m\u001b[32my\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mh\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mv\u001b[0m\u001b[32ma\u001b[0m\u001b[32ml\u001b[0m\u001b[32mu\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mo\u001b[0m\u001b[32mf\u001b[0m\u001b[32m \u001b[0m\u001b[32ma\u001b[0m\u001b[32m \u001b[0m\u001b[32mv\u001b[0m\u001b[32ma\u001b[0m\u001b[32mr\u001b[0m\u001b[32mi\u001b[0m\u001b[32ma\u001b[0m\u001b[32mb\u001b[0m\u001b[32ml\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mn\u001b[0m\u001b[32ma\u001b[0m\u001b[32mm\u001b[0m\u001b[32me\u001b[0m\u001b[32md\u001b[0m\u001b[32m \u001b[0m\u001b[32m\"\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m \u001b[0m\u001b[32mb\u001b[0m\u001b[32my\u001b[0m\u001b[32m \u001b[0m\u001b[32m3\u001b[0m\u001b[32m,\u001b[0m\u001b[32m \u001b[0m\u001b[32mb\u001b[0m\u001b[32mu\u001b[0m\u001b[32mt\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mh\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mv\u001b[0m\u001b[32ma\u001b[0m\u001b[32mr\u001b[0m\u001b[32mi\u001b[0m\u001b[32ma\u001b[0m\u001b[32mb\u001b[0m\u001b[32ml\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32m\"\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m \u001b[0m\u001b[32mh\u001b[0m\u001b[32ma\u001b[0m\u001b[32ms\u001b[0m\u001b[32m \u001b[0m\u001b[32mn\u001b[0m\u001b[32mo\u001b[0m\u001b[32mt\u001b[0m\u001b[32m \u001b[0m\u001b[32mb\u001b[0m\u001b[32me\u001b[0m\u001b[32me\u001b[0m\u001b[32mn\u001b[0m\u001b[32m \u001b[0m\u001b[32md\u001b[0m\u001b[32me\u001b[0m\u001b[32mf\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32me\u001b[0m\u001b[32md\u001b[0m\u001b[32m \u001b[0m\u001b[32my\u001b[0m\u001b[32me\u001b[0m\u001b[32mt\u001b[0m\u001b[32m.\u001b[0m\u001b[32m \u001b[0m\u001b[32mT\u001b[0m\u001b[32mh\u001b[0m\u001b[32mi\u001b[0m\u001b[32ms\u001b[0m\u001b[32m \u001b[0m\u001b[32mi\u001b[0m\u001b[32ms\u001b[0m\u001b[32m \u001b[0m\u001b[32mc\u001b[0m\u001b[32ma\u001b[0m\u001b[32mu\u001b[0m\u001b[32ms\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32mg\u001b[0m\u001b[32m \u001b[0m\u001b[32ma\u001b[0m\u001b[32m \u001b[0m\u001b[32mN\u001b[0m\u001b[32ma\u001b[0m\u001b[32mm\u001b[0m\u001b[32me\u001b[0m\u001b[32mE\u001b[0m\u001b[32mr\u001b[0m\u001b[32mr\u001b[0m\u001b[32mo\u001b[0m\u001b[32mr\u001b[0m\u001b[32m.\u001b[0m\u001b[32m \u001b[0m\u001b[32mT\u001b[0m\u001b[32mo\u001b[0m\u001b[32m \u001b[0m\u001b[32mf\u001b[0m\u001b[32mi\u001b[0m\u001b[32mx\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mh\u001b[0m\u001b[32mi\u001b[0m\u001b[32ms\u001b[0m\u001b[32m,\u001b[0m\u001b[32m \u001b[0m\u001b[32my\u001b[0m\u001b[32mo\u001b[0m\u001b[32mu\u001b[0m\u001b[32m \u001b[0m\u001b[32mn\u001b[0m\u001b[32me\u001b[0m\u001b[32me\u001b[0m\u001b[32md\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mo\u001b[0m\u001b[32m \u001b[0m\u001b[32md\u001b[0m\u001b[32me\u001b[0m\u001b[32mf\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mh\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mv\u001b[0m\u001b[32ma\u001b[0m\u001b[32mr\u001b[0m\u001b[32mi\u001b[0m\u001b[32ma\u001b[0m\u001b[32mb\u001b[0m\u001b[32ml\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32m\"\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m \u001b[0m\u001b[32mb\u001b[0m\u001b[32me\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mr\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mr\u001b[0m\u001b[32my\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32mg\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mo\u001b[0m\u001b[32m \u001b[0m\u001b[32mu\u001b[0m\u001b[32ms\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mi\u001b[0m\u001b[32mt\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mS\u001b[0m\u001b[32mO\u001b[0m\u001b[32mL\u001b[0m\u001b[32mU\u001b[0m\u001b[32mT\u001b[0m\u001b[32mI\u001b[0m\u001b[32mO\u001b[0m\u001b[32mN\u001b[0m\u001b[32m \u001b[0m\u001b[32mP\u001b[0m\u001b[32mA\u001b[0m\u001b[32mR\u001b[0m\u001b[32mT\u001b[0m\u001b[32m \u001b[0m\u001b[32m2\u001b[0m\u001b[32m:\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32m`\u001b[0m\u001b[32m`\u001b[0m\u001b[32m`\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32m#\u001b[0m\u001b[32m \u001b[0m\u001b[32mD\u001b[0m\u001b[32me\u001b[0m\u001b[32mf\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mh\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mv\u001b[0m\u001b[32ma\u001b[0m\u001b[32mr\u001b[0m\u001b[32mi\u001b[0m\u001b[32ma\u001b[0m\u001b[32mb\u001b[0m\u001b[32ml\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32m\"\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m \u001b[0m\u001b[32mb\u001b[0m\u001b[32me\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mr\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mu\u001b[0m\u001b[32ms\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32mg\u001b[0m\u001b[32m \u001b[0m\u001b[32mi\u001b[0m\u001b[32mt\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m \u001b[0m\u001b[32m=\u001b[0m\u001b[32m \u001b[0m\u001b[32m5\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32m#\u001b[0m\u001b[32m \u001b[0m\u001b[32mM\u001b[0m\u001b[32mu\u001b[0m\u001b[32ml\u001b[0m\u001b[32mt\u001b[0m\u001b[32mi\u001b[0m\u001b[32mp\u001b[0m\u001b[32ml\u001b[0m\u001b[32my\u001b[0m\u001b[32m \u001b[0m\u001b[32mt\u001b[0m\u001b[32mh\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mv\u001b[0m\u001b[32ma\u001b[0m\u001b[32ml\u001b[0m\u001b[32mu\u001b[0m\u001b[32me\u001b[0m\u001b[32m \u001b[0m\u001b[32mo\u001b[0m\u001b[32mf\u001b[0m\u001b[32m \u001b[0m\u001b[32m\"\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m \u001b[0m\u001b[32mb\u001b[0m\u001b[32my\u001b[0m\u001b[32m \u001b[0m\u001b[32m3\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mf\u001b[0m\u001b[32mo\u001b[0m\u001b[32mo\u001b[0m\u001b[32m \u001b[0m\u001b[32m*\u001b[0m\u001b[32m=\u001b[0m\u001b[32m \u001b[0m\u001b[32m3\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32m`\u001b[0m\u001b[32m`\u001b[0m\u001b[32m`\u001b[0m"
     ]
    }
   ],
   "source": [
    "res = chat.reply(code='foo *= 3', global_vars='{}', local_vars='{}', \n",
    "                 next_line='?', question='Why does this throw a name error?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T04:46:41.617096Z",
     "start_time": "2023-03-29T04:46:41.532000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=True, callback_manager=<langchain.callbacks.base.CallbackManager object at 0x7fdb286b7c40>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', model_kwargs={'temperature': 0.0, 'top_p': 0.99, 'frequency_penalty': 0.2, 'presence_penalty': 0.0, 'logit_bias': {37811: -100, 27901: -50}, 'stop': ['SOLUTION PART 2']}, openai_api_key=None, request_timeout=60, max_retries=6, streaming=True, n=1, max_tokens=10)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat.from_config('debug',\n",
    "                        {'max_tokens': 10, 'stop': ['SOLUTION PART 2']},\n",
    "                        streaming=True,\n",
    "                        callback_manager=manager)\n",
    "chat.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:43:57.126011Z",
     "start_time": "2023-03-28T03:43:57.092566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gpt-3.5-turbo',\n",
       " 'temperature': 0.0,\n",
       " 'top_p': 0.99,\n",
       " 'max_tokens': 10,\n",
       " 'frequency_penalty': 0.2,\n",
       " 'presence_penalty': 0.0,\n",
       " 'logit_bias': {37811: -100, 27901: -50},\n",
       " 'stop': ['SOLUTION PART 2']}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:43:59.860040Z",
     "start_time": "2023-03-28T03:43:59.827077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.chat.streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:44:00.490267Z",
     "start_time": "2023-03-28T03:44:00.457051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler at 0x7fdb29132910>]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.chat.callback_manager.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:44:02.555899Z",
     "start_time": "2023-03-28T03:44:01.516820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLUTION PART 1:\n",
      "The code is trying"
     ]
    }
   ],
   "source": [
    "res = chat.reply(code='foo *= 3', global_vars='{}', local_vars='{}', \n",
    "                 next_line='?', question='Why does this throw a name error?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:44:06.698672Z",
     "start_time": "2023-03-28T03:44:06.665672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='SOLUTION PART 1:\\nThe code is trying', additional_kwargs={})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:44:16.440190Z",
     "start_time": "2023-03-28T03:44:16.408421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an incredibly effective AI programming assistant. You have in-depth knowledge across a broad range of sub-fields within computer science, software development, and data science, and your goal is to help Python programmers resolve their most challenging bugs.\n",
      "\n",
      "Human: This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Your response must have exactly two parts. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it (if you don't know what the problem is, SOLUTION PART 1 should instead list a few possible causes or things I could try in order to identify the issue). In the section titled SOLUTION PART 2, write a corrected version of the input code snippet (if you don't know, SOLUTION PART 2 should say None). SOLUTION PART 2 must contain only python code - there must not be any English explanation outside of code comments or docstrings. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "Why does this throw a name error?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "foo *= 3\n",
      "\n",
      "NEXT LINE:\n",
      "?\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{}\n",
      "\n",
      "AI: SOLUTION PART 1:\n",
      "The code is trying\n"
     ]
    }
   ],
   "source": [
    "print(chat.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:26:56.570559Z",
     "start_time": "2023-03-28T03:26:56.540153Z"
    }
   },
   "outputs": [],
   "source": [
    "handler = chat.chat.callback_manager.handlers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:26:56.763935Z",
     "start_time": "2023-03-28T03:26:56.726720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.ignore_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:26:57.431540Z",
     "start_time": "2023-03-28T03:26:57.398598Z"
    }
   },
   "outputs": [],
   "source": [
    "handler.on_llm_new_token??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T03:13:14.485973Z",
     "start_time": "2023-03-28T03:13:14.433266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'request_timeout': 60,\n",
       " 'max_tokens': None,\n",
       " 'stream': False,\n",
       " 'n': 1,\n",
       " 'stop': ['a']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatOpenAI(stop=['a'])._default_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Chain for multi part response\n",
    "\n",
    "Trying to see if we can/should use chains for streaming multi-part responses. (When not streaming, we could often just ask for a json/yaml response, though this often seems to mess up code formatting, at least with chatgpt (turbo model). Two key considerations that convinced me this might be necessary vs just regex/str parsing the results:\n",
    "\n",
    "- this assumes we don't want to print certain sep chars/section titles, but ONLY use them for parsing. We can't just parse after the fact because we'd have already printed those bits by then.\n",
    "- we also want to force the model to respond to include each section. Less of a concern with the turbo/4 models, but before it was common to see gpt leave out some sections. If we just requested a static template output, this is a higher risk. If we omit section titles in favor of a some rare sep char to remove (e.g. \"###'), it would be harder to validate which section(s), if any, are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T04:45:32.227754Z",
     "start_time": "2023-03-30T04:45:32.170390Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.schema import LLMResult, AgentAction, AgentFinish\n",
    "from typing import Dict, List, Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T05:39:03.713478Z",
     "start_time": "2023-03-30T05:39:03.678473Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiPartResponseChain(Chain):\n",
    "    chains: List[Chain]\n",
    "        \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return self.chains[0].input_keys\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['text']\n",
    "    \n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        parts = []\n",
    "        prompt_str = self.chains[0].prompt.format(**inputs)\n",
    "        res = self.chains[0](inputs)['text']\n",
    "        prompt_str = prompt_str + res\n",
    "        parts.append(res)\n",
    "        print(prompt_str)\n",
    "        print(parts)\n",
    "        print('-' * 79)\n",
    "        for chain in self.chains[1:]:\n",
    "            res = chain.run(partial_response=prompt_str)\n",
    "            parts.append(res)\n",
    "            prompt_str = chain.prompt.format(partial_response=prompt_str) + res\n",
    "            print(prompt_str)\n",
    "            print(parts)\n",
    "            print('-' * 79)\n",
    "        return {'text': parts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T02:59:05.989973Z",
     "start_time": "2023-03-31T02:59:05.790515Z"
    }
   },
   "outputs": [],
   "source": [
    "# tmp_llm = OpenAI(model_name='text-curie-001', max_tokens=20)\n",
    "tmp_chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, \n",
    "                      max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T05:39:04.236028Z",
     "start_time": "2023-03-30T05:39:04.201183Z"
    }
   },
   "outputs": [],
   "source": [
    "chain0 = LLMChain(\n",
    "    prompt=PromptTemplate(input_variables=['animal'],\n",
    "                          template='I want to know some facts about {animal}s. Answer each question on a new line with only a few words. What sound do they make?'), \n",
    "    llm=tmp_chat\n",
    ")\n",
    "\n",
    "chains = [chain0] + [\n",
    "    LLMChain(\n",
    "        prompt=PromptTemplate(input_variables=['partial_response'],\n",
    "                              template='{partial_response}\\n\\nWhat color are they typically?'), \n",
    "        llm=tmp_chat\n",
    "    ),\n",
    "    LLMChain(\n",
    "        prompt=PromptTemplate(input_variables=['partial_response'],\n",
    "                              template='{partial_response}\\n\\nHow many years do they live on average?'), \n",
    "        llm=tmp_chat\n",
    "    ),\n",
    "    LLMChain(\n",
    "        prompt=PromptTemplate(input_variables=['partial_response'],\n",
    "                              template='{partial_response}\\n\\nWhat is their favorite food?'), \n",
    "        llm=tmp_chat\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# BELOW: old non-chat chains.\n",
    "# chain0 = LLMChain(\n",
    "#     prompt=PromptTemplate(input_variables=['animal'],\n",
    "#                           template='I want to know some facts about {animal}s. Answer each question on a new line with only a few words. What sound do they make?'), \n",
    "#     llm=tmp_llm\n",
    "# )\n",
    "\n",
    "# chains = [chain0] + [\n",
    "#     LLMChain(\n",
    "#         prompt=PromptTemplate(input_variables=['partial_response'],\n",
    "#                               template='{partial_response}\\n\\nWhat color are they typically?'), \n",
    "#         llm=tmp_llm\n",
    "#     ),\n",
    "#     LLMChain(\n",
    "#         prompt=PromptTemplate(input_variables=['partial_response'],\n",
    "#                               template='{partial_response}\\n\\nHow many years do they live on average?'), \n",
    "#         llm=tmp_llm\n",
    "#     ),\n",
    "#     LLMChain(\n",
    "#         prompt=PromptTemplate(input_variables=['partial_response'],\n",
    "#                               template='{partial_response}\\n\\nWhat is their favorite food?'), \n",
    "#         llm=tmp_llm\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T05:39:04.701819Z",
     "start_time": "2023-03-30T05:39:04.665066Z"
    }
   },
   "outputs": [],
   "source": [
    "mchain = MultiPartResponseChain(chains=chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T05:39:06.812546Z",
     "start_time": "2023-03-30T05:39:04.967724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to know some facts about ducks. Answer each question on a new line with only a few words. What sound do they make?\n",
      "\n",
      "Ducks make a quack.\n",
      "['\\n\\nDucks make a quack.']\n",
      "-------------------------------------------------------------------------------\n",
      "I want to know some facts about ducks. Answer each question on a new line with only a few words. What sound do they make?\n",
      "\n",
      "Ducks make a quack.\n",
      "\n",
      "What color are they typically?\n",
      "\n",
      "Most ducks are brown, but some are white.\n",
      "['\\n\\nDucks make a quack.', '\\n\\nMost ducks are brown, but some are white.']\n",
      "-------------------------------------------------------------------------------\n",
      "I want to know some facts about ducks. Answer each question on a new line with only a few words. What sound do they make?\n",
      "\n",
      "Ducks make a quack.\n",
      "\n",
      "What color are they typically?\n",
      "\n",
      "Most ducks are brown, but some are white.\n",
      "\n",
      "How many years do they live on average?\n",
      "\n",
      "Ducks typically live about six years.\n",
      "['\\n\\nDucks make a quack.', '\\n\\nMost ducks are brown, but some are white.', '\\n\\nDucks typically live about six years.']\n",
      "-------------------------------------------------------------------------------\n",
      "I want to know some facts about ducks. Answer each question on a new line with only a few words. What sound do they make?\n",
      "\n",
      "Ducks make a quack.\n",
      "\n",
      "What color are they typically?\n",
      "\n",
      "Most ducks are brown, but some are white.\n",
      "\n",
      "How many years do they live on average?\n",
      "\n",
      "Ducks typically live about six years.\n",
      "\n",
      "What is their favorite food?\n",
      "\n",
      "Ducks love to eat bugs, worms, and other small animals.\n",
      "['\\n\\nDucks make a quack.', '\\n\\nMost ducks are brown, but some are white.', '\\n\\nDucks typically live about six years.', '\\n\\nDucks love to eat bugs, worms, and other small animals.']\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res = mchain.run(animal='duck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T05:39:13.062833Z",
     "start_time": "2023-03-30T05:39:13.030358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nDucks make a quack.',\n",
       " '\\n\\nMost ducks are brown, but some are white.',\n",
       " '\\n\\nDucks typically live about six years.',\n",
       " '\\n\\nDucks love to eat bugs, worms, and other small animals.']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signature surgery\n",
    "\n",
    "Inintial prototype for inserting fields into signature and docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:59:30.438632Z",
     "start_time": "2023-03-26T03:59:30.402871Z"
    }
   },
   "outputs": [],
   "source": [
    "class Foo:\n",
    "    def __init__(self, fields, name2fields):\n",
    "        self.reply = self._make_func(self._reply, fields)\n",
    "        for k, v in name2fields.items():\n",
    "            setattr(self, k, self._make_func(self._reply, v))\n",
    "\n",
    "    def _reply(self, **kwargs):\n",
    "        print('Calling _reply')\n",
    "        return {'kwargs': kwargs, 'completion': 'new text...'}\n",
    "    \n",
    "    def _make_func(self, func, fields):\n",
    "        # In practice I think langchain checks for this anyway if we ask for a\n",
    "        # completion, but outside of that context typecheck would be necessary\n",
    "        # because otherwise we can provide no kwargs and _func won't complain. \n",
    "        @typecheck(**{f: str for f in fields})\n",
    "        @wraps(func)\n",
    "        def wrapper(**kwargs):\n",
    "            return func(**kwargs)\n",
    "        \n",
    "        sig = signature(wrapper)\n",
    "        params_ = {field: Parameter(field, Parameter.KEYWORD_ONLY)\n",
    "                   for field in fields}\n",
    "        wrapper.__signature__ = sig.replace(parameters=params_.values())\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:20.114292Z",
     "start_time": "2023-03-26T03:58:20.074335Z"
    }
   },
   "outputs": [],
   "source": [
    "f = Foo(\n",
    "    ['a', 'dog', 'x'],\n",
    "    {'question': ['fact', 'question', 'answer'],\n",
    "     'statement': ['salutation', 'name']}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:20.457369Z",
     "start_time": "2023-03-26T03:58:20.421405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Foo._reply(*, a, dog, x)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:20.860442Z",
     "start_time": "2023-03-26T03:58:20.822714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Foo._reply(*, fact, question, answer)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:22.481964Z",
     "start_time": "2023-03-26T03:58:22.443057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Foo._reply(*, salutation, name)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:22.724402Z",
     "start_time": "2023-03-26T03:58:22.687908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got TypeError(missing a required argument: 'salutation').\n"
     ]
    }
   ],
   "source": [
    "with assert_raises(TypeError):\n",
    "    f.statement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:23.861949Z",
     "start_time": "2023-03-26T03:58:23.823429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _reply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kwargs': {'salutation': 'hi', 'name': 'harry'}, 'completion': 'new text...'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.statement(salutation='hi', name='harry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T03:58:26.637222Z",
     "start_time": "2023-03-26T03:58:26.593851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling _reply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kwargs': {'fact': 'birds are sad', 'question': 'why?', 'answer': 'yes'},\n",
       " 'completion': 'new text...'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.question(fact='birds are sad', question='why?', answer='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Debug scratch\n",
    "\n",
    "See if we can use frames to identify whether we need to provide context for a user message (i.e. if frame has changed since we last did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T02:35:33.486423Z",
     "start_time": "2023-03-23T02:35:33.415676Z"
    }
   },
   "outputs": [],
   "source": [
    "from roboduck.debugger import duck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T04:02:00.648795Z",
     "start_time": "2023-03-23T04:01:59.817553Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_search(x, nums):\n",
    "    if not nums:\n",
    "        return -1\n",
    "    duck(backend='repeat')\n",
    "    mid = len(nums) // 2\n",
    "    if x == nums[mid]:\n",
    "        return x\n",
    "    if x > nums[mid]:\n",
    "        return binary_search(x, nums[mid + 1:])\n",
    "    if x < nums[mid]:\n",
    "        return binary_search(x, nums[:mid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T04:02:00.686427Z",
     "start_time": "2023-03-23T04:02:00.651642Z"
    }
   },
   "outputs": [],
   "source": [
    "nums = [33, 44, 55, 66, 77, 88, 99, 111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T04:02:44.774242Z",
     "start_time": "2023-03-23T04:02:00.695613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-70-8a37149461d4>(5)binary_search()\n",
      "-> mid = len(nums) // 2\n",
      ">>> l .\n",
      "  1  \tdef binary_search(x, nums):\n",
      "  2  \t    if not nums:\n",
      "  3  \t        return -1\n",
      "  4  \t    duck(backend='repeat')\n",
      "  5  ->\t    mid = len(nums) // 2\n",
      "  6  \t    if x == nums[mid]:\n",
      "  7  \t        return x\n",
      "  8  \t    if x > nums[mid]:\n",
      "  9  \t        return binary_search(x, nums[mid + 1:])\n",
      " 10  \t    if x < nums[mid]:\n",
      " 11  \t        return binary_search(x, nums[:mid])\n",
      ">>> y?\n",
      "next line:     mid = len(nums) // 2\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> n\n",
      "> <ipython-input-70-8a37149461d4>(6)binary_search()\n",
      "-> if x == nums[mid]:\n",
      ">>> n\n",
      "> <ipython-input-70-8a37149461d4>(8)binary_search()\n",
      "-> if x > nums[mid]:\n",
      ">>> n\n",
      "> <ipython-input-70-8a37149461d4>(10)binary_search()\n",
      "-> if x < nums[mid]:\n",
      ">>> n\n",
      "> <ipython-input-70-8a37149461d4>(11)binary_search()\n",
      "-> return binary_search(x, nums[:mid])\n",
      ">>> l .\n",
      "  6  \t    if x == nums[mid]:\n",
      "  7  \t        return x\n",
      "  8  \t    if x > nums[mid]:\n",
      "  9  \t        return binary_search(x, nums[mid + 1:])\n",
      " 10  \t    if x < nums[mid]:\n",
      " 11  ->\t        return binary_search(x, nums[:mid])\n",
      "[EOF]\n",
      ">>> [dev] What wrong?\n",
      "next line:         return binary_search(x, nums[:mid])\n",
      "\u001b[31m\"\"\"ANSWER KEY\n",
      "\n",
      "This code snippet is not working as expected. Help me debug it. First read my question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. In the section titled SOLUTION PART 1, use plain English to explain what the problem is and how to fix it. In the section titled SOLUTION PART 2, write a corrected version of the input code snippet. If you don't know what the problem is, SOLUTION PART 1 should list a few possible causes or things I could try in order to identify the issue and SOLUTION PART 2 should say N/A. Be concise and use simple language because I am a beginning programmer.\n",
      "\n",
      "QUESTION:\n",
      "[dev] What wrong?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "def binary_search(x, nums):\n",
      "    if not nums:\n",
      "        return -1\n",
      "    mid = len(nums) // 2\n",
      "    if x == nums[mid]:\n",
      "        return x\n",
      "    if x > nums[mid]:\n",
      "        return binary_search(x, nums[mid + 1:])\n",
      "    if x < nums[mid]:\n",
      "        return binary_search(x, nums[:mid])\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{\n",
      "    'x': 3,   # type: int\n",
      "    'nums': [33, 44, 55, 66, 77, 88, 99, 111],   # type: list\n",
      "    'mid': 4,   # type: int\n",
      "}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{\n",
      "}\n",
      "\n",
      "SOLUTION PART 1:\u001b[0m\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1d506c8d773a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbinary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-8a37149461d4>\u001b[0m in \u001b[0;36mbinary_search\u001b[0;34m(x, nums)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-8a37149461d4>\u001b[0m in \u001b[0;36mbinary_search\u001b[0;34m(x, nums)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "binary_search(3, nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T02:39:22.174903Z",
     "start_time": "2023-03-23T02:39:22.130813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_search(33, nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T02:39:26.415594Z",
     "start_time": "2023-03-23T02:39:26.371426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_search(39, nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T02:39:40.877084Z",
     "start_time": "2023-03-23T02:39:40.824221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_search(111, nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T02:39:44.851555Z",
     "start_time": "2023-03-23T02:39:44.810780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_search(112, nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T04:06:26.680330Z",
     "start_time": "2023-03-23T04:06:25.828803Z"
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    for i in range(5):\n",
    "        print(i)\n",
    "        duck(backend='repeat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-24T02:44:35.865038Z",
     "start_time": "2023-03-23T04:06:29.000714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "> <ipython-input-73-943befe6b744>(2)test()\n",
      "-> for i in range(5):\n",
      ">>> i\n",
      "0\n",
      ">>> l .\n",
      "  1  \tdef test():\n",
      "  2  ->\t    for i in range(5):\n",
      "  3  \t        print(i)\n",
      "  4  \t        duck(backend='repeat')\n",
      "[EOF]\n",
      ">>> y?\n",
      "frmae_id 140192236901280\n",
      "next line:     for i in range(5):\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> n\n",
      "> <ipython-input-73-943befe6b744>(3)test()\n",
      "-> print(i)\n",
      ">>> i\n",
      "1\n",
      ">>> y?\n",
      "frmae_id 140192236901280\n",
      "next line:         print(i)\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> n\n",
      "1\n",
      "> <ipython-input-73-943befe6b744>(4)test()\n",
      "-> duck(backend='repeat')\n",
      ">>> i\n",
      "1\n",
      ">>> n\n",
      "> <ipython-input-73-943befe6b744>(2)test()\n",
      "-> for i in range(5):\n",
      ">>> i\n",
      "1\n",
      ">>> l .\n",
      "  1  \tdef test():\n",
      "  2  ->\t    for i in range(5):\n",
      "  3  \t        print(i)\n",
      "  4  \t        duck(backend='repeat')\n",
      "[EOF]\n",
      ">>> i\n",
      "1\n",
      ">>> y?\n",
      "frmae_id 140192236901280\n",
      "next line:     for i in range(5):\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> n\n",
      "> <ipython-input-73-943befe6b744>(3)test()\n",
      "-> print(i)\n",
      ">>> l .\n",
      "  1  \tdef test():\n",
      "  2  \t    for i in range(5):\n",
      "  3  ->\t        print(i)\n",
      "  4  \t        duck(backend='repeat')\n",
      "[EOF]\n",
      ">>> y?\n",
      "frmae_id 140192236901280\n",
      "next line:         print(i)\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> i\n",
      "2\n",
      ">>> n\n",
      "2\n",
      "> <ipython-input-73-943befe6b744>(4)test()\n",
      "-> duck(backend='repeat')\n",
      ">>> n\n",
      "> <ipython-input-73-943befe6b744>(2)test()\n",
      "-> for i in range(5):\n",
      ">>> n\n",
      "> <ipython-input-73-943befe6b744>(3)test()\n",
      "-> print(i)\n",
      ">>> i\n",
      "3\n",
      ">>> n\n",
      "3\n",
      "> <ipython-input-73-943befe6b744>(4)test()\n",
      "-> duck(backend='repeat')\n",
      ">>> n\n",
      "> <ipython-input-73-943befe6b744>(2)test()\n",
      "-> for i in range(5):\n",
      ">>> i\n",
      "3\n",
      ">>> n\n",
      "> <ipython-input-73-943befe6b744>(3)test()\n",
      "-> print(i)\n",
      ">>> i\n",
      "4\n",
      ">>> y?\n",
      "frmae_id 140192236901280\n",
      "next line:         print(i)\n",
      "\u001b[32m[Duck] \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m\"\u001b[0m>>> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-943befe6b744>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mduck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'repeat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-943befe6b744>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mduck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'repeat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test docstring/signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:45:44.439598Z",
     "start_time": "2023-04-16T04:45:44.405015Z"
    }
   },
   "outputs": [],
   "source": [
    "from roboduck.debugger import DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T04:29:21.987189Z",
     "start_time": "2023-04-08T04:29:21.900725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_name': 'debug_full', 'chat_class': 'abc'}\n"
     ]
    }
   ],
   "source": [
    "foo(prompt_name='debug_full', chat_class='abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T04:33:05.129111Z",
     "start_time": "2023-04-08T04:33:05.098716Z"
    }
   },
   "outputs": [],
   "source": [
    "@add_docstring(DuckDB.__init__)\n",
    "def foo(**kwargs):\n",
    "    print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:49:31.439788Z",
     "start_time": "2023-04-16T04:49:31.370883Z"
    }
   },
   "outputs": [],
   "source": [
    "from roboduck.langchain.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:49:31.637087Z",
     "start_time": "2023-04-16T04:49:31.596950Z"
    }
   },
   "outputs": [],
   "source": [
    "chat = Chat.from_config('debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:49:32.011708Z",
     "start_time": "2023-04-16T04:49:31.967856Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = chat.user_message(\n",
    "    code='foo.sense()',\n",
    "    global_vars='{\"a\": 6}', local_vars='{True: False}', \n",
    "    next_line='return None', question='Why?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T04:49:32.530714Z",
     "start_time": "2023-04-16T04:49:32.494610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm debugging some code that is not working as expected and I need your help. First read my question, then examine the problematic code snippet and the current program state. Your response must have exactly two sections (1. natural language explanation, and 2. code) separated by an empty line. It should appear to the user as a single section, however - do NOT include section titles of any kind. In section 1, use plain English to answer my question. If you don't know the answer, section 1 should instead list a few possible explanations or actions I could take in order to identify the issue. If it would contribute to a more helpful answer, use section 2 to provide a corrected version of the input code snippet (leave section 2 empty otherwise). If section 2 is not empty, it must must be entirely enclosed in one pair of triple backticks (\"```\") and contain only python code - it cannot include any English explanation outside of code comments or docstrings.\n",
      "\n",
      "QUESTION:\n",
      "Why?\n",
      "\n",
      "CODE SNIPPET:\n",
      "foo.sense()\n",
      "\n",
      "NEXT LINE TO EXECUTE:\n",
      "return None\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{True: False}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{\"a\": 6}\n",
      "\n",
      "RESPONSE FORMAT:\n",
      "{ natural language answer }\n",
      "\n",
      "```\n",
      "{ working python code }\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T04:01:46.209331Z",
     "start_time": "2022-07-11T04:01:46.061604Z"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "Toying around with a custom pdb class for language model-assisted debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- [x] test prompt in playground (maybe exclude the \"full source\" kwarg?)\n",
    "- [x] port prompt to yaml file\n",
    "- [x] enable load_prompt/kwargs etc in LMdb init\n",
    "- [x] consider how we filter locals and globals (currently filter out everything w/ a leading underscore and also do some rather clumsy filtering to make sure global is used in script. But might be able to do better here.)\n",
    "- [x] consider whether to rm some fields (header, globals, code_full) from get_prompt_kwargs method OR include them in prompt\n",
    "- consider if there's a good way to make this more conversational in case we need to ask multiple questions. If we just print gpt's response, this won't work so well. Could try to revise this to fit into ConvManager paradigm.\n",
    "- consider tweaking prompt to use proxy/authority (e.g. \"Answer Key\")\n",
    "- consider adding option for \"I don't know\"\n",
    "    - Or maybe something like \"If you don't know what's causing the bug, say \"I don't know\". Then write a list of 5 plausible causes that the developer can check for when debugging.\" (take advantage of its strength at generating list, thinking of possibilities we might not)\n",
    "- consider how to handle huge data structures (big df, long list, etc.)\n",
    "~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.\n",
    "- hide user warning about using codex model name.\n",
    "- debug slowness when using magic (is it calling query multiple times?)\n",
    "~ - add option to add new cell w/ gpt-fixed function below (may need to adjust prompt a bit to encourage it to provide this)\n",
    "\n",
    "UPDATE: Something weird going on here. Openai response sometimes looks normal, sometimes very weird (like function was called many times repeatedly - maybe some multiproc/multithreaded thing happening?). When I tried hardcoding other backends (search \"partial\" or see DebugMagic.lmdb method), the reply appears to be empty. However, the global var `_lmdb_last_completion` gets updated with the expected response. Might be related to the sys.displayhook usage in the self.shell.debugger call (uncomment the source.getlines calls in the DebugMagic.lmdb method).\n",
    "\n",
    "UPDATE 2: sometimes just need to restart kernel. mock/repeat backends now work as expected.\n",
    "\n",
    "- maybe update prompt(s) to indicate that we are inside a debugger? Otherwise it might be confusing -  if all locals are params, it might seem like we're just telling gpt3 the args.\n",
    "    - should we be passing in 1 code snippet but a whole sequence of states? That might be better.\n",
    "- Think more about whether main use case is error explanation (in which case customized stack trace like pretty_errors might make more sense), natural language debugging (in which case we want to focus more on the conversational/sequential nature, maintain series of states, etc.), or static analysis (in which case a jupyter extension or magic that lets us type questions might be ideal).\n",
    "\n",
    "NOTES\n",
    "\n",
    "Considerations on how to enter qa mode:\n",
    "\n",
    "Option 1. Launch some sort of repl here, then let the user type\n",
    "natural language questions until they want to exit. This would be\n",
    "nice but maybe a bit tricky - seems like pdb may use toolkit already\n",
    "because using prompt here throws an error indicating we're already\n",
    "in an event loop.\n",
    "\n",
    "Option 2: prefix every question with \"chat\" or some command \"Q:\".\n",
    "Have to check if that's possible.\n",
    "\n",
    "Option 3: try to override default action selection so that if we\n",
    "type something that looks like natural language rather than a couple\n",
    "variable names (maybe something ending in or containing a question \n",
    "mark) we query gpt instead of trying to eval vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.474545Z",
     "start_time": "2022-08-18T04:41:08.075519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from /Users/hmamin/jabberwocky/data/misc/gooseai_sample_responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "import cmd\n",
    "from contextlib import redirect_stdout\n",
    "import inspect\n",
    "from IPython.core.magics import NamespaceMagics\n",
    "from IPython.core.magic import cell_magic, line_cell_magic, line_magic, \\\n",
    "    magics_class, Magics, no_var_expand\n",
    "from IPython.core.magic_arguments import argument, magic_arguments, \\\n",
    "    parse_argstring\n",
    "import ipynbname\n",
    "import pandas as pd\n",
    "from pdb import Pdb\n",
    "from prompt_toolkit import prompt\n",
    "import pyperclip\n",
    "import sys\n",
    "\n",
    "from htools import *\n",
    "from jabberwocky.openai_utils import GPT, load_prompt, GPTBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.537492Z",
     "start_time": "2022-08-18T04:41:09.478013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from cli.ReadmeUpdater method.\n",
    "def load_ipynb(path):\n",
    "    \"\"\"Loads ipynb and formats cells into 1 big string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: Path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        cells = json.load(f)['cells']\n",
    "        \n",
    "    cell_str = ''\n",
    "    for cell in cells:\n",
    "        if not cell['source']: continue\n",
    "        source = '\\n' + ''.join(cell['source']) + '\\n'\n",
    "        if cell['cell_type'] == 'code':\n",
    "            source = '\\n```' + source + '```\\n'\n",
    "        cell_str += source\n",
    "    return cell_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.603843Z",
     "start_time": "2022-08-18T04:41:09.554375Z"
    }
   },
   "outputs": [],
   "source": [
    "df is_ipy_cell_var(name):\n",
    "    # True if \"_\" or like \"_17\".\n",
    "    return name.startswith('_') and (name[1:].isdigit() or not name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.709113Z",
     "start_time": "2022-08-18T04:41:09.607001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_1': True,\n",
       " '_99': True,\n",
       " '_': True,\n",
       " '__': False,\n",
       " '_1_': False,\n",
       " '_a': False,\n",
       " '__1': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{name: is_ipy_cell_var(name)\n",
    " for name in ('_1', '_99', '_', '__', '_1_', '_a', '__1')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.776806Z",
     "start_time": "2022-08-18T04:41:09.722044Z"
    }
   },
   "outputs": [],
   "source": [
    "class LMdb(Pdb):\n",
    "    \n",
    "    def __init__(self, *args, backend='openai', model=None, \n",
    "                 full_context=False, log=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.prompt = '[LMdb]'\n",
    "        self.gpt = GPTBackend(log_stdout=False)\n",
    "        # TODO: this does seem to remove the handler from handlers but their\n",
    "        # must be some other trace of it because we still log to stdout.\n",
    "        self.gpt.handlers = [handler for handler in self.gpt.logger.handlers \n",
    "                             if 'stdout' not in str(handler)]\n",
    "        self.query_kwargs = load_prompt(\n",
    "            'debug_full' if full_context else 'debug', \n",
    "            verbose=False\n",
    "        )\n",
    "        self.prompt_template = self.query_kwargs.pop('prompt')\n",
    "        if model is not None:\n",
    "            self.query_kwargs['model'] = model\n",
    "        self.backend = backend\n",
    "        self.full_context = full_context\n",
    "        self.log = log\n",
    "        self._last_completion = ''\n",
    "    \n",
    "    def _get_prompt_kwargs(self):\n",
    "        res = {}\n",
    "        \n",
    "        # Get current code snippet.\n",
    "        try:\n",
    "            res['code'] = inspect.getsource(self.curframe)\n",
    "        except OSError as err:\n",
    "            self.error(err)\n",
    "        res['local_vars'] = {k: v for k, v in self.curframe_locals.items() \n",
    "                             if not is_ipy_cell_var(k)}\n",
    "            \n",
    "        # Get full source code if necessary.\n",
    "        if self.full_context:            \n",
    "            # File is a string, either a file name or something like\n",
    "            # <ipython-input-50-e97ed612f523>.\n",
    "            file = inspect.getsourcefile(self.curframe.f_code)\n",
    "            if file.startswith('<ipython'):\n",
    "                res['full_code'] = load_ipynb(ipynbname.path())\n",
    "                res['file_type'] = 'jupyter notebook'\n",
    "            else:\n",
    "                res['full_code'] = load(file, verbose=False)\n",
    "                res['file_type'] = 'python script'\n",
    "            used_tokens = set(res['full_code'].split())\n",
    "        else:   \n",
    "            # This is intentionally different from the used_tokens line in the\n",
    "            # if clause - we only want to consider local code here.\n",
    "            used_tokens = set(res['code'].split())\n",
    "            \n",
    "        # TODO: code.split() might not work so well in some cases.\n",
    "        # Namespace is often polluted with lots of unused globals (htools is\n",
    "        # very much guilty of this ðŸ˜¬) and we don't want to clutter up the \n",
    "        # prompt with these.\n",
    "        res['global_vars'] = {k: v for k, v in self.curframe.f_globals.items() \n",
    "                              if k in used_tokens and not is_ipy_cell_var(k)}\n",
    "        return res\n",
    "\n",
    "    def onecmd(self, line):\n",
    "        \"\"\"Interpret the argument as though it had been typed in response\n",
    "        to the prompt.\n",
    "        Checks whether this line is typed at the normal prompt or in\n",
    "        a breakpoint command list definition.\n",
    "        \"\"\"\n",
    "        if not self.commands_defining:\n",
    "            if '?' in line:\n",
    "                return self.ask_language_model(line)\n",
    "            return cmd.Cmd.onecmd(self, line)\n",
    "        else:\n",
    "            return self.handle_command_def(line)\n",
    "        \n",
    "    def ask_language_model(self, question):\n",
    "        # TODO: maybe should reconstruct each time q is asked? State changes,\n",
    "        # that's the whole point of this debugger.\n",
    "        prompt_kwargs = self._get_prompt_kwargs()\n",
    "        prompt = self.prompt_template.format(question=question, \n",
    "                                             **prompt_kwargs)\n",
    "        if len(prompt.split()) > 1_000:\n",
    "            warnings.warn(\n",
    "                'Prompt is very long (>1k words). You\\'re approaching a risky'\n",
    "                ' zone where your prompt + completion might exceed the max '\n",
    "                'sequence length.'\n",
    "            )\n",
    "        print(prompt)\n",
    "        \n",
    "        # TODO: could we somehow use convmanager here? Given that I envisioned\n",
    "        # this as a conversation with the kernel/interpreter/script/something.\n",
    "        # TODO: maybe add option in gpt.query to avoid printing to stdout. For\n",
    "        # now, just use redirect_stdout here to see what result will look \n",
    "        # like.\n",
    "        # TODO: temporarily disabled logging.\n",
    "        with self.gpt(self.backend, verbose=False):\n",
    "            res, full = self.gpt.query(prompt, **self.query_kwargs, \n",
    "                                       log=self.log)\n",
    "        print(f'{self.prompt} {res[0]}')\n",
    "        \n",
    "        # TODO: when called from magic, ipython seems to delete reference to \n",
    "        # this obj so for now store it as a global var so we can try inserting\n",
    "        # a new cell.\n",
    "        self._last_completion = res[0]\n",
    "        global _lmdb_last_completion\n",
    "        _lmdb_last_completion = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.821987Z",
     "start_time": "2022-08-18T04:41:09.781519Z"
    }
   },
   "outputs": [],
   "source": [
    "@magics_class\n",
    "class DebugMagic(Magics):\n",
    "\n",
    "    @magic_arguments()\n",
    "    @argument('-i', action='store_true', \n",
    "              help='Boolean flag: if provided, INSERT a new code cell with '\n",
    "                   'the suggested code fix.')\n",
    "    @line_magic\n",
    "    def lmdb(self, line='', cell=None):\n",
    "        \"\"\"Silence warnings for a cell. The -p flag can be used to make the\n",
    "        change persist, at least until the user changes it again.\n",
    "        \"\"\"\n",
    "        args = parse_argstring(self.lmdb, line)\n",
    "        cls = self.shell.debugger_cls\n",
    "        # TODO: change partial back to just LMdb\n",
    "        self.shell.debugger_cls = self.shell.InteractiveTB.debugger_cls = partial(\n",
    "            LMdb, backend='openai', log=True)\n",
    "#         print(inspect.getsource(self.shell.debugger))\n",
    "#         hr()\n",
    "#         print(inspect.getsource(self.shell.InteractiveTB.debugger))\n",
    "#         hr()\n",
    "#         print(self.shell.InteractiveTB.debugger_cls)\n",
    "#         print(self.shell.InteractiveTB.pdb)\n",
    "\n",
    "        print('pdb:', self.shell.pdb)\n",
    "        self.shell.debugger(force=True)\n",
    "        print('pdb:', self.shell.pdb)\n",
    "        if args.i:\n",
    "#             self.shell.set_next_input(self.shell.pdb._last_completion, \n",
    "#                                       replace=False)\n",
    "            self.shell.set_next_input(_lmdb_last_completion, \n",
    "                                      replace=False)\n",
    "        self.shell.debugger_cls = self.shell.InteractiveTB.debugger_cls = cls\n",
    "        \n",
    "get_ipython().register_magics(DebugMagic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.860489Z",
     "start_time": "2022-08-18T04:41:09.824063Z"
    }
   },
   "outputs": [],
   "source": [
    "def chat_db(backend='openai', model=None):\n",
    "    # Equivalent of native breakpoint().\n",
    "    LMdb(backend=backend, model=model).set_trace(sys._getframe().f_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.898502Z",
     "start_time": "2022-08-18T04:41:09.865728Z"
    }
   },
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    for i in range(x):\n",
    "        chat_db()\n",
    "        print(2 / (i - 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.935626Z",
     "start_time": "2022-08-18T04:41:09.902388Z"
    }
   },
   "outputs": [],
   "source": [
    "def buggy_sort(nums):\n",
    "    for i in range(len(nums)):\n",
    "        for j in range(len(nums)):\n",
    "            if nums[j] > nums[j + 1]:\n",
    "                nums[j], nums[j + 1] = nums[j + 1], nums[j]\n",
    "#             chat_db()\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:09.984400Z",
     "start_time": "2022-08-18T04:41:09.944147Z"
    }
   },
   "outputs": [],
   "source": [
    "# def buggy_sort(nums):\n",
    "#     for i in range(len(nums)):\n",
    "#         for j in range(len(nums) - 1):\n",
    "#             if nums[j] > nums[j + 1]:\n",
    "#                 nums[j + 1] = nums[j]\n",
    "#                 nums[j] = nums[j + 1]\n",
    "#             chat_db()\n",
    "#     return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:10.019403Z",
     "start_time": "2022-08-18T04:41:09.987353Z"
    }
   },
   "outputs": [],
   "source": [
    "nums_ = [9, 9, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:10.049948Z",
     "start_time": "2022-08-18T04:41:10.022057Z"
    }
   },
   "outputs": [],
   "source": [
    "# def buggy_sort(nums):\n",
    "#     for i in range(len(nums)):\n",
    "#         for j in range(len(nums) - 1):\n",
    "#             if nums[j] > nums[j + 1]:\n",
    "#                 nums[j], nums[j + 1] = nums[j + 1], nums[j]\n",
    "# #             chat_db()\n",
    "#     return nums_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:10.081629Z",
     "start_time": "2022-08-18T04:41:10.052371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set some globals.\n",
    "z = 100\n",
    "a = ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:10.114703Z",
     "start_time": "2022-08-18T04:41:10.084861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some output.\n"
     ]
    }
   ],
   "source": [
    "print('This is some output.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:10.346956Z",
     "start_time": "2022-08-18T04:41:10.118093Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b83037aa494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuggy_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3715fc4ecf50>\u001b[0m in \u001b[0;36mbuggy_sort\u001b[0;34m(nums)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                 \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#             chat_db()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "buggy_sort([5, 2, 4, 4, 3, 1, 9, 17, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T04:41:35.447296Z",
     "start_time": "2022-08-18T04:41:12.075770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdb: False\n",
      "[1] > \u001b[33;01m<ipython-input-9-3715fc4ecf50>\u001b[00m(\u001b[36;01m4\u001b[00m)buggy_sort()\n",
      "-> if nums[j] > nums[j + 1]:\n",
      "[LMdb]i,j\n",
      "(0, 8)\n",
      "[LMdb]nums\n",
      "[2, 4, 4, 3, 1, 5, 9, 7, 17]\n",
      "[LMdb]Why do we get an index error?\n",
      "This code snippet is not working as expected. Help the developer debug it. First read their question, then examine the snippet of code that is causing the issue and look at the values of the local and global variables. Finally, explain what the problem is and how to fix it. Use simple language a beginning programmer could understand.\n",
      "\n",
      "QUESTION:\n",
      "Why do we get an index error?\n",
      "\n",
      "CURRENT CODE SNIPPET:\n",
      "def buggy_sort(nums):\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(len(nums)):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j], nums[j + 1] = nums[j + 1], nums[j]\n",
      "#             chat_db()\n",
      "    return nums\n",
      "\n",
      "\n",
      "LOCAL VARIABLES:\n",
      "{'nums': [2, 4, 4, 3, 1, 5, 9, 7, 17], 'i': 0, 'j': 8}\n",
      "\n",
      "GLOBAL VARIABLES:\n",
      "{}\n",
      "\n",
      "EXPLANATION AND SUGGESTED FIX:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hmamin/jabberwocky/lib/jabberwocky/openai_utils.py:657: UserWarning: Allowing model \"code-davinci-002\" to pass through because openai_passthrough=True. We trust you to make sure this is a valid model.\n",
      "  f'Allowing model \"{model}\" to pass through because '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LMdb] The problem is that the range of the second for loop is len(nums) which is 9. The last index of nums is 8. So when j is 8, nums[j + 1] is nums[9] which is out of range.\n",
      "\n",
      "The fix is to change the range of the second for loop to range(len(nums) - 1).\n",
      "\n",
      "def buggy_sort(nums):\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(len(nums) - 1):\n",
      "            if nums[j] > nums[j + 1]:\n",
      "                nums[j], nums[j + 1] = nums[j + 1], nums[j]\n",
      "#             chat_db()\n",
      "    return nums\n",
      "[LMdb]q\n",
      "pdb: False\n"
     ]
    }
   ],
   "source": [
    "# Note: couldn't get cell magic version working so far. Says:\n",
    "# \"UsageError: %%lmdb is a cell magic, but the cell body is empty. Did you\n",
    "# mean the line magic %lmdb (single %)?\"\n",
    "# Even when I try to define the method with all the same settings as the \n",
    "# default class.\n",
    "%lmdb -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The problem is that the range of the second for loop is len(nums) which is 9. The last index of nums is 8. So when j is 8, nums[j + 1] is nums[9] which is out of range.\n",
    "\n",
    "The fix is to change the range of the second for loop to range(len(nums) - 1).\n",
    "\n",
    "def buggy_sort(nums):\n",
    "    for i in range(len(nums)):\n",
    "        for j in range(len(nums) - 1):\n",
    "            if nums[j] > nums[j + 1]:\n",
    "                nums[j], nums[j + 1] = nums[j + 1], nums[j]\n",
    "#             chat_db()\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
